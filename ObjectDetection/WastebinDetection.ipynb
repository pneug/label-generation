{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHnVupBBn9eR"
   },
   "source": [
    "# Detectron2 Beginner's Tutorial\n",
    "\n",
    "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
    "\n",
    "Welcome to detectron2! This is the official colab tutorial of detectron2. Here, we will go through some basics usage of detectron2, including the following:\n",
    "* Run inference on images or videos, with an existing detectron2 model\n",
    "* Train a detectron2 model on a new dataset\n",
    "\n",
    "You can make a copy of this tutorial by \"File -> Open in playground mode\" and make changes there. __DO NOT__ request access to this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM54r6jlKTII"
   },
   "source": [
    "# Install detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FsePPpwZSmqt"
   },
   "outputs": [],
   "source": [
    "# !python -m pip install pyyaml==5.1\n",
    "import sys, os, distutils.core\n",
    "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities.\n",
    "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
    "# !git clone 'https://github.com/facebookresearch/detectron2'\n",
    "# dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
    "# !python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
    "# sys.path.insert(0, os.path.abspath('./detectron2'))\n",
    "\n",
    "# Properly install detectron2. (Please do not install twice in both ways)\n",
    "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d288Z2mF5dC",
    "outputId": "c47c5426-64d6-4632-f868-e2f14dfe39be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philip/anaconda3/envs/detector/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch:  1.13 ; cuda:  cu117\n",
      "detectron2: 0.6\n"
     ]
    }
   ],
   "source": [
    "import torch, detectron2\n",
    "# !nvcc --version\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "print(\"detectron2:\", detectron2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZyAvNCJMmvFF"
   },
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_imshow(im, timeout=100):\n",
    "    cv2.imshow(\"sample\", im)\n",
    "    cv2.waitKey(timeout)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_TIME = False\n",
    "DATA_PATH = \"../data/\"\n",
    "DATA_PATH_BASE = DATA_PATH + \"Waste_Bin_Detection_Dataset/\"\n",
    "\n",
    "TRAIN_NAME_ORIGINAL = \"sunny_2021_03_23_14_33_cam5_filtered_detection_ground_truth_labels(training dataset).json\"\n",
    "TRAIN_NAME_ORIGINAL = \"sunny_reduced_static\"\n",
    "# TRAIN_NAME_REDUCED = \"sunny_2021_03_23_14_33_cam5_filtered_detection_ground_truth_labels(training dataset)_reduced.json\"\n",
    "TRAIN_NAME_REDUCED = \"sunny_reduced_static.json\"\n",
    "TRAIN_NAME_GENERATED = \"2021_03_25_14_04/merged_labels.json\"\n",
    "TRAIN_NAME_CLEANED = \"2021_03_25_14_04/cleaned.json\" # merged_labels_modified.json\"\n",
    "TRAIN_SNOWY_CLEANED_NAME = \"2022_01_21_14_04/cleaned.json\"\n",
    "TRAIN_SNOWY2_CLEANED_NAME = \"new/2021_02_18_06_28/cleaned.json\" # 2021_02_18_06_28_unreduced_clean.json\"\n",
    "TRAIN_SNOWY_NAME = \"2022_01_21_14_04/merged_labels_55.json\" # \"2022_01_21_14_04/reduced1.json\"\n",
    "TRAIN_SNOWY2_NAME = \"new/2021_02_18_06_28/merged_labels_78.json\" #\"new/2021_02_18_06_28/merged_labels.json\"\n",
    "TRAIN_SNOWY_NAME_PREV = \"2022_01_21_14_04/reduced1.json\"\n",
    "TRAIN_SNOWY2_NAME_PREV = \"new/2021_02_18_06_28/merged_labels.json\"\n",
    "TRAIN_SNOWY_CLEANED_NAME_PREV = \"2022_01_21_14_04/cleaned.json\"\n",
    "TRAIN_SNOWY2_CLEANED_NAME_PREV = \"new/2021_02_18_06_28/2021_02_18_06_28_unreduced_clean.json\"\n",
    "\n",
    "\n",
    "# VAL_NAME = \"detection_validation_dataset_ground_truth_labels(balanced).json\"\n",
    "VAL_NAME = \"validation_set_static_gt.json\"\n",
    "# TEST_NAME = \"detection_test_dataset_ground_truth_labels(balanced).json\"\n",
    "TEST_NAME = \"test_set_static_gt.json\"\n",
    "TEST_SNOWY_DAY_NAME = \"snowy_day.json\"\n",
    "TEST_SNOWY_NIGHT_NAME = \"snowy_night.json\"\n",
    "TEST_RB_NAME = \"rb.json\"\n",
    "\n",
    "TRAIN_PATH_GENERATED = DATA_PATH + \"generated_trainings_data/\"\n",
    "TRAIN_PATH_ORIGINAL = DATA_PATH_BASE + \"sunny_2021_03_23_14_33_cam5_filtered (training dataset images and ground truths)/\"\n",
    "VAL_PATH = DATA_PATH_BASE + \"cloudy_2021_04_09_16_02_cam5_filtered (validation and test dataset images and ground truths)/\"\n",
    "TEST_PATH = VAL_PATH\n",
    "TEST_SNOWY_PATH = DATA_PATH + \"/test_datasets/snowy/\"\n",
    "TEST_RB_PATH = DATA_PATH + \"/test_datasets/rb/\"\n",
    "\n",
    "all_inferences = {}\n",
    "# VAL_PATH = DATA_PATH_BASE + \"cloudy_2021_04_09_16_02_cam5_filtered (validation and test dataset images and ground truths)/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_name:  2021_02_18_06_28\n",
      "ds_name:  2021_03_25_14_04\n",
      "ds_name:  2021_04_05_14_35\n",
      "ds_name:  2021_05_18_14_02\n",
      "ds_name:  2021_06_05_12_08\n",
      "ds_name:  2021_07_07_06_41\n",
      "ds_name:  2021_08_09_06_30\n",
      "ds_name:  2021_09_15_06_28\n",
      "ds_name:  2021_10_15_18_16\n",
      "ds_name:  2021_11_02_12_59\n",
      "ds_name:  2021_12_15_12_54\n",
      "ds_name:  2022_01_21_14_04\n"
     ]
    }
   ],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "generated_data_ad = (\"new/2021_02_18_06_28/\", \"2021_03_25_14_04/\", \"new/2021_04_05_14_35/\", \"new/2021_05_18_14_02/\", \"new/2021_06_05_12_08/\", \"new/2021_07_07_06_41/\", \"new/2021_08_09_06_30/\", \"new/2021_09_15_06_28/\", \"new/2021_10_15_18_16/\", \"new/2021_11_02_12_59/\", \"new/2021_12_15_12_54/\", \"2022_01_21_14_04/\")\n",
    "generated_data_ad_path = \"../data/generated_data_ad/\"\n",
    "registered_generated_data_ad = []\n",
    "\n",
    "for dataset in generated_data_ad:\n",
    "    data_name = dataset.split(\"/\")[-2]\n",
    "    registered_generated_data_ad.append(data_name)\n",
    "    print(\"ds_name: \", data_name)\n",
    "    register_coco_instances(data_name, {}, generated_data_ad_path + dataset + \"merged_labels.json\", generated_data_ad_path + dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# registrate the snowy_prev datatsets\n",
    "register_coco_instances(\"dataset_train_snowy_prev2\", {}, TRAIN_PATH_GENERATED + TRAIN_SNOWY_NAME_PREV, TRAIN_PATH_GENERATED)\n",
    "register_coco_instances(\"dataset_train_snowy2_prev2\", {}, TRAIN_PATH_GENERATED + TRAIN_SNOWY2_NAME_PREV, TRAIN_PATH_GENERATED)\n",
    "register_coco_instances(\"dataset_train_snowy_clean_prev2\", {}, TRAIN_PATH_GENERATED + TRAIN_SNOWY_CLEANED_NAME_PREV, TRAIN_PATH_GENERATED)\n",
    "register_coco_instances(\"dataset_train_snowy2_clean_prev2\", {}, TRAIN_PATH_GENERATED + TRAIN_SNOWY2_CLEANED_NAME_PREV, TRAIN_PATH_GENERATED)\n",
    "\n",
    "# register_coco_instances(\"dataset_train_snowy_prev\", {}, DATA_PATH + TRAIN_SNOWY_NAME_PREV, TRAIN_SNOWY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register_coco_instances(\"dataset_train_reduced2\", {}, TRAIN_PATH_ORIGINAL + TRAIN_NAME_REDUCED, TRAIN_PATH_ORIGINAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_train_val_test(label_path, train_split, val_split, test_split):\n",
    "    train_name = label_path[:-5] + \"_train.json\"\n",
    "    val_name = label_path[:-5] + \"_val.json\"\n",
    "    test_name = label_path[:-5] + \"_test.json\"\n",
    "\n",
    "    # check if the splits already exist\n",
    "    if os.path.exists(train_name) and os.path.exists(val_name) and os.path.exists(test_name):\n",
    "        print(\"Splits already exist!\")\n",
    "        return train_name, val_name, test_name\n",
    "\n",
    "    with open(label_path) as f:\n",
    "        gt_labels = json.load(f)\n",
    "    \n",
    "    # get the image ids\n",
    "    image_ids = [img[\"id\"] for img in gt_labels[\"images\"]]\n",
    "    num_images = len(image_ids)\n",
    "    num_train = int(num_images * train_split)\n",
    "    num_val = int(num_images * val_split)\n",
    "    num_test = int(num_images * test_split)\n",
    "    train_split = image_ids[:num_train]\n",
    "    val_split = image_ids[num_train:num_train+num_val]\n",
    "    test_split = image_ids[num_train+num_val:]\n",
    "    print(\"Num train: \", len(train_split))\n",
    "    print(\"Num val: \", len(val_split))\n",
    "    print(\"Num test: \", len(test_split))\n",
    "    # save the splits to a json file\n",
    "    train_split_dict = {\"images\": [], \"annotations\": []}\n",
    "    val_split_dict = {\"images\": [], \"annotations\": []}\n",
    "    test_split_dict = {\"images\": [], \"annotations\": []}\n",
    "    for img in gt_labels[\"images\"]:\n",
    "        if img[\"id\"] in train_split:\n",
    "            train_split_dict[\"images\"].append(img)\n",
    "        elif img[\"id\"] in val_split:\n",
    "            val_split_dict[\"images\"].append(img)\n",
    "        elif img[\"id\"] in test_split:\n",
    "            test_split_dict[\"images\"].append(img)\n",
    "        else:\n",
    "            print(\"Image id not found in any split!\")\n",
    "    for ann in gt_labels[\"annotations\"]:\n",
    "        if ann[\"image_id\"] in train_split:\n",
    "            train_split_dict[\"annotations\"].append(ann)\n",
    "        elif ann[\"image_id\"] in val_split:\n",
    "            val_split_dict[\"annotations\"].append(ann)\n",
    "        elif ann[\"image_id\"] in test_split:\n",
    "            test_split_dict[\"annotations\"].append(ann)\n",
    "        else:\n",
    "            print(\"Annotation image id not found in any split!\")\n",
    "\n",
    "    # add the other data required for coco format\n",
    "    train_split_dict[\"info\"] = gt_labels[\"info\"]\n",
    "    train_split_dict[\"licenses\"] = gt_labels[\"licenses\"]\n",
    "    train_split_dict[\"categories\"] = gt_labels[\"categories\"]\n",
    "    val_split_dict[\"info\"] = gt_labels[\"info\"]\n",
    "    val_split_dict[\"licenses\"] = gt_labels[\"licenses\"]\n",
    "    val_split_dict[\"categories\"] = gt_labels[\"categories\"]\n",
    "    test_split_dict[\"info\"] = gt_labels[\"info\"]\n",
    "    test_split_dict[\"licenses\"] = gt_labels[\"licenses\"]\n",
    "    test_split_dict[\"categories\"] = gt_labels[\"categories\"]\n",
    "\n",
    "    # assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds()))\n",
    "    # asser that the set of image ids in the annotations is the same as the set of image ids in the images\n",
    "    img_ids_train = set([img[\"id\"] for img in train_split_dict[\"images\"]])\n",
    "    annot_img_ids_train = set([annot[\"image_id\"] for annot in train_split_dict[\"annotations\"]])\n",
    "    assert annot_img_ids_train == annot_img_ids_train & img_ids_train\n",
    "    img_ids_val = set([img[\"id\"] for img in val_split_dict[\"images\"]])\n",
    "    annot_img_ids_val = set([annot[\"image_id\"] for annot in val_split_dict[\"annotations\"]])\n",
    "    assert annot_img_ids_val == annot_img_ids_val & img_ids_val\n",
    "    img_ids_test = set([img[\"id\"] for img in test_split_dict[\"images\"]])\n",
    "    annot_img_ids_test = set([annot[\"image_id\"] for annot in test_split_dict[\"annotations\"]])\n",
    "    assert annot_img_ids_test == annot_img_ids_test & img_ids_test\n",
    "    \n",
    "    # save the splits to a json file\n",
    "    with open(train_name, \"w\") as f:\n",
    "        json.dump(train_split_dict, f)\n",
    "    with open(val_name, \"w\") as f:\n",
    "        json.dump(val_split_dict, f)\n",
    "    with open(test_name, \"w\") as f:\n",
    "        json.dump(test_split_dict, f)\n",
    "\n",
    "    return train_name, val_name, test_name\n",
    "\n",
    "# label_path = \"../data/Waste_Bin_Detection_Dataset/sunny_2021_03_23_14_33_cam5_filtered (training dataset images and ground truths)/\"\n",
    "# label_path += \"sunny_reduced_static.json\"\n",
    "# split_into_train_val_test(label_path, 0.6, 0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits already exist!\n",
      "Splits already exist!\n",
      "Splits already exist!\n",
      "Splits already exist!\n",
      "Splits already exist!\n",
      "Splits already exist!\n"
     ]
    }
   ],
   "source": [
    "# split the sunny, cloudy, snowy and rb datasets into train, val, and test\n",
    "dataset_splits = {TRAIN_PATH_ORIGINAL + TRAIN_NAME_REDUCED: (0.5, 0.25, 0.25),\n",
    "                VAL_PATH + VAL_NAME: (1.0, 0, 0), TEST_PATH + TEST_NAME: (0.0, 0.5, 0.5),\n",
    "                TEST_SNOWY_PATH + TEST_SNOWY_DAY_NAME: (0.5, 0.25, 0.25),\n",
    "                TEST_SNOWY_PATH + TEST_SNOWY_NIGHT_NAME: (0.5, 0.25, 0.25),\n",
    "                TEST_RB_PATH + TEST_RB_NAME: (0.5, 0.25, 0.25)}\n",
    "dataset_names = [\"sunny\", \"cloudy_train\", \"cloudy_val\", \"snowy_day\", \"snowy_night\", \"rb\"]\n",
    "dataset_paths = [TRAIN_PATH_ORIGINAL, VAL_PATH, TEST_PATH, TEST_SNOWY_PATH, TEST_SNOWY_PATH, TEST_RB_PATH]\n",
    "for i, p in enumerate(dataset_splits.keys()):\n",
    "    train_name, val_name, test_name = split_into_train_val_test(p, dataset_splits[p][0], dataset_splits[p][1], dataset_splits[p][2])\n",
    "    register_coco_instances(f\"{dataset_names[i]}_train\", {}, train_name, dataset_paths[i])\n",
    "    register_coco_instances(f\"{dataset_names[i]}_val\", {}, val_name, dataset_paths[i])\n",
    "    register_coco_instances(f\"{dataset_names[i]}_test\", {}, test_name, dataset_paths[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PIbAM2pv-urF"
   },
   "outputs": [],
   "source": [
    "# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "register_coco_instances(\"dataset_train_original\", {}, TRAIN_PATH_ORIGINAL + TRAIN_NAME_ORIGINAL, TRAIN_PATH_ORIGINAL)\n",
    "register_coco_instances(\"dataset_train_generated\", {}, TRAIN_PATH_GENERATED + TRAIN_NAME_GENERATED, TRAIN_PATH_GENERATED)\n",
    "register_coco_instances(\"dataset_train_generated_cleaned\", {}, TRAIN_PATH_GENERATED + TRAIN_NAME_CLEANED, TRAIN_PATH_GENERATED)\n",
    "register_coco_instances(\"dataset_train_reduced\", {}, TRAIN_PATH_ORIGINAL + TRAIN_NAME_REDUCED, TRAIN_PATH_ORIGINAL)\n",
    "\n",
    "register_coco_instances(\"dataset_train_snowy\", {}, TRAIN_PATH_GENERATED + TRAIN_SNOWY_NAME, TRAIN_PATH_GENERATED)\n",
    "register_coco_instances(\"dataset_train_snowy2\", {}, TRAIN_PATH_GENERATED + TRAIN_SNOWY2_NAME, TRAIN_PATH_GENERATED)\n",
    "register_coco_instances(\"dataset_train_snowy_clean\", {}, TRAIN_PATH_GENERATED + TRAIN_SNOWY_CLEANED_NAME, TRAIN_PATH_GENERATED)\n",
    "register_coco_instances(\"dataset_train_snowy2_clean\", {}, TRAIN_PATH_GENERATED + TRAIN_SNOWY2_CLEANED_NAME, TRAIN_PATH_GENERATED)\n",
    "\n",
    "register_coco_instances(\"dataset_val\", {}, VAL_PATH + VAL_NAME, VAL_PATH)\n",
    "register_coco_instances(\"dataset_test\", {}, TEST_PATH + TEST_NAME, TEST_PATH)\n",
    "\n",
    "register_coco_instances(\"dataset_snowy_day\", {}, TEST_SNOWY_PATH + TEST_SNOWY_DAY_NAME, TEST_SNOWY_PATH)\n",
    "register_coco_instances(\"dataset_snowy_night\", {}, TEST_SNOWY_PATH + TEST_SNOWY_NIGHT_NAME, TEST_SNOWY_PATH)\n",
    "register_coco_instances(\"dataset_rb\", {}, TEST_RB_PATH + TEST_RB_NAME, TEST_RB_PATH)\n",
    "\n",
    "from detectron2.structures import BoxMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_coco_instances(\"dataset_rb2\", {}, TEST_RB_PATH + TEST_RB_NAME, TEST_RB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk4gID50K03a"
   },
   "source": [
    "# Run a pre-trained detectron2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgKyUL4pngvE"
   },
   "source": [
    "We first download an image from the COCO dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "dq9GY37ml1kr",
    "outputId": "c4755b80-0d0b-4094-d0ce-ae2e39af6df3"
   },
   "outputs": [],
   "source": [
    "#!wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg\n",
    "#im = cv2.imread(\"./input.jpg\")\n",
    "\n",
    "# im = cv2.imread(TRAIN_PATH + \"/images/camera5_1616524512_925074188.jpg\")\n",
    "im = cv2.imread(TRAIN_PATH_GENERATED + \"/2021_03_25_14_04/images/camera5/1616697195_746899928.jpg\")\n",
    "\n",
    "# cv2.imshow(\"sample\", im)\n",
    "# cv2.waitKey(10000)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# cv2_imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM1thbN-ntjI"
   },
   "source": [
    "Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUjkwRsOn1O0",
    "outputId": "46bafc09-d3d5-482b-90b6-315b75040f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/09 04:55:06 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philip/anaconda3/envs/detector/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.005  # set threshold for this model\n",
    "\n",
    "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d3KxiHO_0gb",
    "outputId": "f64ba888-ba17-432d-f4b5-7fac14f7f8fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  2,  2,  2, 11,  2,  2,  2,  2,  2,  2,  2,  2,  2,  7,  2,  2,  2,\n",
      "         0,  9], device='cuda:0')\n",
      "Boxes(tensor([[266.4833, 180.8726, 317.3206, 211.3985],\n",
      "        [223.7761, 189.1363, 266.9907, 209.8377],\n",
      "        [167.2536, 193.4805, 190.5445, 212.0146],\n",
      "        [350.9119, 186.8939, 374.5399, 221.9578],\n",
      "        [119.9063, 104.4992, 155.9364, 148.1499],\n",
      "        [203.9761, 191.2700, 231.2309, 209.5346],\n",
      "        [309.9510, 179.3568, 355.0557, 205.8382],\n",
      "        [189.1120, 196.9571, 204.9933, 211.5256],\n",
      "        [214.2793, 192.9922, 229.5054, 208.0728],\n",
      "        [203.0917, 192.0317, 218.8095, 209.9182],\n",
      "        [326.1642, 188.8716, 357.5744, 206.0282],\n",
      "        [213.3864, 192.2182, 242.6201, 210.0946],\n",
      "        [266.0551, 182.1116, 290.0341, 212.0143],\n",
      "        [196.8726, 198.0351, 214.3091, 210.5594],\n",
      "        [305.1704, 176.8456, 350.8604, 205.8191],\n",
      "        [354.4117, 186.3788, 375.5167, 202.8780],\n",
      "        [285.9145, 180.7034, 335.8421, 208.9566],\n",
      "        [179.1970, 196.3996, 198.3061, 212.1292],\n",
      "        [ 51.1034, 166.8302,  71.4667, 239.2993],\n",
      "        [121.5652,  89.0161, 159.7787, 105.0577]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification\n",
    "print(outputs[\"instances\"].pred_classes)\n",
    "print(outputs[\"instances\"].pred_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "8IRGo8d0qkgR",
    "outputId": "d0c28273-71c8-48c6-c901-af0fa0cfd3a5"
   },
   "outputs": [],
   "source": [
    "# We can use `Visualizer` to draw the predictions on the image.\n",
    "if FIRST_TIME:\n",
    "    v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1], 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2bjrfb2LDeo"
   },
   "source": [
    "# Train on a custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjbUIhSxUdm_"
   },
   "source": [
    "In this section, we show how to train an existing detectron2 model on a custom dataset in a new format.\n",
    "\n",
    "We use [the balloon segmentation dataset](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon)\n",
    "which only has one class: balloon.\n",
    "We'll train a balloon segmentation model from an existing model pre-trained on COCO dataset, available in detectron2's model zoo.\n",
    "\n",
    "Note that COCO dataset does not have the \"balloon\" category. We'll be able to recognize this new class in a few minutes.\n",
    "\n",
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Qg7zSVOulkb",
    "outputId": "b3c3e5b1-44f0-4402-bd63-076af70ef442"
   },
   "outputs": [],
   "source": [
    "# download, decompress the data\n",
    "if FIRST_TIME:\n",
    "    !wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\n",
    "    !unzip balloon_dataset.zip > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVJoOm6LVJwW"
   },
   "source": [
    "Register the balloon dataset to detectron2, following the [detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).\n",
    "Here, the dataset is in its custom format, therefore we write a function to parse it and prepare it into detectron2's standard format. User should write such a function when using a dataset in custom format. See the tutorial for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ljbWTX0Wi8E"
   },
   "source": [
    "To verify the dataset is in correct format, let's visualize the annotations of randomly selected samples in the training set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UkNbUzUOLYf0",
    "outputId": "4f5ed932-624a-4ede-9d5b-22371569fe1d"
   },
   "outputs": [],
   "source": [
    "# dataset_dicts = get_wastebin_dicts(TRAIN_PATH)\n",
    "if FIRST_TIME:\n",
    "    dataset_dicts = DatasetCatalog.get(\"dataset_train_reduced\")\n",
    "    metadata = MetadataCatalog.get(\"dataset_train_reduced\")\n",
    "    for d in random.sample(dataset_dicts, 10):\n",
    "        img = cv2.imread(d[\"file_name\"])\n",
    "        \n",
    "        visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, scale=0.5)\n",
    "        out = visualizer.draw_dataset_dict(d)\n",
    "        cv2_imshow(out.get_image()[:, :, ::-1], 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dicts2 = DatasetCatalog.get(\"dataset_train_reduced2\")\n",
    "# print(len(dataset_dicts2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlqXIXXhW8dA"
   },
   "source": [
    "## Train!\n",
    "\n",
    "Now, let's fine-tune a COCO-pretrained R50-FPN Mask R-CNN model on the balloon dataset. It takes ~2 minutes to train 300 iterations on a P100 GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDNN_BENCHMARK: False\n",
      "DATALOADER:\n",
      "  ASPECT_RATIO_GROUPING: True\n",
      "  FILTER_EMPTY_ANNOTATIONS: True\n",
      "  NUM_WORKERS: 4\n",
      "  REPEAT_THRESHOLD: 0.0\n",
      "  SAMPLER_TRAIN: TrainingSampler\n",
      "DATASETS:\n",
      "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
      "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
      "  PROPOSAL_FILES_TEST: ()\n",
      "  PROPOSAL_FILES_TRAIN: ()\n",
      "  TEST: ('coco_2017_val',)\n",
      "  TRAIN: ('coco_2017_train',)\n",
      "GLOBAL:\n",
      "  HACK: 1.0\n",
      "INPUT:\n",
      "  CROP:\n",
      "    ENABLED: False\n",
      "    SIZE: [0.9, 0.9]\n",
      "    TYPE: relative_range\n",
      "  FORMAT: BGR\n",
      "  MASK_FORMAT: polygon\n",
      "  MAX_SIZE_TEST: 1333\n",
      "  MAX_SIZE_TRAIN: 1333\n",
      "  MIN_SIZE_TEST: 800\n",
      "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
      "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
      "  RANDOM_FLIP: horizontal\n",
      "MODEL:\n",
      "  ANCHOR_GENERATOR:\n",
      "    ANGLES: [[-90, 0, 90]]\n",
      "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
      "    NAME: DefaultAnchorGenerator\n",
      "    OFFSET: 0.0\n",
      "    SIZES: [[32], [64], [128], [256], [512]]\n",
      "  BACKBONE:\n",
      "    FREEZE_AT: 2\n",
      "    NAME: build_resnet_fpn_backbone\n",
      "  DEVICE: cuda\n",
      "  FPN:\n",
      "    FUSE_TYPE: sum\n",
      "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
      "    NORM: \n",
      "    OUT_CHANNELS: 256\n",
      "  KEYPOINT_ON: False\n",
      "  LOAD_PROPOSALS: False\n",
      "  MASK_ON: False\n",
      "  META_ARCHITECTURE: GeneralizedRCNN\n",
      "  PANOPTIC_FPN:\n",
      "    COMBINE:\n",
      "      ENABLED: True\n",
      "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
      "      OVERLAP_THRESH: 0.5\n",
      "      STUFF_AREA_LIMIT: 4096\n",
      "    INSTANCE_LOSS_WEIGHT: 1.0\n",
      "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
      "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
      "  PROPOSAL_GENERATOR:\n",
      "    MIN_SIZE: 0\n",
      "    NAME: RPN\n",
      "  RESNETS:\n",
      "    DEFORM_MODULATED: False\n",
      "    DEFORM_NUM_GROUPS: 1\n",
      "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
      "    DEPTH: 50\n",
      "    NORM: FrozenBN\n",
      "    NUM_GROUPS: 1\n",
      "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
      "    RES2_OUT_CHANNELS: 256\n",
      "    RES5_DILATION: 1\n",
      "    STEM_OUT_CHANNELS: 64\n",
      "    STRIDE_IN_1X1: True\n",
      "    WIDTH_PER_GROUP: 64\n",
      "  RETINANET:\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
      "    FOCAL_LOSS_ALPHA: 0.25\n",
      "    FOCAL_LOSS_GAMMA: 2.0\n",
      "    IN_FEATURES: ['p3', 'p4', 'p5', 'p6', 'p7']\n",
      "    IOU_LABELS: [0, -1, 1]\n",
      "    IOU_THRESHOLDS: [0.4, 0.5]\n",
      "    NMS_THRESH_TEST: 0.5\n",
      "    NORM: \n",
      "    NUM_CLASSES: 80\n",
      "    NUM_CONVS: 4\n",
      "    PRIOR_PROB: 0.01\n",
      "    SCORE_THRESH_TEST: 0.05\n",
      "    SMOOTH_L1_LOSS_BETA: 0.1\n",
      "    TOPK_CANDIDATES_TEST: 1000\n",
      "  ROI_BOX_CASCADE_HEAD:\n",
      "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
      "    IOUS: (0.5, 0.6, 0.7)\n",
      "  ROI_BOX_HEAD:\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
      "    CLS_AGNOSTIC_BBOX_REG: False\n",
      "    CONV_DIM: 256\n",
      "    FC_DIM: 1024\n",
      "    FED_LOSS_FREQ_WEIGHT_POWER: 0.5\n",
      "    FED_LOSS_NUM_CLASSES: 50\n",
      "    NAME: FastRCNNConvFCHead\n",
      "    NORM: \n",
      "    NUM_CONV: 0\n",
      "    NUM_FC: 2\n",
      "    POOLER_RESOLUTION: 7\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "    SMOOTH_L1_BETA: 0.0\n",
      "    TRAIN_ON_PRED_BOXES: False\n",
      "    USE_FED_LOSS: False\n",
      "    USE_SIGMOID_CE: False\n",
      "  ROI_HEADS:\n",
      "    BATCH_SIZE_PER_IMAGE: 512\n",
      "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
      "    IOU_LABELS: [0, 1]\n",
      "    IOU_THRESHOLDS: [0.5]\n",
      "    NAME: StandardROIHeads\n",
      "    NMS_THRESH_TEST: 0.5\n",
      "    NUM_CLASSES: 80\n",
      "    POSITIVE_FRACTION: 0.25\n",
      "    PROPOSAL_APPEND_GT: True\n",
      "    SCORE_THRESH_TEST: 0.05\n",
      "  ROI_KEYPOINT_HEAD:\n",
      "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
      "    NAME: KRCNNConvDeconvUpsampleHead\n",
      "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
      "    NUM_KEYPOINTS: 17\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "  ROI_MASK_HEAD:\n",
      "    CLS_AGNOSTIC_MASK: False\n",
      "    CONV_DIM: 256\n",
      "    NAME: MaskRCNNConvUpsampleHead\n",
      "    NORM: \n",
      "    NUM_CONV: 4\n",
      "    POOLER_RESOLUTION: 14\n",
      "    POOLER_SAMPLING_RATIO: 0\n",
      "    POOLER_TYPE: ROIAlignV2\n",
      "  RPN:\n",
      "    BATCH_SIZE_PER_IMAGE: 256\n",
      "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
      "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
      "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
      "    BOUNDARY_THRESH: -1\n",
      "    CONV_DIMS: [-1]\n",
      "    HEAD_NAME: StandardRPNHead\n",
      "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6']\n",
      "    IOU_LABELS: [0, -1, 1]\n",
      "    IOU_THRESHOLDS: [0.3, 0.7]\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    NMS_THRESH: 0.7\n",
      "    POSITIVE_FRACTION: 0.5\n",
      "    POST_NMS_TOPK_TEST: 1000\n",
      "    POST_NMS_TOPK_TRAIN: 1000\n",
      "    PRE_NMS_TOPK_TEST: 1000\n",
      "    PRE_NMS_TOPK_TRAIN: 2000\n",
      "    SMOOTH_L1_BETA: 0.0\n",
      "  SEM_SEG_HEAD:\n",
      "    COMMON_STRIDE: 4\n",
      "    CONVS_DIM: 128\n",
      "    IGNORE_VALUE: 255\n",
      "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
      "    LOSS_WEIGHT: 1.0\n",
      "    NAME: SemSegFPNHead\n",
      "    NORM: GN\n",
      "    NUM_CLASSES: 54\n",
      "  WEIGHTS: detectron2://ImageNetPretrained/MSRA/R-50.pkl\n",
      "OUTPUT_DIR: ./output\n",
      "SEED: -1\n",
      "SOLVER:\n",
      "  AMP:\n",
      "    ENABLED: False\n",
      "  BASE_LR: 0.02\n",
      "  BASE_LR_END: 0.0\n",
      "  BIAS_LR_FACTOR: 1.0\n",
      "  CHECKPOINT_PERIOD: 5000\n",
      "  CLIP_GRADIENTS:\n",
      "    CLIP_TYPE: value\n",
      "    CLIP_VALUE: 1.0\n",
      "    ENABLED: False\n",
      "    NORM_TYPE: 2.0\n",
      "  GAMMA: 0.1\n",
      "  IMS_PER_BATCH: 16\n",
      "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
      "  MAX_ITER: 270000\n",
      "  MOMENTUM: 0.9\n",
      "  NESTEROV: False\n",
      "  NUM_DECAYS: 3\n",
      "  REFERENCE_WORLD_SIZE: 0\n",
      "  RESCALE_INTERVAL: False\n",
      "  STEPS: (210000, 250000)\n",
      "  WARMUP_FACTOR: 0.001\n",
      "  WARMUP_ITERS: 1000\n",
      "  WARMUP_METHOD: linear\n",
      "  WEIGHT_DECAY: 0.0001\n",
      "  WEIGHT_DECAY_BIAS: None\n",
      "  WEIGHT_DECAY_NORM: 0.0\n",
      "TEST:\n",
      "  AUG:\n",
      "    ENABLED: False\n",
      "    FLIP: True\n",
      "    MAX_SIZE: 4000\n",
      "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
      "  DETECTIONS_PER_IMAGE: 100\n",
      "  EVAL_PERIOD: 0\n",
      "  EXPECTED_RESULTS: []\n",
      "  KEYPOINT_OKS_SIGMAS: []\n",
      "  PRECISE_BN:\n",
      "    ENABLED: False\n",
      "    NUM_ITER: 200\n",
      "VERSION: 2\n",
      "VIS_PERIOD: 0\n"
     ]
    }
   ],
   "source": [
    "cfg1 = get_cfg()\n",
    "cfg1.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "print(cfg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7unkuuiqLdqd",
    "outputId": "ba1716cd-3f3b-401d-bae5-8fbbd2199d9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/09 04:55:08 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[32m[06/09 04:55:08 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[06/09 04:55:08 d2.data.datasets.coco]: \u001b[0mLoaded 8 images in COCO format from ../data/generated_data_ad/new/2021_02_18_06_28/merged_labels.json\n",
      "\u001b[32m[06/09 04:55:08 d2.data.datasets.coco]: \u001b[0mLoaded 32 images in COCO format from ../data/generated_data_ad/2021_03_25_14_04/merged_labels.json\n",
      "\u001b[32m[06/09 04:55:08 d2.data.datasets.coco]: \u001b[0mLoaded 14 images in COCO format from ../data/generated_data_ad/new/2021_04_05_14_35/merged_labels.json\n",
      "\u001b[32m[06/09 04:55:08 d2.data.datasets.coco]: \u001b[0mLoaded 3 images in COCO format from ../data/generated_data_ad/new/2021_05_18_14_02/merged_labels.json\n",
      "\u001b[32m[06/09 04:55:08 d2.data.datasets.coco]: \u001b[0mLoaded 4 images in COCO format from ../data/generated_data_ad/new/2021_06_05_12_08/merged_labels.json\n",
      "\u001b[32m[06/09 04:55:08 d2.data.datasets.coco]: \u001b[0mLoaded 3 images in COCO format from ../data/generated_data_ad/new/2021_07_07_06_41/merged_labels.json\n",
      "\u001b[32m[06/09 04:55:08 d2.data.datasets.coco]: \u001b[0mLoaded 6 images in COCO format from ../data/generated_data_ad/new/2021_08_09_06_30/merged_labels.json\n",
      "\u001b[32m[06/09 04:55:08 d2.data.datasets.coco]: \u001b[0mLoaded 10 images in COCO format from ../data/generated_data_ad/new/2021_09_15_06_28/merged_labels.json\n",
      "\u001b[32m[06/09 04:55:08 d2.data.datasets.coco]: \u001b[0mLoaded 1 images in COCO format from ../data/generated_data_ad/new/2021_10_15_18_16/merged_labels.json\n",
      "\u001b[32m[06/09 04:55:08 d2.data.datasets.coco]: \u001b[0mLoaded 7 images in COCO format from ../data/generated_data_ad/new/2021_11_02_12_59/merged_labels.json\n",
      "\u001b[32m[06/09 04:55:08 d2.data.datasets.coco]: \u001b[0mLoaded 10 images in COCO format from ../data/generated_data_ad/new/2021_12_15_12_54/merged_labels.json\n",
      "\u001b[32m[06/09 04:55:08 d2.data.datasets.coco]: \u001b[0mLoaded 6 images in COCO format from ../data/generated_data_ad/2022_01_21_14_04/merged_labels.json\n",
      "\u001b[32m[06/09 04:55:08 d2.data.datasets.coco]: \u001b[0mLoaded 7144 images in COCO format from ../data/Waste_Bin_Detection_Dataset/sunny_2021_03_23_14_33_cam5_filtered (training dataset images and ground truths)/sunny_reduced_static.json\n",
      "\u001b[32m[06/09 04:55:08 d2.data.build]: \u001b[0mRemoved 6940 images with no usable annotations. 308 images left.\n",
      "\u001b[32m[06/09 04:55:08 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[06/09 04:55:08 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[06/09 04:55:08 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>\n",
      "\u001b[32m[06/09 04:55:08 d2.data.common]: \u001b[0mSerializing 308 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[06/09 04:55:08 d2.data.common]: \u001b[0mSerialized dataset takes 0.12 MiB\n",
      "\u001b[32m[06/09 04:55:08 d2.data.datasets.coco]: \u001b[0mLoaded 3438 images in COCO format from ../data/Waste_Bin_Detection_Dataset/cloudy_2021_04_09_16_02_cam5_filtered (validation and test dataset images and ground truths)/validation_set_static_gt.json\n",
      "\u001b[32m[06/09 04:55:08 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[06/09 04:55:08 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>\n",
      "\u001b[32m[06/09 04:55:08 d2.data.common]: \u001b[0mSerializing 354 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[06/09 04:55:08 d2.data.common]: \u001b[0mSerialized dataset takes 0.15 MiB\n",
      "\u001b[32m[06/09 04:55:08 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/09 04:55:08 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "after_step at step 0\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 04:55:09 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 04:55:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0018 s/iter. Inference: 0.4484 s/iter. Eval: 0.0010 s/iter. Total: 0.4513 s/iter. ETA=0:00:35\n",
      "\u001b[32m[06/09 04:55:20 d2.evaluation.evaluator]: \u001b[0mInference done 22/89. Dataloading: 0.0025 s/iter. Inference: 0.4529 s/iter. Eval: 0.0010 s/iter. Total: 0.4565 s/iter. ETA=0:00:30\n",
      "\u001b[32m[06/09 04:55:25 d2.evaluation.evaluator]: \u001b[0mInference done 34/89. Dataloading: 0.0026 s/iter. Inference: 0.4488 s/iter. Eval: 0.0010 s/iter. Total: 0.4524 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 04:55:30 d2.evaluation.evaluator]: \u001b[0mInference done 45/89. Dataloading: 0.0026 s/iter. Inference: 0.4494 s/iter. Eval: 0.0010 s/iter. Total: 0.4530 s/iter. ETA=0:00:19\n",
      "\u001b[32m[06/09 04:55:35 d2.evaluation.evaluator]: \u001b[0mInference done 56/89. Dataloading: 0.0026 s/iter. Inference: 0.4501 s/iter. Eval: 0.0010 s/iter. Total: 0.4537 s/iter. ETA=0:00:14\n",
      "\u001b[32m[06/09 04:55:40 d2.evaluation.evaluator]: \u001b[0mInference done 67/89. Dataloading: 0.0027 s/iter. Inference: 0.4505 s/iter. Eval: 0.0009 s/iter. Total: 0.4543 s/iter. ETA=0:00:09\n",
      "\u001b[32m[06/09 04:55:46 d2.evaluation.evaluator]: \u001b[0mInference done 79/89. Dataloading: 0.0027 s/iter. Inference: 0.4504 s/iter. Eval: 0.0009 s/iter. Total: 0.4541 s/iter. ETA=0:00:04\n",
      "\u001b[32m[06/09 04:55:50 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.964240 (0.451955 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 04:55:50 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.447724 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 04:55:50 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 04:55:50 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 04:55:50 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.13s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 04:55:50 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 04:55:51 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.14 seconds.\n",
      "\u001b[32m[06/09 04:55:51 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 04:55:51 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.05 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.005\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.012\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.108\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.043\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.181\n",
      "\u001b[32m[06/09 04:55:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.141 | 0.629  | 0.003  | 0.156 | 0.060 | 0.152 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 0.14143079652759807, 'AP50': 0.6289959469336789, 'AP75': 0.0031060143780637086, 'APs': 0.15639541166278906, 'APm': 0.060119105365111884, 'APl': 0.15168507890201657})])\n",
      "Putting scalar for val/AP:  0.14143079652759807 at iter 0 storage at iter  0\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 04:56:06 d2.utils.events]: \u001b[0m eta: 6:52:49  iter: 19  total_loss: 0.8776  loss_cls: 0.7129  loss_box_reg: 0.1198  loss_rpn_cls: 0.04227  loss_rpn_loc: 0.007604  time: 0.8117  data_time: 0.0152  lr: 4.9953e-06  max_mem: 4944M\n",
      "\u001b[32m[06/09 04:56:23 d2.utils.events]: \u001b[0m eta: 6:52:48  iter: 39  total_loss: 0.8387  loss_cls: 0.6568  loss_box_reg: 0.1265  loss_rpn_cls: 0.05135  loss_rpn_loc: 0.007688  time: 0.8197  data_time: 0.0059  lr: 9.9902e-06  max_mem: 4944M\n",
      "\u001b[32m[06/09 04:56:39 d2.utils.events]: \u001b[0m eta: 6:52:49  iter: 59  total_loss: 0.7138  loss_cls: 0.5239  loss_box_reg: 0.1297  loss_rpn_cls: 0.04543  loss_rpn_loc: 0.008457  time: 0.8182  data_time: 0.0061  lr: 1.4985e-05  max_mem: 4944M\n",
      "\u001b[32m[06/09 04:56:56 d2.utils.events]: \u001b[0m eta: 6:53:32  iter: 79  total_loss: 0.6018  loss_cls: 0.4035  loss_box_reg: 0.1182  loss_rpn_cls: 0.046  loss_rpn_loc: 0.01376  time: 0.8216  data_time: 0.0064  lr: 1.998e-05  max_mem: 4944M\n",
      "\u001b[32m[06/09 04:57:13 d2.utils.events]: \u001b[0m eta: 6:52:16  iter: 99  total_loss: 0.5269  loss_cls: 0.319  loss_box_reg: 0.1566  loss_rpn_cls: 0.04503  loss_rpn_loc: 0.01108  time: 0.8204  data_time: 0.0070  lr: 2.4975e-05  max_mem: 4944M\n",
      "after_step at step 100\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 04:57:14 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 04:57:19 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0025 s/iter. Inference: 0.4336 s/iter. Eval: 0.0011 s/iter. Total: 0.4372 s/iter. ETA=0:00:34\n",
      "\u001b[32m[06/09 04:57:24 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0027 s/iter. Inference: 0.4407 s/iter. Eval: 0.0010 s/iter. Total: 0.4445 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 04:57:30 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0028 s/iter. Inference: 0.4407 s/iter. Eval: 0.0041 s/iter. Total: 0.4477 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 04:57:35 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0029 s/iter. Inference: 0.4418 s/iter. Eval: 0.0032 s/iter. Total: 0.4480 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 04:57:41 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0029 s/iter. Inference: 0.4413 s/iter. Eval: 0.0027 s/iter. Total: 0.4470 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 04:57:46 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0029 s/iter. Inference: 0.4419 s/iter. Eval: 0.0024 s/iter. Total: 0.4472 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 04:57:51 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0029 s/iter. Inference: 0.4417 s/iter. Eval: 0.0022 s/iter. Total: 0.4468 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 04:57:54 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.311038 (0.444179 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 04:57:54 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:36 (0.438724 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 04:57:54 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 04:57:54 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 04:57:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 04:57:54 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 04:57:54 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.13 seconds.\n",
      "\u001b[32m[06/09 04:57:54 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 04:57:54 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.04 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.011\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.016\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.080\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.305\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.132\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.399\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.501\n",
      "\u001b[32m[06/09 04:57:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.250 | 0.669  | 0.111  | 0.072 | 0.289 | 1.056 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 0.2501941093349285, 'AP50': 0.6688600206108952, 'AP75': 0.11077632512059438, 'APs': 0.0720407471368639, 'APm': 0.2885652817941044, 'APl': 1.0563011598561063})])\n",
      "Putting scalar for val/AP:  0.2501941093349285 at iter 100 storage at iter  100\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 04:58:13 d2.utils.events]: \u001b[0m eta: 6:51:06  iter: 119  total_loss: 0.4203  loss_cls: 0.2508  loss_box_reg: 0.09941  loss_rpn_cls: 0.03963  loss_rpn_loc: 0.007433  time: 0.8186  data_time: 0.0060  lr: 2.997e-05  max_mem: 4944M\n",
      "\u001b[32m[06/09 04:58:30 d2.utils.events]: \u001b[0m eta: 6:50:13  iter: 139  total_loss: 0.4196  loss_cls: 0.2239  loss_box_reg: 0.1278  loss_rpn_cls: 0.04239  loss_rpn_loc: 0.009247  time: 0.8183  data_time: 0.0083  lr: 3.4965e-05  max_mem: 4944M\n",
      "\u001b[32m[06/09 04:58:46 d2.utils.events]: \u001b[0m eta: 6:49:46  iter: 159  total_loss: 0.3713  loss_cls: 0.1991  loss_box_reg: 0.1106  loss_rpn_cls: 0.03477  loss_rpn_loc: 0.007532  time: 0.8172  data_time: 0.0074  lr: 3.996e-05  max_mem: 4944M\n",
      "\u001b[32m[06/09 04:59:02 d2.utils.events]: \u001b[0m eta: 6:49:00  iter: 179  total_loss: 0.3625  loss_cls: 0.1787  loss_box_reg: 0.1161  loss_rpn_cls: 0.04232  loss_rpn_loc: 0.01151  time: 0.8175  data_time: 0.0065  lr: 4.4955e-05  max_mem: 4944M\n",
      "\u001b[32m[06/09 04:59:18 d2.utils.events]: \u001b[0m eta: 6:48:17  iter: 199  total_loss: 0.3915  loss_cls: 0.1854  loss_box_reg: 0.1693  loss_rpn_cls: 0.03034  loss_rpn_loc: 0.008483  time: 0.8171  data_time: 0.0074  lr: 4.995e-05  max_mem: 4944M\n",
      "after_step at step 200\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 04:59:20 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 04:59:25 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0022 s/iter. Inference: 0.4410 s/iter. Eval: 0.0010 s/iter. Total: 0.4442 s/iter. ETA=0:00:34\n",
      "\u001b[32m[06/09 04:59:31 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0026 s/iter. Inference: 0.4460 s/iter. Eval: 0.0010 s/iter. Total: 0.4497 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 04:59:36 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0027 s/iter. Inference: 0.4465 s/iter. Eval: 0.0010 s/iter. Total: 0.4502 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 04:59:42 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0027 s/iter. Inference: 0.4460 s/iter. Eval: 0.0010 s/iter. Total: 0.4498 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 04:59:47 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0027 s/iter. Inference: 0.4456 s/iter. Eval: 0.0010 s/iter. Total: 0.4494 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 04:59:52 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0027 s/iter. Inference: 0.4455 s/iter. Eval: 0.0025 s/iter. Total: 0.4507 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 04:59:58 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0027 s/iter. Inference: 0.4449 s/iter. Eval: 0.0022 s/iter. Total: 0.4500 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:00:00 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.583863 (0.447427 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:00:00 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.442119 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:00:00 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:00:00 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:00:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:00:01 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:00:01 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.21 seconds.\n",
      "\u001b[32m[06/09 05:00:01 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:00:01 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.05 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.048\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.088\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.091\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.306\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.290\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.604\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.707\n",
      "\u001b[32m[06/09 05:00:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 2.704 | 4.788  | 2.416  | 0.632 | 2.771 | 8.808 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 2.7040754017611484, 'AP50': 4.788337283101019, 'AP75': 2.4155951266846443, 'APs': 0.6318144029903298, 'APm': 2.770911580188696, 'APl': 8.80840093165568})])\n",
      "Putting scalar for val/AP:  2.7040754017611484 at iter 200 storage at iter  200\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:00:19 d2.utils.events]: \u001b[0m eta: 6:47:25  iter: 219  total_loss: 0.4412  loss_cls: 0.2005  loss_box_reg: 0.1661  loss_rpn_cls: 0.03289  loss_rpn_loc: 0.009575  time: 0.8152  data_time: 0.0079  lr: 5.4945e-05  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:00:35 d2.utils.events]: \u001b[0m eta: 6:45:59  iter: 239  total_loss: 0.4627  loss_cls: 0.1903  loss_box_reg: 0.204  loss_rpn_cls: 0.03628  loss_rpn_loc: 0.01018  time: 0.8125  data_time: 0.0063  lr: 5.994e-05  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:00:51 d2.utils.events]: \u001b[0m eta: 6:44:48  iter: 259  total_loss: 0.4  loss_cls: 0.1728  loss_box_reg: 0.1721  loss_rpn_cls: 0.03318  loss_rpn_loc: 0.01103  time: 0.8108  data_time: 0.0083  lr: 6.4935e-05  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:01:08 d2.utils.events]: \u001b[0m eta: 6:45:19  iter: 279  total_loss: 0.3517  loss_cls: 0.1497  loss_box_reg: 0.1685  loss_rpn_cls: 0.02933  loss_rpn_loc: 0.006128  time: 0.8110  data_time: 0.0069  lr: 6.993e-05  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:01:25 d2.utils.events]: \u001b[0m eta: 6:44:55  iter: 299  total_loss: 0.4444  loss_cls: 0.1744  loss_box_reg: 0.2238  loss_rpn_cls: 0.03657  loss_rpn_loc: 0.007748  time: 0.8114  data_time: 0.0082  lr: 7.4925e-05  max_mem: 4944M\n",
      "after_step at step 300\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:01:26 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:01:31 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0021 s/iter. Inference: 0.4357 s/iter. Eval: 0.0012 s/iter. Total: 0.4390 s/iter. ETA=0:00:34\n",
      "\u001b[32m[06/09 05:01:36 d2.evaluation.evaluator]: \u001b[0mInference done 22/89. Dataloading: 0.0026 s/iter. Inference: 0.4454 s/iter. Eval: 0.0011 s/iter. Total: 0.4491 s/iter. ETA=0:00:30\n",
      "\u001b[32m[06/09 05:01:42 d2.evaluation.evaluator]: \u001b[0mInference done 34/89. Dataloading: 0.0027 s/iter. Inference: 0.4452 s/iter. Eval: 0.0010 s/iter. Total: 0.4490 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:01:47 d2.evaluation.evaluator]: \u001b[0mInference done 46/89. Dataloading: 0.0028 s/iter. Inference: 0.4460 s/iter. Eval: 0.0010 s/iter. Total: 0.4499 s/iter. ETA=0:00:19\n",
      "\u001b[32m[06/09 05:01:52 d2.evaluation.evaluator]: \u001b[0mInference done 58/89. Dataloading: 0.0028 s/iter. Inference: 0.4466 s/iter. Eval: 0.0010 s/iter. Total: 0.4504 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:01:58 d2.evaluation.evaluator]: \u001b[0mInference done 70/89. Dataloading: 0.0028 s/iter. Inference: 0.4462 s/iter. Eval: 0.0010 s/iter. Total: 0.4500 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:02:03 d2.evaluation.evaluator]: \u001b[0mInference done 82/89. Dataloading: 0.0028 s/iter. Inference: 0.4462 s/iter. Eval: 0.0010 s/iter. Total: 0.4501 s/iter. ETA=0:00:03\n",
      "\u001b[32m[06/09 05:02:06 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.616949 (0.447821 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:02:06 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.443404 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:02:06 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:02:06 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:02:06 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.12s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:02:07 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:02:07 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.14 seconds.\n",
      "\u001b[32m[06/09 05:02:07 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:02:07 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.05 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.234\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.346\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.251\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.081\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.294\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.262\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.490\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.565\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.392\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.653\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.780\n",
      "\u001b[32m[06/09 05:02:07 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 23.415 | 34.602 | 25.103 | 8.144 | 29.434 | 52.856 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 23.414619408922963, 'AP50': 34.60225991312108, 'AP75': 25.102834292821097, 'APs': 8.143523295606572, 'APm': 29.433857073399466, 'APl': 52.856434649959404})])\n",
      "Putting scalar for val/AP:  23.414619408922963 at iter 300 storage at iter  300\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:02:26 d2.utils.events]: \u001b[0m eta: 6:44:11  iter: 319  total_loss: 0.4302  loss_cls: 0.1686  loss_box_reg: 0.2312  loss_rpn_cls: 0.02679  loss_rpn_loc: 0.007103  time: 0.8106  data_time: 0.0081  lr: 7.992e-05  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:02:42 d2.utils.events]: \u001b[0m eta: 6:43:39  iter: 339  total_loss: 0.4086  loss_cls: 0.145  loss_box_reg: 0.2089  loss_rpn_cls: 0.03291  loss_rpn_loc: 0.008596  time: 0.8105  data_time: 0.0086  lr: 8.4915e-05  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:02:59 d2.utils.events]: \u001b[0m eta: 6:43:06  iter: 359  total_loss: 0.4182  loss_cls: 0.1436  loss_box_reg: 0.2059  loss_rpn_cls: 0.02708  loss_rpn_loc: 0.008043  time: 0.8103  data_time: 0.0085  lr: 8.991e-05  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:03:15 d2.utils.events]: \u001b[0m eta: 6:43:06  iter: 379  total_loss: 0.3855  loss_cls: 0.149  loss_box_reg: 0.2039  loss_rpn_cls: 0.029  loss_rpn_loc: 0.01136  time: 0.8107  data_time: 0.0089  lr: 9.4905e-05  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:03:33 d2.utils.events]: \u001b[0m eta: 6:42:56  iter: 399  total_loss: 0.3751  loss_cls: 0.1268  loss_box_reg: 0.2119  loss_rpn_cls: 0.02739  loss_rpn_loc: 0.005291  time: 0.8108  data_time: 0.0081  lr: 9.99e-05  max_mem: 4944M\n",
      "after_step at step 400\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:03:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:03:39 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0019 s/iter. Inference: 0.4466 s/iter. Eval: 0.0009 s/iter. Total: 0.4493 s/iter. ETA=0:00:35\n",
      "\u001b[32m[06/09 05:03:44 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0025 s/iter. Inference: 0.4476 s/iter. Eval: 0.0010 s/iter. Total: 0.4512 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:03:49 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0027 s/iter. Inference: 0.4473 s/iter. Eval: 0.0010 s/iter. Total: 0.4510 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:03:55 d2.evaluation.evaluator]: \u001b[0mInference done 46/89. Dataloading: 0.0027 s/iter. Inference: 0.4480 s/iter. Eval: 0.0033 s/iter. Total: 0.4541 s/iter. ETA=0:00:19\n",
      "\u001b[32m[06/09 05:04:00 d2.evaluation.evaluator]: \u001b[0mInference done 58/89. Dataloading: 0.0027 s/iter. Inference: 0.4476 s/iter. Eval: 0.0028 s/iter. Total: 0.4531 s/iter. ETA=0:00:14\n",
      "\u001b[32m[06/09 05:04:05 d2.evaluation.evaluator]: \u001b[0mInference done 70/89. Dataloading: 0.0027 s/iter. Inference: 0.4467 s/iter. Eval: 0.0024 s/iter. Total: 0.4519 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:04:11 d2.evaluation.evaluator]: \u001b[0mInference done 82/89. Dataloading: 0.0028 s/iter. Inference: 0.4472 s/iter. Eval: 0.0022 s/iter. Total: 0.4522 s/iter. ETA=0:00:03\n",
      "\u001b[32m[06/09 05:04:14 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.723015 (0.449084 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:04:14 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.443769 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:04:14 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:04:14 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:04:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:04:14 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:04:14 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.12 seconds.\n",
      "\u001b[32m[06/09 05:04:14 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:04:14 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.04 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.342\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.506\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.393\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.165\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.420\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.612\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.342\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.554\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.591\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.433\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.667\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.804\n",
      "\u001b[32m[06/09 05:04:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 34.166 | 50.584 | 39.255 | 16.549 | 42.008 | 61.193 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 34.166490922013764, 'AP50': 50.58406235461449, 'AP75': 39.25504719231926, 'APs': 16.54862711499925, 'APm': 42.00775591777381, 'APl': 61.19310225670678})])\n",
      "Putting scalar for val/AP:  34.166490922013764 at iter 400 storage at iter  400\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:04:33 d2.utils.events]: \u001b[0m eta: 6:42:55  iter: 419  total_loss: 0.4713  loss_cls: 0.1354  loss_box_reg: 0.2608  loss_rpn_cls: 0.02408  loss_rpn_loc: 0.0106  time: 0.8111  data_time: 0.0077  lr: 0.0001049  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:04:50 d2.utils.events]: \u001b[0m eta: 6:42:53  iter: 439  total_loss: 0.381  loss_cls: 0.1225  loss_box_reg: 0.2162  loss_rpn_cls: 0.02274  loss_rpn_loc: 0.009277  time: 0.8113  data_time: 0.0066  lr: 0.00010989  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:05:06 d2.utils.events]: \u001b[0m eta: 6:42:51  iter: 459  total_loss: 0.385  loss_cls: 0.1277  loss_box_reg: 0.2024  loss_rpn_cls: 0.02798  loss_rpn_loc: 0.01103  time: 0.8120  data_time: 0.0067  lr: 0.00011489  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:05:23 d2.utils.events]: \u001b[0m eta: 6:42:48  iter: 479  total_loss: 0.3777  loss_cls: 0.1044  loss_box_reg: 0.23  loss_rpn_cls: 0.02429  loss_rpn_loc: 0.01097  time: 0.8127  data_time: 0.0065  lr: 0.00011988  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:05:40 d2.utils.events]: \u001b[0m eta: 6:42:44  iter: 499  total_loss: 0.3496  loss_cls: 0.09623  loss_box_reg: 0.2113  loss_rpn_cls: 0.02429  loss_rpn_loc: 0.005139  time: 0.8137  data_time: 0.0067  lr: 0.00012488  max_mem: 4944M\n",
      "after_step at step 500\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:05:41 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:05:47 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0024 s/iter. Inference: 0.4671 s/iter. Eval: 0.0008 s/iter. Total: 0.4703 s/iter. ETA=0:00:36\n",
      "\u001b[32m[06/09 05:05:52 d2.evaluation.evaluator]: \u001b[0mInference done 22/89. Dataloading: 0.0027 s/iter. Inference: 0.4628 s/iter. Eval: 0.0008 s/iter. Total: 0.4665 s/iter. ETA=0:00:31\n",
      "\u001b[32m[06/09 05:05:57 d2.evaluation.evaluator]: \u001b[0mInference done 33/89. Dataloading: 0.0028 s/iter. Inference: 0.4583 s/iter. Eval: 0.0008 s/iter. Total: 0.4620 s/iter. ETA=0:00:25\n",
      "\u001b[32m[06/09 05:06:02 d2.evaluation.evaluator]: \u001b[0mInference done 44/89. Dataloading: 0.0028 s/iter. Inference: 0.4593 s/iter. Eval: 0.0009 s/iter. Total: 0.4631 s/iter. ETA=0:00:20\n",
      "\u001b[32m[06/09 05:06:07 d2.evaluation.evaluator]: \u001b[0mInference done 55/89. Dataloading: 0.0028 s/iter. Inference: 0.4589 s/iter. Eval: 0.0009 s/iter. Total: 0.4627 s/iter. ETA=0:00:15\n",
      "\u001b[32m[06/09 05:06:12 d2.evaluation.evaluator]: \u001b[0mInference done 66/89. Dataloading: 0.0028 s/iter. Inference: 0.4582 s/iter. Eval: 0.0009 s/iter. Total: 0.4620 s/iter. ETA=0:00:10\n",
      "\u001b[32m[06/09 05:06:17 d2.evaluation.evaluator]: \u001b[0mInference done 77/89. Dataloading: 0.0028 s/iter. Inference: 0.4575 s/iter. Eval: 0.0009 s/iter. Total: 0.4613 s/iter. ETA=0:00:05\n",
      "\u001b[32m[06/09 05:06:22 d2.evaluation.evaluator]: \u001b[0mInference done 88/89. Dataloading: 0.0028 s/iter. Inference: 0.4576 s/iter. Eval: 0.0009 s/iter. Total: 0.4614 s/iter. ETA=0:00:00\n",
      "\u001b[32m[06/09 05:06:22 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:38.559260 (0.459039 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:06:22 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:38 (0.454567 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:06:22 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:06:22 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:06:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.10s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:06:23 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:06:23 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.10 seconds.\n",
      "\u001b[32m[06/09 05:06:23 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:06:23 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.03 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.382\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.568\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.446\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.201\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.464\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.631\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.376\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.584\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.621\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.475\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.685\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831\n",
      "\u001b[32m[06/09 05:06:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 38.191 | 56.837 | 44.636 | 20.125 | 46.354 | 63.110 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 38.19100603477721, 'AP50': 56.837436701016905, 'AP75': 44.63626058209798, 'APs': 20.12516042960945, 'APm': 46.35401255898709, 'APl': 63.10963255342013})])\n",
      "Putting scalar for val/AP:  38.19100603477721 at iter 500 storage at iter  500\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:06:42 d2.utils.events]: \u001b[0m eta: 6:42:43  iter: 519  total_loss: 0.4128  loss_cls: 0.1142  loss_box_reg: 0.2482  loss_rpn_cls: 0.01799  loss_rpn_loc: 0.007057  time: 0.8142  data_time: 0.0065  lr: 0.00012987  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:07:00 d2.utils.events]: \u001b[0m eta: 6:42:40  iter: 539  total_loss: 0.3937  loss_cls: 0.105  loss_box_reg: 0.2448  loss_rpn_cls: 0.01936  loss_rpn_loc: 0.007383  time: 0.8148  data_time: 0.0081  lr: 0.00013487  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:07:17 d2.utils.events]: \u001b[0m eta: 6:42:46  iter: 559  total_loss: 0.3449  loss_cls: 0.1011  loss_box_reg: 0.2205  loss_rpn_cls: 0.02138  loss_rpn_loc: 0.009954  time: 0.8156  data_time: 0.0070  lr: 0.00013986  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:07:34 d2.utils.events]: \u001b[0m eta: 6:42:46  iter: 579  total_loss: 0.3931  loss_cls: 0.1098  loss_box_reg: 0.2497  loss_rpn_cls: 0.01894  loss_rpn_loc: 0.008158  time: 0.8164  data_time: 0.0080  lr: 0.00014486  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:07:51 d2.utils.events]: \u001b[0m eta: 6:42:48  iter: 599  total_loss: 0.3382  loss_cls: 0.0889  loss_box_reg: 0.212  loss_rpn_cls: 0.02041  loss_rpn_loc: 0.004955  time: 0.8168  data_time: 0.0081  lr: 0.00014985  max_mem: 4944M\n",
      "after_step at step 600\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:07:51 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:07:57 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0022 s/iter. Inference: 0.4582 s/iter. Eval: 0.0007 s/iter. Total: 0.4611 s/iter. ETA=0:00:35\n",
      "\u001b[32m[06/09 05:08:02 d2.evaluation.evaluator]: \u001b[0mInference done 22/89. Dataloading: 0.0026 s/iter. Inference: 0.4607 s/iter. Eval: 0.0008 s/iter. Total: 0.4642 s/iter. ETA=0:00:31\n",
      "\u001b[32m[06/09 05:08:07 d2.evaluation.evaluator]: \u001b[0mInference done 33/89. Dataloading: 0.0027 s/iter. Inference: 0.4619 s/iter. Eval: 0.0008 s/iter. Total: 0.4655 s/iter. ETA=0:00:26\n",
      "\u001b[32m[06/09 05:08:12 d2.evaluation.evaluator]: \u001b[0mInference done 44/89. Dataloading: 0.0028 s/iter. Inference: 0.4611 s/iter. Eval: 0.0008 s/iter. Total: 0.4648 s/iter. ETA=0:00:20\n",
      "\u001b[32m[06/09 05:08:17 d2.evaluation.evaluator]: \u001b[0mInference done 55/89. Dataloading: 0.0028 s/iter. Inference: 0.4595 s/iter. Eval: 0.0008 s/iter. Total: 0.4632 s/iter. ETA=0:00:15\n",
      "\u001b[32m[06/09 05:08:22 d2.evaluation.evaluator]: \u001b[0mInference done 66/89. Dataloading: 0.0028 s/iter. Inference: 0.4590 s/iter. Eval: 0.0008 s/iter. Total: 0.4627 s/iter. ETA=0:00:10\n",
      "\u001b[32m[06/09 05:08:27 d2.evaluation.evaluator]: \u001b[0mInference done 77/89. Dataloading: 0.0028 s/iter. Inference: 0.4583 s/iter. Eval: 0.0008 s/iter. Total: 0.4619 s/iter. ETA=0:00:05\n",
      "\u001b[32m[06/09 05:08:32 d2.evaluation.evaluator]: \u001b[0mInference done 88/89. Dataloading: 0.0028 s/iter. Inference: 0.4583 s/iter. Eval: 0.0008 s/iter. Total: 0.4619 s/iter. ETA=0:00:00\n",
      "\u001b[32m[06/09 05:08:33 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:38.565672 (0.459115 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:08:33 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:38 (0.455095 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:08:33 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:08:33 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:08:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:08:33 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:08:33 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.16 seconds.\n",
      "\u001b[32m[06/09 05:08:33 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:08:33 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.02 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.427\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.620\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.508\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.251\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.505\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.666\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.399\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.620\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.645\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.511\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.707\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.830\n",
      "\u001b[32m[06/09 05:08:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 42.652 | 62.029 | 50.816 | 25.093 | 50.505 | 66.595 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 42.65233976733626, 'AP50': 62.02898158603213, 'AP75': 50.815741457832395, 'APs': 25.092953716919602, 'APm': 50.50534331877431, 'APl': 66.59511033438068})])\n",
      "Putting scalar for val/AP:  42.65233976733626 at iter 600 storage at iter  600\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:08:53 d2.utils.events]: \u001b[0m eta: 6:42:35  iter: 619  total_loss: 0.3638  loss_cls: 0.08776  loss_box_reg: 0.2365  loss_rpn_cls: 0.0201  loss_rpn_loc: 0.007094  time: 0.8168  data_time: 0.0075  lr: 0.00015485  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:09:10 d2.utils.events]: \u001b[0m eta: 6:42:22  iter: 639  total_loss: 0.3486  loss_cls: 0.0874  loss_box_reg: 0.2213  loss_rpn_cls: 0.01558  loss_rpn_loc: 0.01054  time: 0.8170  data_time: 0.0066  lr: 0.00015984  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:09:26 d2.utils.events]: \u001b[0m eta: 6:42:12  iter: 659  total_loss: 0.3389  loss_cls: 0.09676  loss_box_reg: 0.2246  loss_rpn_cls: 0.02334  loss_rpn_loc: 0.009169  time: 0.8170  data_time: 0.0074  lr: 0.00016484  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:09:42 d2.utils.events]: \u001b[0m eta: 6:42:05  iter: 679  total_loss: 0.3694  loss_cls: 0.08595  loss_box_reg: 0.2252  loss_rpn_cls: 0.0143  loss_rpn_loc: 0.006207  time: 0.8172  data_time: 0.0061  lr: 0.00016983  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:09:59 d2.utils.events]: \u001b[0m eta: 6:42:09  iter: 699  total_loss: 0.3114  loss_cls: 0.08226  loss_box_reg: 0.1913  loss_rpn_cls: 0.01704  loss_rpn_loc: 0.008123  time: 0.8175  data_time: 0.0071  lr: 0.00017483  max_mem: 4944M\n",
      "after_step at step 700\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:10:01 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:10:06 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0020 s/iter. Inference: 0.4524 s/iter. Eval: 0.0007 s/iter. Total: 0.4551 s/iter. ETA=0:00:35\n",
      "\u001b[32m[06/09 05:10:12 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0026 s/iter. Inference: 0.4488 s/iter. Eval: 0.0007 s/iter. Total: 0.4522 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:10:17 d2.evaluation.evaluator]: \u001b[0mInference done 34/89. Dataloading: 0.0027 s/iter. Inference: 0.4512 s/iter. Eval: 0.0007 s/iter. Total: 0.4547 s/iter. ETA=0:00:25\n",
      "\u001b[32m[06/09 05:10:22 d2.evaluation.evaluator]: \u001b[0mInference done 45/89. Dataloading: 0.0028 s/iter. Inference: 0.4526 s/iter. Eval: 0.0007 s/iter. Total: 0.4562 s/iter. ETA=0:00:20\n",
      "\u001b[32m[06/09 05:10:27 d2.evaluation.evaluator]: \u001b[0mInference done 56/89. Dataloading: 0.0028 s/iter. Inference: 0.4528 s/iter. Eval: 0.0007 s/iter. Total: 0.4564 s/iter. ETA=0:00:15\n",
      "\u001b[32m[06/09 05:10:32 d2.evaluation.evaluator]: \u001b[0mInference done 67/89. Dataloading: 0.0028 s/iter. Inference: 0.4537 s/iter. Eval: 0.0007 s/iter. Total: 0.4573 s/iter. ETA=0:00:10\n",
      "\u001b[32m[06/09 05:10:37 d2.evaluation.evaluator]: \u001b[0mInference done 78/89. Dataloading: 0.0028 s/iter. Inference: 0.4547 s/iter. Eval: 0.0007 s/iter. Total: 0.4583 s/iter. ETA=0:00:05\n",
      "\u001b[32m[06/09 05:10:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:38.333156 (0.456347 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:10:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.452280 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:10:42 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:10:42 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:10:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:10:42 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:10:42 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "\u001b[32m[06/09 05:10:42 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:10:42 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.02 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.464\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.656\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.550\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.294\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.539\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.701\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.425\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.648\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.668\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.542\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.726\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.844\n",
      "\u001b[32m[06/09 05:10:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 46.426 | 65.587 | 55.000 | 29.445 | 53.877 | 70.077 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 46.42580258931067, 'AP50': 65.5870903229167, 'AP75': 54.99961723230216, 'APs': 29.445436968468336, 'APm': 53.87681809503376, 'APl': 70.0767160842552})])\n",
      "Putting scalar for val/AP:  46.42580258931067 at iter 700 storage at iter  700\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:11:01 d2.utils.events]: \u001b[0m eta: 6:41:56  iter: 719  total_loss: 0.3149  loss_cls: 0.07754  loss_box_reg: 0.2027  loss_rpn_cls: 0.01344  loss_rpn_loc: 0.00932  time: 0.8175  data_time: 0.0068  lr: 0.00017982  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:11:18 d2.utils.events]: \u001b[0m eta: 6:41:41  iter: 739  total_loss: 0.2778  loss_cls: 0.0706  loss_box_reg: 0.1395  loss_rpn_cls: 0.02086  loss_rpn_loc: 0.006874  time: 0.8173  data_time: 0.0061  lr: 0.00018482  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:11:34 d2.utils.events]: \u001b[0m eta: 6:41:30  iter: 759  total_loss: 0.3037  loss_cls: 0.09232  loss_box_reg: 0.1754  loss_rpn_cls: 0.01704  loss_rpn_loc: 0.008399  time: 0.8176  data_time: 0.0064  lr: 0.00018981  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:11:51 d2.utils.events]: \u001b[0m eta: 6:41:24  iter: 779  total_loss: 0.2842  loss_cls: 0.07498  loss_box_reg: 0.181  loss_rpn_cls: 0.0218  loss_rpn_loc: 0.007571  time: 0.8179  data_time: 0.0067  lr: 0.00019481  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:12:08 d2.utils.events]: \u001b[0m eta: 6:41:14  iter: 799  total_loss: 0.2977  loss_cls: 0.09152  loss_box_reg: 0.1788  loss_rpn_cls: 0.01755  loss_rpn_loc: 0.009072  time: 0.8182  data_time: 0.0062  lr: 0.0001998  max_mem: 4944M\n",
      "after_step at step 800\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:12:09 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:12:14 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0021 s/iter. Inference: 0.4481 s/iter. Eval: 0.0006 s/iter. Total: 0.4509 s/iter. ETA=0:00:35\n",
      "\u001b[32m[06/09 05:12:19 d2.evaluation.evaluator]: \u001b[0mInference done 22/89. Dataloading: 0.0026 s/iter. Inference: 0.4517 s/iter. Eval: 0.0007 s/iter. Total: 0.4551 s/iter. ETA=0:00:30\n",
      "\u001b[32m[06/09 05:12:24 d2.evaluation.evaluator]: \u001b[0mInference done 33/89. Dataloading: 0.0028 s/iter. Inference: 0.4540 s/iter. Eval: 0.0007 s/iter. Total: 0.4576 s/iter. ETA=0:00:25\n",
      "\u001b[32m[06/09 05:12:29 d2.evaluation.evaluator]: \u001b[0mInference done 44/89. Dataloading: 0.0028 s/iter. Inference: 0.4548 s/iter. Eval: 0.0007 s/iter. Total: 0.4584 s/iter. ETA=0:00:20\n",
      "\u001b[32m[06/09 05:12:35 d2.evaluation.evaluator]: \u001b[0mInference done 55/89. Dataloading: 0.0028 s/iter. Inference: 0.4550 s/iter. Eval: 0.0007 s/iter. Total: 0.4586 s/iter. ETA=0:00:15\n",
      "\u001b[32m[06/09 05:12:40 d2.evaluation.evaluator]: \u001b[0mInference done 67/89. Dataloading: 0.0028 s/iter. Inference: 0.4541 s/iter. Eval: 0.0007 s/iter. Total: 0.4577 s/iter. ETA=0:00:10\n",
      "\u001b[32m[06/09 05:12:45 d2.evaluation.evaluator]: \u001b[0mInference done 79/89. Dataloading: 0.0029 s/iter. Inference: 0.4532 s/iter. Eval: 0.0007 s/iter. Total: 0.4568 s/iter. ETA=0:00:04\n",
      "\u001b[32m[06/09 05:12:50 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:38.149244 (0.454158 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:12:50 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.450157 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:12:50 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:12:50 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:12:50 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.09s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:12:50 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:12:50 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "\u001b[32m[06/09 05:12:50 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:12:50 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.02 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.697\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.559\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.329\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.560\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.713\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.432\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.659\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.675\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.563\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.722\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.849\n",
      "\u001b[32m[06/09 05:12:50 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 48.940 | 69.721 | 55.914 | 32.941 | 56.038 | 71.262 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 48.939672497928186, 'AP50': 69.72071129202392, 'AP75': 55.913696444371475, 'APs': 32.94064707418298, 'APm': 56.03821654355778, 'APl': 71.26205707999134})])\n",
      "Putting scalar for val/AP:  48.939672497928186 at iter 800 storage at iter  800\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:13:09 d2.utils.events]: \u001b[0m eta: 6:40:52  iter: 819  total_loss: 0.2464  loss_cls: 0.07788  loss_box_reg: 0.1534  loss_rpn_cls: 0.01308  loss_rpn_loc: 0.007805  time: 0.8178  data_time: 0.0078  lr: 0.0002048  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:13:24 d2.utils.events]: \u001b[0m eta: 6:40:20  iter: 839  total_loss: 0.2344  loss_cls: 0.06944  loss_box_reg: 0.1506  loss_rpn_cls: 0.01852  loss_rpn_loc: 0.007375  time: 0.8167  data_time: 0.0054  lr: 0.00020979  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:13:40 d2.utils.events]: \u001b[0m eta: 6:39:46  iter: 859  total_loss: 0.2479  loss_cls: 0.07237  loss_box_reg: 0.1558  loss_rpn_cls: 0.01468  loss_rpn_loc: 0.008218  time: 0.8158  data_time: 0.0062  lr: 0.00021479  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:13:56 d2.utils.events]: \u001b[0m eta: 6:39:09  iter: 879  total_loss: 0.247  loss_cls: 0.06773  loss_box_reg: 0.1578  loss_rpn_cls: 0.01158  loss_rpn_loc: 0.004785  time: 0.8149  data_time: 0.0064  lr: 0.00021978  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:14:12 d2.utils.events]: \u001b[0m eta: 6:38:41  iter: 899  total_loss: 0.2709  loss_cls: 0.08187  loss_box_reg: 0.1626  loss_rpn_cls: 0.01583  loss_rpn_loc: 0.0087  time: 0.8141  data_time: 0.0066  lr: 0.00022478  max_mem: 4944M\n",
      "after_step at step 900\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:14:13 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:14:18 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0019 s/iter. Inference: 0.4312 s/iter. Eval: 0.0006 s/iter. Total: 0.4338 s/iter. ETA=0:00:33\n",
      "\u001b[32m[06/09 05:14:23 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0025 s/iter. Inference: 0.4254 s/iter. Eval: 0.0006 s/iter. Total: 0.4287 s/iter. ETA=0:00:28\n",
      "\u001b[32m[06/09 05:14:28 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0026 s/iter. Inference: 0.4224 s/iter. Eval: 0.0007 s/iter. Total: 0.4258 s/iter. ETA=0:00:22\n",
      "\u001b[32m[06/09 05:14:33 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0027 s/iter. Inference: 0.4223 s/iter. Eval: 0.0007 s/iter. Total: 0.4257 s/iter. ETA=0:00:17\n",
      "\u001b[32m[06/09 05:14:38 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0027 s/iter. Inference: 0.4224 s/iter. Eval: 0.0007 s/iter. Total: 0.4258 s/iter. ETA=0:00:12\n",
      "\u001b[32m[06/09 05:14:43 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0027 s/iter. Inference: 0.4228 s/iter. Eval: 0.0007 s/iter. Total: 0.4263 s/iter. ETA=0:00:07\n",
      "\u001b[32m[06/09 05:14:49 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0027 s/iter. Inference: 0.4236 s/iter. Eval: 0.0007 s/iter. Total: 0.4271 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:14:51 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:35.853780 (0.426831 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:14:51 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:35 (0.422826 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:14:51 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:14:51 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:14:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:14:51 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:14:51 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.14 seconds.\n",
      "\u001b[32m[06/09 05:14:51 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:14:51 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.473\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.577\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.314\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.544\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.685\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.436\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.635\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.547\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.692\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.811\n",
      "\u001b[32m[06/09 05:14:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 47.348 | 70.779 | 57.699 | 31.416 | 54.422 | 68.549 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 47.34773689793661, 'AP50': 70.7787657293172, 'AP75': 57.69885513643798, 'APs': 31.416382366163344, 'APm': 54.422358593212564, 'APl': 68.54935911440245})])\n",
      "Putting scalar for val/AP:  47.34773689793661 at iter 900 storage at iter  900\n",
      "Early stopping counter:  1 / 5\n",
      "\u001b[32m[06/09 05:15:07 d2.utils.events]: \u001b[0m eta: 6:38:25  iter: 919  total_loss: 0.2727  loss_cls: 0.07919  loss_box_reg: 0.1711  loss_rpn_cls: 0.01498  loss_rpn_loc: 0.008314  time: 0.8141  data_time: 0.0072  lr: 0.00022977  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:15:23 d2.utils.events]: \u001b[0m eta: 6:38:15  iter: 939  total_loss: 0.242  loss_cls: 0.07604  loss_box_reg: 0.1506  loss_rpn_cls: 0.01299  loss_rpn_loc: 0.006159  time: 0.8144  data_time: 0.0072  lr: 0.00023477  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:15:40 d2.utils.events]: \u001b[0m eta: 6:38:08  iter: 959  total_loss: 0.2594  loss_cls: 0.08807  loss_box_reg: 0.1592  loss_rpn_cls: 0.01225  loss_rpn_loc: 0.006581  time: 0.8148  data_time: 0.0061  lr: 0.00023976  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:15:56 d2.utils.events]: \u001b[0m eta: 6:37:41  iter: 979  total_loss: 0.2839  loss_cls: 0.07724  loss_box_reg: 0.1651  loss_rpn_cls: 0.01253  loss_rpn_loc: 0.009576  time: 0.8142  data_time: 0.0068  lr: 0.00024476  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:16:12 d2.utils.events]: \u001b[0m eta: 6:37:19  iter: 999  total_loss: 0.2264  loss_cls: 0.06426  loss_box_reg: 0.1589  loss_rpn_cls: 0.01004  loss_rpn_loc: 0.007097  time: 0.8139  data_time: 0.0074  lr: 0.00024975  max_mem: 4944M\n",
      "after_step at step 1000\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:16:13 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:16:18 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0021 s/iter. Inference: 0.4571 s/iter. Eval: 0.0007 s/iter. Total: 0.4599 s/iter. ETA=0:00:35\n",
      "\u001b[32m[06/09 05:16:23 d2.evaluation.evaluator]: \u001b[0mInference done 22/89. Dataloading: 0.0027 s/iter. Inference: 0.4568 s/iter. Eval: 0.0007 s/iter. Total: 0.4603 s/iter. ETA=0:00:30\n",
      "\u001b[32m[06/09 05:16:28 d2.evaluation.evaluator]: \u001b[0mInference done 33/89. Dataloading: 0.0028 s/iter. Inference: 0.4557 s/iter. Eval: 0.0007 s/iter. Total: 0.4592 s/iter. ETA=0:00:25\n",
      "\u001b[32m[06/09 05:16:34 d2.evaluation.evaluator]: \u001b[0mInference done 44/89. Dataloading: 0.0028 s/iter. Inference: 0.4569 s/iter. Eval: 0.0007 s/iter. Total: 0.4605 s/iter. ETA=0:00:20\n",
      "\u001b[32m[06/09 05:16:39 d2.evaluation.evaluator]: \u001b[0mInference done 55/89. Dataloading: 0.0028 s/iter. Inference: 0.4571 s/iter. Eval: 0.0007 s/iter. Total: 0.4607 s/iter. ETA=0:00:15\n",
      "\u001b[32m[06/09 05:16:44 d2.evaluation.evaluator]: \u001b[0mInference done 66/89. Dataloading: 0.0029 s/iter. Inference: 0.4567 s/iter. Eval: 0.0007 s/iter. Total: 0.4603 s/iter. ETA=0:00:10\n",
      "\u001b[32m[06/09 05:16:49 d2.evaluation.evaluator]: \u001b[0mInference done 77/89. Dataloading: 0.0029 s/iter. Inference: 0.4559 s/iter. Eval: 0.0007 s/iter. Total: 0.4596 s/iter. ETA=0:00:05\n",
      "\u001b[32m[06/09 05:16:54 d2.evaluation.evaluator]: \u001b[0mInference done 89/89. Dataloading: 0.0029 s/iter. Inference: 0.4503 s/iter. Eval: 0.0007 s/iter. Total: 0.4539 s/iter. ETA=0:00:00\n",
      "\u001b[32m[06/09 05:16:54 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:38.179681 (0.454520 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:16:54 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.450308 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:16:54 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:16:54 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:16:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:16:54 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:16:54 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.06 seconds.\n",
      "\u001b[32m[06/09 05:16:54 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:16:54 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.02 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.520\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.729\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.598\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.351\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.597\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.746\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.475\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.673\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.686\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.566\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.736\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.864\n",
      "\u001b[32m[06/09 05:16:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.004 | 72.892 | 59.838 | 35.050 | 59.731 | 74.588 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.00449294791686, 'AP50': 72.89183706347806, 'AP75': 59.837588501985906, 'APs': 35.05021550283553, 'APm': 59.73085333091592, 'APl': 74.58755517822023})])\n",
      "Putting scalar for val/AP:  52.00449294791686 at iter 1000 storage at iter  1000\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:17:13 d2.utils.events]: \u001b[0m eta: 6:36:43  iter: 1019  total_loss: 0.2505  loss_cls: 0.06929  loss_box_reg: 0.1664  loss_rpn_cls: 0.009249  loss_rpn_loc: 0.00614  time: 0.8135  data_time: 0.0087  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:17:29 d2.utils.events]: \u001b[0m eta: 6:36:20  iter: 1039  total_loss: 0.2841  loss_cls: 0.07416  loss_box_reg: 0.1689  loss_rpn_cls: 0.009879  loss_rpn_loc: 0.005673  time: 0.8136  data_time: 0.0062  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:17:46 d2.utils.events]: \u001b[0m eta: 6:36:05  iter: 1059  total_loss: 0.295  loss_cls: 0.08462  loss_box_reg: 0.1886  loss_rpn_cls: 0.01111  loss_rpn_loc: 0.008754  time: 0.8137  data_time: 0.0075  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:18:03 d2.utils.events]: \u001b[0m eta: 6:35:48  iter: 1079  total_loss: 0.2424  loss_cls: 0.07288  loss_box_reg: 0.1519  loss_rpn_cls: 0.0117  loss_rpn_loc: 0.00999  time: 0.8139  data_time: 0.0078  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:18:20 d2.utils.events]: \u001b[0m eta: 6:35:34  iter: 1099  total_loss: 0.2486  loss_cls: 0.06317  loss_box_reg: 0.1578  loss_rpn_cls: 0.01005  loss_rpn_loc: 0.00806  time: 0.8140  data_time: 0.0070  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 1100\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:18:21 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:18:26 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0022 s/iter. Inference: 0.4472 s/iter. Eval: 0.0006 s/iter. Total: 0.4500 s/iter. ETA=0:00:35\n",
      "\u001b[32m[06/09 05:18:31 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0027 s/iter. Inference: 0.4489 s/iter. Eval: 0.0006 s/iter. Total: 0.4524 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:18:37 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0028 s/iter. Inference: 0.4475 s/iter. Eval: 0.0006 s/iter. Total: 0.4510 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:18:42 d2.evaluation.evaluator]: \u001b[0mInference done 46/89. Dataloading: 0.0028 s/iter. Inference: 0.4501 s/iter. Eval: 0.0006 s/iter. Total: 0.4537 s/iter. ETA=0:00:19\n",
      "\u001b[32m[06/09 05:18:47 d2.evaluation.evaluator]: \u001b[0mInference done 58/89. Dataloading: 0.0028 s/iter. Inference: 0.4489 s/iter. Eval: 0.0007 s/iter. Total: 0.4525 s/iter. ETA=0:00:14\n",
      "\u001b[32m[06/09 05:18:53 d2.evaluation.evaluator]: \u001b[0mInference done 70/89. Dataloading: 0.0029 s/iter. Inference: 0.4489 s/iter. Eval: 0.0007 s/iter. Total: 0.4525 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:18:58 d2.evaluation.evaluator]: \u001b[0mInference done 82/89. Dataloading: 0.0029 s/iter. Inference: 0.4490 s/iter. Eval: 0.0007 s/iter. Total: 0.4526 s/iter. ETA=0:00:03\n",
      "\u001b[32m[06/09 05:19:01 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.747492 (0.449375 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:19:01 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.445398 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:19:01 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:19:01 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:19:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:19:01 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:19:01 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.12 seconds.\n",
      "\u001b[32m[06/09 05:19:01 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:19:01 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.517\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.730\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.596\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.351\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.583\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.756\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.471\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.666\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.669\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.550\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.717\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.854\n",
      "\u001b[32m[06/09 05:19:01 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 51.665 | 73.039 | 59.594 | 35.059 | 58.333 | 75.630 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 51.66452650428426, 'AP50': 73.03896769119677, 'AP75': 59.594072883644266, 'APs': 35.05916020973638, 'APm': 58.332881759205776, 'APl': 75.62991915242091})])\n",
      "Putting scalar for val/AP:  51.66452650428426 at iter 1100 storage at iter  1100\n",
      "Early stopping counter:  1 / 5\n",
      "\u001b[32m[06/09 05:19:17 d2.utils.events]: \u001b[0m eta: 6:35:21  iter: 1119  total_loss: 0.2583  loss_cls: 0.06628  loss_box_reg: 0.1761  loss_rpn_cls: 0.0103  loss_rpn_loc: 0.00628  time: 0.8141  data_time: 0.0072  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:19:33 d2.utils.events]: \u001b[0m eta: 6:35:06  iter: 1139  total_loss: 0.2657  loss_cls: 0.07898  loss_box_reg: 0.1641  loss_rpn_cls: 0.01464  loss_rpn_loc: 0.009774  time: 0.8143  data_time: 0.0079  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:19:50 d2.utils.events]: \u001b[0m eta: 6:35:11  iter: 1159  total_loss: 0.2495  loss_cls: 0.06381  loss_box_reg: 0.1646  loss_rpn_cls: 0.0089  loss_rpn_loc: 0.006563  time: 0.8145  data_time: 0.0079  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:20:07 d2.utils.events]: \u001b[0m eta: 6:35:07  iter: 1179  total_loss: 0.2399  loss_cls: 0.05797  loss_box_reg: 0.1432  loss_rpn_cls: 0.01058  loss_rpn_loc: 0.006226  time: 0.8147  data_time: 0.0078  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:20:23 d2.utils.events]: \u001b[0m eta: 6:34:50  iter: 1199  total_loss: 0.2393  loss_cls: 0.06952  loss_box_reg: 0.1519  loss_rpn_cls: 0.01076  loss_rpn_loc: 0.006286  time: 0.8144  data_time: 0.0079  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 1200\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:20:25 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:20:30 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0022 s/iter. Inference: 0.4312 s/iter. Eval: 0.0007 s/iter. Total: 0.4341 s/iter. ETA=0:00:33\n",
      "\u001b[32m[06/09 05:20:35 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0027 s/iter. Inference: 0.4405 s/iter. Eval: 0.0007 s/iter. Total: 0.4440 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:20:40 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0035 s/iter. Inference: 0.4422 s/iter. Eval: 0.0007 s/iter. Total: 0.4465 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:20:46 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0034 s/iter. Inference: 0.4426 s/iter. Eval: 0.0007 s/iter. Total: 0.4467 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 05:20:51 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0033 s/iter. Inference: 0.4426 s/iter. Eval: 0.0007 s/iter. Total: 0.4466 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:20:57 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0032 s/iter. Inference: 0.4435 s/iter. Eval: 0.0007 s/iter. Total: 0.4474 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:21:02 d2.evaluation.evaluator]: \u001b[0mInference done 82/89. Dataloading: 0.0032 s/iter. Inference: 0.4447 s/iter. Eval: 0.0007 s/iter. Total: 0.4487 s/iter. ETA=0:00:03\n",
      "\u001b[32m[06/09 05:21:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.419190 (0.445467 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:21:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.441127 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:21:04 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:21:04 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:21:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:21:04 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:21:05 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "\u001b[32m[06/09 05:21:05 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:21:05 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.524\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.748\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.603\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.366\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.585\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.762\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.473\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.666\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.671\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.558\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.714\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.854\n",
      "\u001b[32m[06/09 05:21:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.376 | 74.774 | 60.328 | 36.567 | 58.491 | 76.201 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.37604867787833, 'AP50': 74.7743963528561, 'AP75': 60.32845626595337, 'APs': 36.56717269201435, 'APm': 58.4914440758798, 'APl': 76.2006134641603})])\n",
      "Putting scalar for val/AP:  52.37604867787833 at iter 1200 storage at iter  1200\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:21:23 d2.utils.events]: \u001b[0m eta: 6:34:34  iter: 1219  total_loss: 0.2311  loss_cls: 0.05969  loss_box_reg: 0.153  loss_rpn_cls: 0.01066  loss_rpn_loc: 0.006807  time: 0.8141  data_time: 0.0091  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:21:40 d2.utils.events]: \u001b[0m eta: 6:34:21  iter: 1239  total_loss: 0.2803  loss_cls: 0.07564  loss_box_reg: 0.1754  loss_rpn_cls: 0.009074  loss_rpn_loc: 0.009116  time: 0.8139  data_time: 0.0073  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:21:57 d2.utils.events]: \u001b[0m eta: 6:34:10  iter: 1259  total_loss: 0.2367  loss_cls: 0.06689  loss_box_reg: 0.1595  loss_rpn_cls: 0.005961  loss_rpn_loc: 0.003983  time: 0.8138  data_time: 0.0083  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:22:13 d2.utils.events]: \u001b[0m eta: 6:33:40  iter: 1279  total_loss: 0.2671  loss_cls: 0.07489  loss_box_reg: 0.1647  loss_rpn_cls: 0.009303  loss_rpn_loc: 0.008224  time: 0.8134  data_time: 0.0091  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:22:30 d2.utils.events]: \u001b[0m eta: 6:33:20  iter: 1299  total_loss: 0.2596  loss_cls: 0.06986  loss_box_reg: 0.1794  loss_rpn_cls: 0.01064  loss_rpn_loc: 0.006979  time: 0.8133  data_time: 0.0091  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 1300\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:22:31 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:22:36 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0020 s/iter. Inference: 0.4413 s/iter. Eval: 0.0006 s/iter. Total: 0.4439 s/iter. ETA=0:00:34\n",
      "\u001b[32m[06/09 05:22:41 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0026 s/iter. Inference: 0.4453 s/iter. Eval: 0.0007 s/iter. Total: 0.4486 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:22:47 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0027 s/iter. Inference: 0.4447 s/iter. Eval: 0.0007 s/iter. Total: 0.4481 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:22:52 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0027 s/iter. Inference: 0.4450 s/iter. Eval: 0.0007 s/iter. Total: 0.4485 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 05:22:57 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0027 s/iter. Inference: 0.4438 s/iter. Eval: 0.0007 s/iter. Total: 0.4473 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:23:03 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0028 s/iter. Inference: 0.4440 s/iter. Eval: 0.0007 s/iter. Total: 0.4475 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:23:08 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0028 s/iter. Inference: 0.4437 s/iter. Eval: 0.0007 s/iter. Total: 0.4473 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:23:11 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.311551 (0.444185 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:23:11 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:36 (0.440234 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:23:11 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:23:11 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:23:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:23:11 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:23:11 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.13 seconds.\n",
      "\u001b[32m[06/09 05:23:11 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:23:11 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.523\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.740\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.591\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.361\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.582\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.771\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.479\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.671\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.679\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.567\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.721\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.863\n",
      "\u001b[32m[06/09 05:23:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.319 | 73.954 | 59.112 | 36.066 | 58.244 | 77.123 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.31860928543177, 'AP50': 73.95354858566176, 'AP75': 59.111847524377126, 'APs': 36.06591482643303, 'APm': 58.24444678271856, 'APl': 77.12275073712064})])\n",
      "Putting scalar for val/AP:  52.31860928543177 at iter 1300 storage at iter  1300\n",
      "Early stopping counter:  1 / 5\n",
      "\u001b[32m[06/09 05:23:26 d2.utils.events]: \u001b[0m eta: 6:33:03  iter: 1319  total_loss: 0.3007  loss_cls: 0.07831  loss_box_reg: 0.2016  loss_rpn_cls: 0.007974  loss_rpn_loc: 0.008535  time: 0.8131  data_time: 0.0075  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:23:42 d2.utils.events]: \u001b[0m eta: 6:32:46  iter: 1339  total_loss: 0.2296  loss_cls: 0.06224  loss_box_reg: 0.1558  loss_rpn_cls: 0.006662  loss_rpn_loc: 0.007698  time: 0.8131  data_time: 0.0069  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:23:59 d2.utils.events]: \u001b[0m eta: 6:32:29  iter: 1359  total_loss: 0.2181  loss_cls: 0.06293  loss_box_reg: 0.1388  loss_rpn_cls: 0.00702  loss_rpn_loc: 0.006585  time: 0.8131  data_time: 0.0077  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:24:16 d2.utils.events]: \u001b[0m eta: 6:32:08  iter: 1379  total_loss: 0.2562  loss_cls: 0.06759  loss_box_reg: 0.1633  loss_rpn_cls: 0.009795  loss_rpn_loc: 0.009226  time: 0.8130  data_time: 0.0067  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:24:32 d2.utils.events]: \u001b[0m eta: 6:31:50  iter: 1399  total_loss: 0.251  loss_cls: 0.05207  loss_box_reg: 0.1755  loss_rpn_cls: 0.005376  loss_rpn_loc: 0.004701  time: 0.8130  data_time: 0.0063  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 1400\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:24:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:24:38 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0020 s/iter. Inference: 0.4401 s/iter. Eval: 0.0006 s/iter. Total: 0.4427 s/iter. ETA=0:00:34\n",
      "\u001b[32m[06/09 05:24:43 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0026 s/iter. Inference: 0.4445 s/iter. Eval: 0.0007 s/iter. Total: 0.4478 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:24:49 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0028 s/iter. Inference: 0.4444 s/iter. Eval: 0.0007 s/iter. Total: 0.4479 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:24:54 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0028 s/iter. Inference: 0.4443 s/iter. Eval: 0.0007 s/iter. Total: 0.4479 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 05:24:59 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0028 s/iter. Inference: 0.4443 s/iter. Eval: 0.0007 s/iter. Total: 0.4479 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:25:05 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0029 s/iter. Inference: 0.4451 s/iter. Eval: 0.0007 s/iter. Total: 0.4487 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:25:10 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0029 s/iter. Inference: 0.4447 s/iter. Eval: 0.0007 s/iter. Total: 0.4483 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:25:13 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.410904 (0.445368 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:25:13 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.441355 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:25:13 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:25:13 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:25:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:25:13 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:25:13 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "\u001b[32m[06/09 05:25:13 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:25:13 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.521\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.743\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.620\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.355\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.585\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.767\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.475\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.672\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.681\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.565\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.729\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.854\n",
      "\u001b[32m[06/09 05:25:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.096 | 74.333 | 62.035 | 35.479 | 58.494 | 76.691 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.09648622791953, 'AP50': 74.33281097454758, 'AP75': 62.03529991055723, 'APs': 35.47904404547207, 'APm': 58.494249466829096, 'APl': 76.69110933690703})])\n",
      "Putting scalar for val/AP:  52.09648622791953 at iter 1400 storage at iter  1400\n",
      "Early stopping counter:  2 / 5\n",
      "\u001b[32m[06/09 05:25:28 d2.utils.events]: \u001b[0m eta: 6:31:16  iter: 1419  total_loss: 0.2783  loss_cls: 0.07407  loss_box_reg: 0.16  loss_rpn_cls: 0.007914  loss_rpn_loc: 0.009129  time: 0.8129  data_time: 0.0064  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:25:45 d2.utils.events]: \u001b[0m eta: 6:30:54  iter: 1439  total_loss: 0.2394  loss_cls: 0.05693  loss_box_reg: 0.1595  loss_rpn_cls: 0.007659  loss_rpn_loc: 0.006527  time: 0.8129  data_time: 0.0088  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:26:01 d2.utils.events]: \u001b[0m eta: 6:30:12  iter: 1459  total_loss: 0.2327  loss_cls: 0.05582  loss_box_reg: 0.1645  loss_rpn_cls: 0.008517  loss_rpn_loc: 0.005862  time: 0.8128  data_time: 0.0083  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:26:17 d2.utils.events]: \u001b[0m eta: 6:29:27  iter: 1479  total_loss: 0.2527  loss_cls: 0.06992  loss_box_reg: 0.1742  loss_rpn_cls: 0.007709  loss_rpn_loc: 0.007374  time: 0.8123  data_time: 0.0068  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:26:33 d2.utils.events]: \u001b[0m eta: 6:28:38  iter: 1499  total_loss: 0.1924  loss_cls: 0.04405  loss_box_reg: 0.1459  loss_rpn_cls: 0.005073  loss_rpn_loc: 0.00495  time: 0.8117  data_time: 0.0061  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 1500\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:26:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:26:38 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0021 s/iter. Inference: 0.4266 s/iter. Eval: 0.0006 s/iter. Total: 0.4292 s/iter. ETA=0:00:33\n",
      "\u001b[32m[06/09 05:26:43 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0025 s/iter. Inference: 0.4222 s/iter. Eval: 0.0006 s/iter. Total: 0.4254 s/iter. ETA=0:00:28\n",
      "\u001b[32m[06/09 05:26:49 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0026 s/iter. Inference: 0.4236 s/iter. Eval: 0.0006 s/iter. Total: 0.4269 s/iter. ETA=0:00:23\n",
      "\u001b[32m[06/09 05:26:54 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0027 s/iter. Inference: 0.4245 s/iter. Eval: 0.0006 s/iter. Total: 0.4278 s/iter. ETA=0:00:17\n",
      "\u001b[32m[06/09 05:26:59 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0027 s/iter. Inference: 0.4254 s/iter. Eval: 0.0006 s/iter. Total: 0.4287 s/iter. ETA=0:00:12\n",
      "\u001b[32m[06/09 05:27:04 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0027 s/iter. Inference: 0.4264 s/iter. Eval: 0.0006 s/iter. Total: 0.4297 s/iter. ETA=0:00:07\n",
      "\u001b[32m[06/09 05:27:09 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0027 s/iter. Inference: 0.4258 s/iter. Eval: 0.0006 s/iter. Total: 0.4292 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:27:12 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:35.838733 (0.426652 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:27:12 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:35 (0.422918 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:27:12 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:27:12 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:27:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:27:12 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:27:12 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.12 seconds.\n",
      "\u001b[32m[06/09 05:27:12 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:27:12 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.524\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.738\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.609\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.361\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.574\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.786\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.479\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.664\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.665\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.550\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.706\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.860\n",
      "\u001b[32m[06/09 05:27:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.404 | 73.814 | 60.923 | 36.081 | 57.399 | 78.589 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.404138401235876, 'AP50': 73.81376597488223, 'AP75': 60.92263974592844, 'APs': 36.08107849687464, 'APm': 57.39939057334647, 'APl': 78.58939294915376})])\n",
      "Putting scalar for val/AP:  52.404138401235876 at iter 1500 storage at iter  1500\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:27:30 d2.utils.events]: \u001b[0m eta: 6:27:47  iter: 1519  total_loss: 0.2329  loss_cls: 0.05495  loss_box_reg: 0.157  loss_rpn_cls: 0.007962  loss_rpn_loc: 0.005014  time: 0.8111  data_time: 0.0072  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:27:45 d2.utils.events]: \u001b[0m eta: 6:27:08  iter: 1539  total_loss: 0.2377  loss_cls: 0.06402  loss_box_reg: 0.1655  loss_rpn_cls: 0.006911  loss_rpn_loc: 0.006307  time: 0.8106  data_time: 0.0054  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:28:01 d2.utils.events]: \u001b[0m eta: 6:26:24  iter: 1559  total_loss: 0.26  loss_cls: 0.06246  loss_box_reg: 0.1664  loss_rpn_cls: 0.006044  loss_rpn_loc: 0.005284  time: 0.8102  data_time: 0.0071  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:28:17 d2.utils.events]: \u001b[0m eta: 6:25:42  iter: 1579  total_loss: 0.2079  loss_cls: 0.04406  loss_box_reg: 0.1548  loss_rpn_cls: 0.005404  loss_rpn_loc: 0.005465  time: 0.8098  data_time: 0.0060  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:28:32 d2.utils.events]: \u001b[0m eta: 6:24:50  iter: 1599  total_loss: 0.2453  loss_cls: 0.07406  loss_box_reg: 0.1539  loss_rpn_cls: 0.006954  loss_rpn_loc: 0.008273  time: 0.8095  data_time: 0.0056  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 1600\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:28:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:28:38 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0023 s/iter. Inference: 0.4240 s/iter. Eval: 0.0006 s/iter. Total: 0.4269 s/iter. ETA=0:00:33\n",
      "\u001b[32m[06/09 05:28:43 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0026 s/iter. Inference: 0.4211 s/iter. Eval: 0.0006 s/iter. Total: 0.4244 s/iter. ETA=0:00:28\n",
      "\u001b[32m[06/09 05:28:48 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0027 s/iter. Inference: 0.4233 s/iter. Eval: 0.0006 s/iter. Total: 0.4266 s/iter. ETA=0:00:23\n",
      "\u001b[32m[06/09 05:28:54 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0027 s/iter. Inference: 0.4233 s/iter. Eval: 0.0006 s/iter. Total: 0.4267 s/iter. ETA=0:00:17\n",
      "\u001b[32m[06/09 05:28:59 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0027 s/iter. Inference: 0.4228 s/iter. Eval: 0.0006 s/iter. Total: 0.4262 s/iter. ETA=0:00:12\n",
      "\u001b[32m[06/09 05:29:04 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0027 s/iter. Inference: 0.4230 s/iter. Eval: 0.0006 s/iter. Total: 0.4264 s/iter. ETA=0:00:07\n",
      "\u001b[32m[06/09 05:29:09 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0027 s/iter. Inference: 0.4231 s/iter. Eval: 0.0006 s/iter. Total: 0.4265 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:29:11 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:35.562328 (0.423361 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:29:11 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:35 (0.419575 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:29:11 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:29:11 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:29:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:29:11 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:29:11 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "\u001b[32m[06/09 05:29:11 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:29:11 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.524\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.746\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.616\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.360\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.583\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.786\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.488\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.676\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.683\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.558\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.736\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.869\n",
      "\u001b[32m[06/09 05:29:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.449 | 74.637 | 61.641 | 36.006 | 58.343 | 78.621 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.44875027346739, 'AP50': 74.63661458861549, 'AP75': 61.641458364092074, 'APs': 36.006474945747144, 'APm': 58.34316678037015, 'APl': 78.62134743250276})])\n",
      "Putting scalar for val/AP:  52.44875027346739 at iter 1600 storage at iter  1600\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:29:29 d2.utils.events]: \u001b[0m eta: 6:23:59  iter: 1619  total_loss: 0.2695  loss_cls: 0.06396  loss_box_reg: 0.1886  loss_rpn_cls: 0.007374  loss_rpn_loc: 0.009073  time: 0.8089  data_time: 0.0067  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:29:45 d2.utils.events]: \u001b[0m eta: 6:22:47  iter: 1639  total_loss: 0.2737  loss_cls: 0.0506  loss_box_reg: 0.1959  loss_rpn_cls: 0.005503  loss_rpn_loc: 0.005804  time: 0.8085  data_time: 0.0058  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:30:00 d2.utils.events]: \u001b[0m eta: 6:21:49  iter: 1659  total_loss: 0.2294  loss_cls: 0.05221  loss_box_reg: 0.1652  loss_rpn_cls: 0.006208  loss_rpn_loc: 0.009046  time: 0.8080  data_time: 0.0066  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:30:16 d2.utils.events]: \u001b[0m eta: 6:20:53  iter: 1679  total_loss: 0.2293  loss_cls: 0.05997  loss_box_reg: 0.153  loss_rpn_cls: 0.005163  loss_rpn_loc: 0.00543  time: 0.8076  data_time: 0.0061  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:30:32 d2.utils.events]: \u001b[0m eta: 6:19:47  iter: 1699  total_loss: 0.2498  loss_cls: 0.05795  loss_box_reg: 0.1831  loss_rpn_cls: 0.004092  loss_rpn_loc: 0.005683  time: 0.8072  data_time: 0.0061  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 1700\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:30:34 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:30:39 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0021 s/iter. Inference: 0.4169 s/iter. Eval: 0.0006 s/iter. Total: 0.4196 s/iter. ETA=0:00:32\n",
      "\u001b[32m[06/09 05:30:44 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0025 s/iter. Inference: 0.4203 s/iter. Eval: 0.0006 s/iter. Total: 0.4235 s/iter. ETA=0:00:27\n",
      "\u001b[32m[06/09 05:30:49 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0026 s/iter. Inference: 0.4227 s/iter. Eval: 0.0006 s/iter. Total: 0.4260 s/iter. ETA=0:00:23\n",
      "\u001b[32m[06/09 05:30:54 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0027 s/iter. Inference: 0.4239 s/iter. Eval: 0.0006 s/iter. Total: 0.4272 s/iter. ETA=0:00:17\n",
      "\u001b[32m[06/09 05:31:00 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0027 s/iter. Inference: 0.4249 s/iter. Eval: 0.0006 s/iter. Total: 0.4283 s/iter. ETA=0:00:12\n",
      "\u001b[32m[06/09 05:31:05 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0027 s/iter. Inference: 0.4254 s/iter. Eval: 0.0006 s/iter. Total: 0.4289 s/iter. ETA=0:00:07\n",
      "\u001b[32m[06/09 05:31:10 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0027 s/iter. Inference: 0.4245 s/iter. Eval: 0.0006 s/iter. Total: 0.4279 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:31:12 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:35.719287 (0.425230 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:31:12 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:35 (0.421451 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:31:12 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:31:12 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:31:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:31:12 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:31:12 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "\u001b[32m[06/09 05:31:12 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:31:12 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.520\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.742\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.610\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.352\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.577\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.780\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.475\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.662\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.668\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.546\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.719\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.853\n",
      "\u001b[32m[06/09 05:31:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 51.957 | 74.234 | 61.011 | 35.178 | 57.736 | 78.035 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 51.95685935888292, 'AP50': 74.23359391592766, 'AP75': 61.010671950966014, 'APs': 35.177564859378506, 'APm': 57.73604973986007, 'APl': 78.03521592017754})])\n",
      "Putting scalar for val/AP:  51.95685935888292 at iter 1700 storage at iter  1700\n",
      "Early stopping counter:  1 / 5\n",
      "\u001b[32m[06/09 05:31:27 d2.utils.events]: \u001b[0m eta: 6:18:39  iter: 1719  total_loss: 0.2134  loss_cls: 0.04544  loss_box_reg: 0.1507  loss_rpn_cls: 0.006476  loss_rpn_loc: 0.005112  time: 0.8069  data_time: 0.0073  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:31:43 d2.utils.events]: \u001b[0m eta: 6:17:31  iter: 1739  total_loss: 0.2335  loss_cls: 0.0593  loss_box_reg: 0.1655  loss_rpn_cls: 0.003989  loss_rpn_loc: 0.006221  time: 0.8065  data_time: 0.0067  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:31:59 d2.utils.events]: \u001b[0m eta: 6:16:26  iter: 1759  total_loss: 0.2229  loss_cls: 0.05524  loss_box_reg: 0.1608  loss_rpn_cls: 0.007765  loss_rpn_loc: 0.006894  time: 0.8063  data_time: 0.0065  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:32:16 d2.utils.events]: \u001b[0m eta: 6:16:01  iter: 1779  total_loss: 0.2433  loss_cls: 0.05208  loss_box_reg: 0.1673  loss_rpn_cls: 0.004125  loss_rpn_loc: 0.006188  time: 0.8064  data_time: 0.0070  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:32:32 d2.utils.events]: \u001b[0m eta: 6:15:41  iter: 1799  total_loss: 0.2427  loss_cls: 0.05442  loss_box_reg: 0.1616  loss_rpn_cls: 0.007069  loss_rpn_loc: 0.005996  time: 0.8065  data_time: 0.0082  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 1800\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:32:33 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:32:38 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0023 s/iter. Inference: 0.4464 s/iter. Eval: 0.0007 s/iter. Total: 0.4493 s/iter. ETA=0:00:35\n",
      "\u001b[32m[06/09 05:32:44 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0027 s/iter. Inference: 0.4458 s/iter. Eval: 0.0006 s/iter. Total: 0.4492 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:32:49 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0028 s/iter. Inference: 0.4469 s/iter. Eval: 0.0006 s/iter. Total: 0.4504 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:32:55 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0028 s/iter. Inference: 0.4469 s/iter. Eval: 0.0006 s/iter. Total: 0.4505 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 05:33:00 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0028 s/iter. Inference: 0.4476 s/iter. Eval: 0.0006 s/iter. Total: 0.4512 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:33:06 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0028 s/iter. Inference: 0.4474 s/iter. Eval: 0.0006 s/iter. Total: 0.4510 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:33:11 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0028 s/iter. Inference: 0.4468 s/iter. Eval: 0.0007 s/iter. Total: 0.4504 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:33:13 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.626031 (0.447929 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:33:13 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.443970 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:33:13 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:33:13 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:33:13 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:33:13 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:33:14 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "\u001b[32m[06/09 05:33:14 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:33:14 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.524\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.748\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.617\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.363\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.572\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.790\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.479\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.657\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.660\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.551\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.699\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.847\n",
      "\u001b[32m[06/09 05:33:14 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.449 | 74.825 | 61.669 | 36.329 | 57.194 | 78.999 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.44944160875219, 'AP50': 74.82469883658197, 'AP75': 61.66905015271289, 'APs': 36.32883511460306, 'APm': 57.194051128461496, 'APl': 78.99854668933783})])\n",
      "Putting scalar for val/AP:  52.44944160875219 at iter 1800 storage at iter  1800\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:33:32 d2.utils.events]: \u001b[0m eta: 6:15:38  iter: 1819  total_loss: 0.2444  loss_cls: 0.05361  loss_box_reg: 0.1745  loss_rpn_cls: 0.006227  loss_rpn_loc: 0.008761  time: 0.8065  data_time: 0.0096  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:33:49 d2.utils.events]: \u001b[0m eta: 6:16:18  iter: 1839  total_loss: 0.2398  loss_cls: 0.0506  loss_box_reg: 0.1701  loss_rpn_cls: 0.005183  loss_rpn_loc: 0.005395  time: 0.8066  data_time: 0.0075  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:34:05 d2.utils.events]: \u001b[0m eta: 6:16:35  iter: 1859  total_loss: 0.2718  loss_cls: 0.0619  loss_box_reg: 0.1798  loss_rpn_cls: 0.005987  loss_rpn_loc: 0.008028  time: 0.8065  data_time: 0.0071  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:34:21 d2.utils.events]: \u001b[0m eta: 6:17:11  iter: 1879  total_loss: 0.2159  loss_cls: 0.05747  loss_box_reg: 0.1482  loss_rpn_cls: 0.003858  loss_rpn_loc: 0.005878  time: 0.8065  data_time: 0.0087  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:34:37 d2.utils.events]: \u001b[0m eta: 6:17:15  iter: 1899  total_loss: 0.2449  loss_cls: 0.05271  loss_box_reg: 0.1688  loss_rpn_cls: 0.004116  loss_rpn_loc: 0.005264  time: 0.8063  data_time: 0.0088  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 1900\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:34:38 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:34:43 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0025 s/iter. Inference: 0.4449 s/iter. Eval: 0.0006 s/iter. Total: 0.4480 s/iter. ETA=0:00:34\n",
      "\u001b[32m[06/09 05:34:48 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0028 s/iter. Inference: 0.4428 s/iter. Eval: 0.0006 s/iter. Total: 0.4463 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:34:54 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0029 s/iter. Inference: 0.4418 s/iter. Eval: 0.0006 s/iter. Total: 0.4454 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:34:59 d2.evaluation.evaluator]: \u001b[0mInference done 46/89. Dataloading: 0.0029 s/iter. Inference: 0.4448 s/iter. Eval: 0.0006 s/iter. Total: 0.4485 s/iter. ETA=0:00:19\n",
      "\u001b[32m[06/09 05:35:04 d2.evaluation.evaluator]: \u001b[0mInference done 58/89. Dataloading: 0.0029 s/iter. Inference: 0.4448 s/iter. Eval: 0.0006 s/iter. Total: 0.4485 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:35:09 d2.evaluation.evaluator]: \u001b[0mInference done 70/89. Dataloading: 0.0029 s/iter. Inference: 0.4436 s/iter. Eval: 0.0006 s/iter. Total: 0.4472 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:35:15 d2.evaluation.evaluator]: \u001b[0mInference done 82/89. Dataloading: 0.0029 s/iter. Inference: 0.4436 s/iter. Eval: 0.0006 s/iter. Total: 0.4472 s/iter. ETA=0:00:03\n",
      "\u001b[32m[06/09 05:35:18 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.296623 (0.444007 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:35:18 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:36 (0.439736 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:35:18 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:35:18 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:35:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:35:18 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:35:18 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.12 seconds.\n",
      "\u001b[32m[06/09 05:35:18 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:35:18 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.526\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.751\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.622\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.341\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.592\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.797\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.485\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.660\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.661\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.526\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.718\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.863\n",
      "\u001b[32m[06/09 05:35:18 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.558 | 75.093 | 62.169 | 34.069 | 59.196 | 79.674 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.55762787489344, 'AP50': 75.0929452009626, 'AP75': 62.16883663637643, 'APs': 34.06917989848506, 'APm': 59.19608966901041, 'APl': 79.67386014069677})])\n",
      "Putting scalar for val/AP:  52.55762787489344 at iter 1900 storage at iter  1900\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:35:36 d2.utils.events]: \u001b[0m eta: 6:16:42  iter: 1919  total_loss: 0.2409  loss_cls: 0.05432  loss_box_reg: 0.1827  loss_rpn_cls: 0.004224  loss_rpn_loc: 0.005496  time: 0.8062  data_time: 0.0083  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:35:53 d2.utils.events]: \u001b[0m eta: 6:16:23  iter: 1939  total_loss: 0.2172  loss_cls: 0.04729  loss_box_reg: 0.1516  loss_rpn_cls: 0.004368  loss_rpn_loc: 0.004089  time: 0.8062  data_time: 0.0075  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:36:09 d2.utils.events]: \u001b[0m eta: 6:16:07  iter: 1959  total_loss: 0.2693  loss_cls: 0.05392  loss_box_reg: 0.1939  loss_rpn_cls: 0.007138  loss_rpn_loc: 0.007266  time: 0.8063  data_time: 0.0068  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:36:26 d2.utils.events]: \u001b[0m eta: 6:15:59  iter: 1979  total_loss: 0.2672  loss_cls: 0.06138  loss_box_reg: 0.1871  loss_rpn_cls: 0.005997  loss_rpn_loc: 0.008093  time: 0.8063  data_time: 0.0080  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:36:42 d2.utils.events]: \u001b[0m eta: 6:15:55  iter: 1999  total_loss: 0.2177  loss_cls: 0.04734  loss_box_reg: 0.1542  loss_rpn_cls: 0.003177  loss_rpn_loc: 0.006587  time: 0.8061  data_time: 0.0085  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 2000\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:36:43 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:36:48 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0022 s/iter. Inference: 0.4434 s/iter. Eval: 0.0007 s/iter. Total: 0.4463 s/iter. ETA=0:00:34\n",
      "\u001b[32m[06/09 05:36:54 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0027 s/iter. Inference: 0.4445 s/iter. Eval: 0.0007 s/iter. Total: 0.4480 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:36:59 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0027 s/iter. Inference: 0.4435 s/iter. Eval: 0.0007 s/iter. Total: 0.4470 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:37:04 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0028 s/iter. Inference: 0.4451 s/iter. Eval: 0.0007 s/iter. Total: 0.4486 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 05:37:10 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0028 s/iter. Inference: 0.4446 s/iter. Eval: 0.0007 s/iter. Total: 0.4482 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:37:15 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0028 s/iter. Inference: 0.4433 s/iter. Eval: 0.0007 s/iter. Total: 0.4469 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:37:20 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0028 s/iter. Inference: 0.4423 s/iter. Eval: 0.0007 s/iter. Total: 0.4459 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:37:23 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.248776 (0.443438 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:37:23 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:36 (0.439423 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:37:23 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:37:23 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:37:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:37:23 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:37:23 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "\u001b[32m[06/09 05:37:23 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:37:23 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.528\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.758\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.629\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.375\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.577\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.790\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.488\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.670\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.672\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.555\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.717\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.861\n",
      "\u001b[32m[06/09 05:37:23 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.794 | 75.781 | 62.945 | 37.481 | 57.730 | 79.023 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.79445714058344, 'AP50': 75.78074226266133, 'AP75': 62.945399428427216, 'APs': 37.48050544223293, 'APm': 57.730154688445566, 'APl': 79.02340457836608})])\n",
      "Putting scalar for val/AP:  52.79445714058344 at iter 2000 storage at iter  2000\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:37:42 d2.utils.events]: \u001b[0m eta: 6:15:37  iter: 2019  total_loss: 0.2097  loss_cls: 0.04462  loss_box_reg: 0.1539  loss_rpn_cls: 0.004302  loss_rpn_loc: 0.006389  time: 0.8061  data_time: 0.0068  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:37:58 d2.utils.events]: \u001b[0m eta: 6:15:18  iter: 2039  total_loss: 0.2148  loss_cls: 0.04564  loss_box_reg: 0.1642  loss_rpn_cls: 0.003898  loss_rpn_loc: 0.003981  time: 0.8061  data_time: 0.0077  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:38:15 d2.utils.events]: \u001b[0m eta: 6:14:55  iter: 2059  total_loss: 0.2705  loss_cls: 0.05442  loss_box_reg: 0.199  loss_rpn_cls: 0.004825  loss_rpn_loc: 0.00693  time: 0.8061  data_time: 0.0072  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:38:31 d2.utils.events]: \u001b[0m eta: 6:14:37  iter: 2079  total_loss: 0.219  loss_cls: 0.04269  loss_box_reg: 0.1606  loss_rpn_cls: 0.003167  loss_rpn_loc: 0.005266  time: 0.8061  data_time: 0.0081  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:38:47 d2.utils.events]: \u001b[0m eta: 6:14:17  iter: 2099  total_loss: 0.2589  loss_cls: 0.05344  loss_box_reg: 0.1773  loss_rpn_cls: 0.003784  loss_rpn_loc: 0.008904  time: 0.8060  data_time: 0.0093  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 2100\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:38:48 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:38:53 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0025 s/iter. Inference: 0.4364 s/iter. Eval: 0.0007 s/iter. Total: 0.4395 s/iter. ETA=0:00:34\n",
      "\u001b[32m[06/09 05:38:59 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0028 s/iter. Inference: 0.4421 s/iter. Eval: 0.0006 s/iter. Total: 0.4456 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:39:04 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0028 s/iter. Inference: 0.4415 s/iter. Eval: 0.0007 s/iter. Total: 0.4451 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:39:09 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0028 s/iter. Inference: 0.4398 s/iter. Eval: 0.0006 s/iter. Total: 0.4434 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 05:39:15 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0028 s/iter. Inference: 0.4407 s/iter. Eval: 0.0006 s/iter. Total: 0.4442 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:39:20 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0029 s/iter. Inference: 0.4406 s/iter. Eval: 0.0006 s/iter. Total: 0.4442 s/iter. ETA=0:00:07\n",
      "\u001b[32m[06/09 05:39:26 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0029 s/iter. Inference: 0.4409 s/iter. Eval: 0.0006 s/iter. Total: 0.4445 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:39:28 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.106055 (0.441739 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:39:28 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:36 (0.437768 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:39:28 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:39:28 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:39:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:39:28 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:39:28 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.13 seconds.\n",
      "\u001b[32m[06/09 05:39:28 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:39:28 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.526\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.755\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.621\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.369\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.583\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.771\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.483\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.661\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.664\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.543\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.713\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.851\n",
      "\u001b[32m[06/09 05:39:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.554 | 75.453 | 62.099 | 36.866 | 58.340 | 77.134 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.55393319844568, 'AP50': 75.45304829094033, 'AP75': 62.0991100254565, 'APs': 36.8664108705854, 'APm': 58.34038464618886, 'APl': 77.133751025642})])\n",
      "Putting scalar for val/AP:  52.55393319844568 at iter 2100 storage at iter  2100\n",
      "Early stopping counter:  1 / 5\n",
      "\u001b[32m[06/09 05:39:44 d2.utils.events]: \u001b[0m eta: 6:14:02  iter: 2119  total_loss: 0.2416  loss_cls: 0.05555  loss_box_reg: 0.1751  loss_rpn_cls: 0.004184  loss_rpn_loc: 0.004669  time: 0.8061  data_time: 0.0069  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:40:00 d2.utils.events]: \u001b[0m eta: 6:13:44  iter: 2139  total_loss: 0.228  loss_cls: 0.04101  loss_box_reg: 0.1643  loss_rpn_cls: 0.00452  loss_rpn_loc: 0.006407  time: 0.8062  data_time: 0.0072  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:40:16 d2.utils.events]: \u001b[0m eta: 6:13:27  iter: 2159  total_loss: 0.2145  loss_cls: 0.03917  loss_box_reg: 0.1476  loss_rpn_cls: 0.003843  loss_rpn_loc: 0.005957  time: 0.8061  data_time: 0.0081  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:40:32 d2.utils.events]: \u001b[0m eta: 6:13:02  iter: 2179  total_loss: 0.2299  loss_cls: 0.04596  loss_box_reg: 0.1572  loss_rpn_cls: 0.003651  loss_rpn_loc: 0.006603  time: 0.8061  data_time: 0.0083  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:40:49 d2.utils.events]: \u001b[0m eta: 6:12:44  iter: 2199  total_loss: 0.2259  loss_cls: 0.04209  loss_box_reg: 0.1742  loss_rpn_cls: 0.003678  loss_rpn_loc: 0.004535  time: 0.8061  data_time: 0.0078  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 2200\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:40:50 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:40:55 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0022 s/iter. Inference: 0.4519 s/iter. Eval: 0.0006 s/iter. Total: 0.4547 s/iter. ETA=0:00:35\n",
      "\u001b[32m[06/09 05:41:00 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0026 s/iter. Inference: 0.4491 s/iter. Eval: 0.0006 s/iter. Total: 0.4525 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:41:06 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0028 s/iter. Inference: 0.4467 s/iter. Eval: 0.0006 s/iter. Total: 0.4502 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:41:11 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0028 s/iter. Inference: 0.4451 s/iter. Eval: 0.0006 s/iter. Total: 0.4487 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 05:41:16 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0028 s/iter. Inference: 0.4434 s/iter. Eval: 0.0006 s/iter. Total: 0.4470 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:41:22 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0029 s/iter. Inference: 0.4435 s/iter. Eval: 0.0006 s/iter. Total: 0.4471 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:41:27 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0029 s/iter. Inference: 0.4437 s/iter. Eval: 0.0006 s/iter. Total: 0.4473 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:41:30 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.324790 (0.444343 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:41:30 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:36 (0.440355 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:41:30 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:41:30 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:41:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:41:30 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:41:30 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "\u001b[32m[06/09 05:41:30 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:41:30 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.528\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.751\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.618\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.357\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.585\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.478\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.655\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.655\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.527\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.703\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.866\n",
      "\u001b[32m[06/09 05:41:30 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.800 | 75.095 | 61.773 | 35.654 | 58.474 | 79.305 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.80025909261238, 'AP50': 75.09512600829164, 'AP75': 61.77300198626393, 'APs': 35.654334579701526, 'APm': 58.47416950086271, 'APl': 79.30502218426723})])\n",
      "Putting scalar for val/AP:  52.80025909261238 at iter 2200 storage at iter  2200\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:41:49 d2.utils.events]: \u001b[0m eta: 6:12:32  iter: 2219  total_loss: 0.2343  loss_cls: 0.04804  loss_box_reg: 0.1718  loss_rpn_cls: 0.004425  loss_rpn_loc: 0.006381  time: 0.8060  data_time: 0.0091  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:42:05 d2.utils.events]: \u001b[0m eta: 6:12:19  iter: 2239  total_loss: 0.2086  loss_cls: 0.04257  loss_box_reg: 0.1575  loss_rpn_cls: 0.004794  loss_rpn_loc: 0.00586  time: 0.8060  data_time: 0.0074  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:42:21 d2.utils.events]: \u001b[0m eta: 6:12:00  iter: 2259  total_loss: 0.2334  loss_cls: 0.04308  loss_box_reg: 0.1843  loss_rpn_cls: 0.003171  loss_rpn_loc: 0.005314  time: 0.8060  data_time: 0.0078  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:42:38 d2.utils.events]: \u001b[0m eta: 6:11:50  iter: 2279  total_loss: 0.2123  loss_cls: 0.05131  loss_box_reg: 0.1468  loss_rpn_cls: 0.003279  loss_rpn_loc: 0.005503  time: 0.8061  data_time: 0.0073  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:42:54 d2.utils.events]: \u001b[0m eta: 6:11:35  iter: 2299  total_loss: 0.2071  loss_cls: 0.04376  loss_box_reg: 0.1498  loss_rpn_cls: 0.004734  loss_rpn_loc: 0.004827  time: 0.8061  data_time: 0.0076  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 2300\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:42:56 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:43:01 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0024 s/iter. Inference: 0.4474 s/iter. Eval: 0.0006 s/iter. Total: 0.4505 s/iter. ETA=0:00:35\n",
      "\u001b[32m[06/09 05:43:06 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0027 s/iter. Inference: 0.4487 s/iter. Eval: 0.0006 s/iter. Total: 0.4521 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:43:12 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0027 s/iter. Inference: 0.4474 s/iter. Eval: 0.0006 s/iter. Total: 0.4509 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:43:17 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0028 s/iter. Inference: 0.4476 s/iter. Eval: 0.0006 s/iter. Total: 0.4511 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 05:43:23 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0028 s/iter. Inference: 0.4457 s/iter. Eval: 0.0006 s/iter. Total: 0.4492 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:43:28 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0029 s/iter. Inference: 0.4447 s/iter. Eval: 0.0006 s/iter. Total: 0.4483 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:43:33 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0029 s/iter. Inference: 0.4443 s/iter. Eval: 0.0006 s/iter. Total: 0.4479 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:43:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.351645 (0.444662 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:43:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.440624 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:43:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:43:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:43:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:43:36 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:43:36 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.13 seconds.\n",
      "\u001b[32m[06/09 05:43:36 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:43:36 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.536\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.759\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.629\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.355\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.601\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.807\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.481\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.654\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.654\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.518\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.706\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.873\n",
      "\u001b[32m[06/09 05:43:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 53.565 | 75.850 | 62.919 | 35.523 | 60.075 | 80.660 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 53.56451891603101, 'AP50': 75.85011030020067, 'AP75': 62.91903284237617, 'APs': 35.52335267438221, 'APm': 60.075272218223766, 'APl': 80.66020395183153})])\n",
      "Putting scalar for val/AP:  53.56451891603101 at iter 2300 storage at iter  2300\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:43:54 d2.utils.events]: \u001b[0m eta: 6:11:17  iter: 2319  total_loss: 0.2492  loss_cls: 0.04607  loss_box_reg: 0.1773  loss_rpn_cls: 0.003248  loss_rpn_loc: 0.006452  time: 0.8061  data_time: 0.0091  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:44:11 d2.utils.events]: \u001b[0m eta: 6:10:52  iter: 2339  total_loss: 0.2104  loss_cls: 0.04546  loss_box_reg: 0.1423  loss_rpn_cls: 0.002948  loss_rpn_loc: 0.006415  time: 0.8060  data_time: 0.0072  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:44:28 d2.utils.events]: \u001b[0m eta: 6:10:37  iter: 2359  total_loss: 0.2696  loss_cls: 0.04992  loss_box_reg: 0.1803  loss_rpn_cls: 0.003571  loss_rpn_loc: 0.005854  time: 0.8062  data_time: 0.0072  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:44:44 d2.utils.events]: \u001b[0m eta: 6:10:21  iter: 2379  total_loss: 0.2498  loss_cls: 0.05766  loss_box_reg: 0.1778  loss_rpn_cls: 0.005331  loss_rpn_loc: 0.005474  time: 0.8062  data_time: 0.0081  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:45:01 d2.utils.events]: \u001b[0m eta: 6:10:13  iter: 2399  total_loss: 0.2086  loss_cls: 0.0416  loss_box_reg: 0.1677  loss_rpn_cls: 0.00276  loss_rpn_loc: 0.004781  time: 0.8062  data_time: 0.0088  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 2400\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:45:02 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:45:07 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0020 s/iter. Inference: 0.4405 s/iter. Eval: 0.0006 s/iter. Total: 0.4432 s/iter. ETA=0:00:34\n",
      "\u001b[32m[06/09 05:45:12 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0026 s/iter. Inference: 0.4457 s/iter. Eval: 0.0006 s/iter. Total: 0.4491 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:45:18 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0027 s/iter. Inference: 0.4453 s/iter. Eval: 0.0006 s/iter. Total: 0.4488 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:45:23 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0028 s/iter. Inference: 0.4447 s/iter. Eval: 0.0007 s/iter. Total: 0.4483 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 05:45:28 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0028 s/iter. Inference: 0.4452 s/iter. Eval: 0.0007 s/iter. Total: 0.4487 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:45:34 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0028 s/iter. Inference: 0.4442 s/iter. Eval: 0.0006 s/iter. Total: 0.4478 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:45:39 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0028 s/iter. Inference: 0.4439 s/iter. Eval: 0.0006 s/iter. Total: 0.4475 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:45:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.318333 (0.444266 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:45:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:36 (0.440305 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:45:42 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:45:42 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:45:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:45:42 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:45:42 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "\u001b[32m[06/09 05:45:42 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:45:42 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.529\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.763\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.626\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.349\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.591\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.803\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.482\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.654\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.655\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.523\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.705\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.866\n",
      "\u001b[32m[06/09 05:45:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.871 | 76.279 | 62.615 | 34.877 | 59.090 | 80.272 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.870527177778385, 'AP50': 76.2785338802829, 'AP75': 62.61462108643045, 'APs': 34.87696799131979, 'APm': 59.08950482993457, 'APl': 80.27196950733777})])\n",
      "Putting scalar for val/AP:  52.870527177778385 at iter 2400 storage at iter  2400\n",
      "Early stopping counter:  1 / 5\n",
      "\u001b[32m[06/09 05:45:57 d2.utils.events]: \u001b[0m eta: 6:09:58  iter: 2419  total_loss: 0.2096  loss_cls: 0.0441  loss_box_reg: 0.1655  loss_rpn_cls: 0.002188  loss_rpn_loc: 0.005545  time: 0.8062  data_time: 0.0088  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:46:13 d2.utils.events]: \u001b[0m eta: 6:09:40  iter: 2439  total_loss: 0.1974  loss_cls: 0.04682  loss_box_reg: 0.1533  loss_rpn_cls: 0.003015  loss_rpn_loc: 0.004981  time: 0.8061  data_time: 0.0068  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:46:29 d2.utils.events]: \u001b[0m eta: 6:09:25  iter: 2459  total_loss: 0.2238  loss_cls: 0.04734  loss_box_reg: 0.158  loss_rpn_cls: 0.00456  loss_rpn_loc: 0.01022  time: 0.8060  data_time: 0.0088  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:46:46 d2.utils.events]: \u001b[0m eta: 6:09:28  iter: 2479  total_loss: 0.2021  loss_cls: 0.03794  loss_box_reg: 0.1525  loss_rpn_cls: 0.002336  loss_rpn_loc: 0.005376  time: 0.8061  data_time: 0.0067  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:47:02 d2.utils.events]: \u001b[0m eta: 6:09:38  iter: 2499  total_loss: 0.2151  loss_cls: 0.03442  loss_box_reg: 0.1543  loss_rpn_cls: 0.003519  loss_rpn_loc: 0.006123  time: 0.8061  data_time: 0.0070  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 2500\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:47:03 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:47:08 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0023 s/iter. Inference: 0.4502 s/iter. Eval: 0.0006 s/iter. Total: 0.4532 s/iter. ETA=0:00:35\n",
      "\u001b[32m[06/09 05:47:13 d2.evaluation.evaluator]: \u001b[0mInference done 22/89. Dataloading: 0.0028 s/iter. Inference: 0.4507 s/iter. Eval: 0.0006 s/iter. Total: 0.4542 s/iter. ETA=0:00:30\n",
      "\u001b[32m[06/09 05:47:18 d2.evaluation.evaluator]: \u001b[0mInference done 34/89. Dataloading: 0.0029 s/iter. Inference: 0.4484 s/iter. Eval: 0.0006 s/iter. Total: 0.4520 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:47:24 d2.evaluation.evaluator]: \u001b[0mInference done 46/89. Dataloading: 0.0029 s/iter. Inference: 0.4476 s/iter. Eval: 0.0006 s/iter. Total: 0.4513 s/iter. ETA=0:00:19\n",
      "\u001b[32m[06/09 05:47:29 d2.evaluation.evaluator]: \u001b[0mInference done 58/89. Dataloading: 0.0029 s/iter. Inference: 0.4466 s/iter. Eval: 0.0006 s/iter. Total: 0.4503 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:47:34 d2.evaluation.evaluator]: \u001b[0mInference done 70/89. Dataloading: 0.0029 s/iter. Inference: 0.4460 s/iter. Eval: 0.0006 s/iter. Total: 0.4497 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:47:40 d2.evaluation.evaluator]: \u001b[0mInference done 82/89. Dataloading: 0.0029 s/iter. Inference: 0.4454 s/iter. Eval: 0.0006 s/iter. Total: 0.4491 s/iter. ETA=0:00:03\n",
      "\u001b[32m[06/09 05:47:43 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.444256 (0.445765 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:47:43 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.441706 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:47:43 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:47:43 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:47:43 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:47:43 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:47:43 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "\u001b[32m[06/09 05:47:43 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:47:43 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.529\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.767\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.605\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.354\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.595\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.798\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.488\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.659\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.660\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.530\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.714\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.860\n",
      "\u001b[32m[06/09 05:47:43 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.887 | 76.701 | 60.495 | 35.435 | 59.515 | 79.812 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.88659945243158, 'AP50': 76.70125393718892, 'AP75': 60.49513601046333, 'APs': 35.434798052180405, 'APm': 59.51478191280701, 'APl': 79.81152582085247})])\n",
      "Putting scalar for val/AP:  52.88659945243158 at iter 2500 storage at iter  2500\n",
      "Early stopping counter:  2 / 5\n",
      "\u001b[32m[06/09 05:47:58 d2.utils.events]: \u001b[0m eta: 6:09:34  iter: 2519  total_loss: 0.2285  loss_cls: 0.04647  loss_box_reg: 0.1839  loss_rpn_cls: 0.002551  loss_rpn_loc: 0.007255  time: 0.8061  data_time: 0.0076  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:48:15 d2.utils.events]: \u001b[0m eta: 6:09:33  iter: 2539  total_loss: 0.2146  loss_cls: 0.04869  loss_box_reg: 0.1617  loss_rpn_cls: 0.002097  loss_rpn_loc: 0.007083  time: 0.8061  data_time: 0.0069  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:48:31 d2.utils.events]: \u001b[0m eta: 6:09:29  iter: 2559  total_loss: 0.2353  loss_cls: 0.05295  loss_box_reg: 0.1699  loss_rpn_cls: 0.003585  loss_rpn_loc: 0.005405  time: 0.8061  data_time: 0.0070  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:48:47 d2.utils.events]: \u001b[0m eta: 6:09:27  iter: 2579  total_loss: 0.2367  loss_cls: 0.04197  loss_box_reg: 0.1769  loss_rpn_cls: 0.002976  loss_rpn_loc: 0.00666  time: 0.8061  data_time: 0.0088  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:49:04 d2.utils.events]: \u001b[0m eta: 6:09:21  iter: 2599  total_loss: 0.211  loss_cls: 0.04087  loss_box_reg: 0.1632  loss_rpn_cls: 0.00237  loss_rpn_loc: 0.005966  time: 0.8061  data_time: 0.0087  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 2600\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:49:05 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:49:10 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0022 s/iter. Inference: 0.4519 s/iter. Eval: 0.0007 s/iter. Total: 0.4548 s/iter. ETA=0:00:35\n",
      "\u001b[32m[06/09 05:49:15 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0026 s/iter. Inference: 0.4506 s/iter. Eval: 0.0007 s/iter. Total: 0.4539 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:49:20 d2.evaluation.evaluator]: \u001b[0mInference done 34/89. Dataloading: 0.0027 s/iter. Inference: 0.4509 s/iter. Eval: 0.0007 s/iter. Total: 0.4544 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:49:26 d2.evaluation.evaluator]: \u001b[0mInference done 46/89. Dataloading: 0.0028 s/iter. Inference: 0.4482 s/iter. Eval: 0.0007 s/iter. Total: 0.4517 s/iter. ETA=0:00:19\n",
      "\u001b[32m[06/09 05:49:31 d2.evaluation.evaluator]: \u001b[0mInference done 58/89. Dataloading: 0.0028 s/iter. Inference: 0.4479 s/iter. Eval: 0.0007 s/iter. Total: 0.4515 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:49:36 d2.evaluation.evaluator]: \u001b[0mInference done 70/89. Dataloading: 0.0028 s/iter. Inference: 0.4467 s/iter. Eval: 0.0007 s/iter. Total: 0.4503 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:49:42 d2.evaluation.evaluator]: \u001b[0mInference done 82/89. Dataloading: 0.0028 s/iter. Inference: 0.4462 s/iter. Eval: 0.0007 s/iter. Total: 0.4498 s/iter. ETA=0:00:03\n",
      "\u001b[32m[06/09 05:49:45 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.495947 (0.446380 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:49:45 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.442426 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:49:45 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:49:45 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:49:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:49:45 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:49:45 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.13 seconds.\n",
      "\u001b[32m[06/09 05:49:45 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:49:45 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.525\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.766\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.612\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.361\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.583\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.795\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.487\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.651\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.651\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.507\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.711\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.871\n",
      "\u001b[32m[06/09 05:49:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.458 | 76.580 | 61.210 | 36.081 | 58.312 | 79.548 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.45792309211809, 'AP50': 76.58006180166005, 'AP75': 61.21022064670715, 'APs': 36.08117150438378, 'APm': 58.311684096730076, 'APl': 79.54836306623909})])\n",
      "Putting scalar for val/AP:  52.45792309211809 at iter 2600 storage at iter  2600\n",
      "Early stopping counter:  3 / 5\n",
      "\u001b[32m[06/09 05:50:00 d2.utils.events]: \u001b[0m eta: 6:09:15  iter: 2619  total_loss: 0.2097  loss_cls: 0.0421  loss_box_reg: 0.1662  loss_rpn_cls: 0.002595  loss_rpn_loc: 0.005513  time: 0.8061  data_time: 0.0092  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:50:17 d2.utils.events]: \u001b[0m eta: 6:09:13  iter: 2639  total_loss: 0.202  loss_cls: 0.04593  loss_box_reg: 0.1508  loss_rpn_cls: 0.002128  loss_rpn_loc: 0.005207  time: 0.8062  data_time: 0.0077  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:50:33 d2.utils.events]: \u001b[0m eta: 6:09:06  iter: 2659  total_loss: 0.2475  loss_cls: 0.05598  loss_box_reg: 0.1741  loss_rpn_cls: 0.003437  loss_rpn_loc: 0.007556  time: 0.8062  data_time: 0.0084  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:50:49 d2.utils.events]: \u001b[0m eta: 6:08:59  iter: 2679  total_loss: 0.2362  loss_cls: 0.04738  loss_box_reg: 0.1729  loss_rpn_cls: 0.003771  loss_rpn_loc: 0.006324  time: 0.8062  data_time: 0.0078  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:51:06 d2.utils.events]: \u001b[0m eta: 6:08:52  iter: 2699  total_loss: 0.2095  loss_cls: 0.03603  loss_box_reg: 0.1605  loss_rpn_cls: 0.003092  loss_rpn_loc: 0.006084  time: 0.8062  data_time: 0.0076  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 2700\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:51:07 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:51:12 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0023 s/iter. Inference: 0.4472 s/iter. Eval: 0.0006 s/iter. Total: 0.4501 s/iter. ETA=0:00:35\n",
      "\u001b[32m[06/09 05:51:17 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0027 s/iter. Inference: 0.4460 s/iter. Eval: 0.0006 s/iter. Total: 0.4494 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:51:23 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0028 s/iter. Inference: 0.4463 s/iter. Eval: 0.0007 s/iter. Total: 0.4499 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:51:28 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0028 s/iter. Inference: 0.4461 s/iter. Eval: 0.0007 s/iter. Total: 0.4497 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 05:51:33 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0029 s/iter. Inference: 0.4456 s/iter. Eval: 0.0007 s/iter. Total: 0.4492 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:51:39 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0029 s/iter. Inference: 0.4451 s/iter. Eval: 0.0007 s/iter. Total: 0.4487 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:51:44 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0029 s/iter. Inference: 0.4444 s/iter. Eval: 0.0006 s/iter. Total: 0.4480 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:51:46 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.378857 (0.444986 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:51:46 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.440970 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:51:46 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:51:46 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:51:46 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:51:46 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:51:47 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001b[32m[06/09 05:51:47 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:51:47 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.524\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.753\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.621\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.348\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.588\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.790\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.477\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.646\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.647\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.512\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.702\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.854\n",
      "\u001b[32m[06/09 05:51:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.427 | 75.255 | 62.134 | 34.769 | 58.799 | 79.042 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.42660869503627, 'AP50': 75.2553753328592, 'AP75': 62.133717259297036, 'APs': 34.76934432246968, 'APm': 58.79893363510414, 'APl': 79.04151556211232})])\n",
      "Putting scalar for val/AP:  52.42660869503627 at iter 2700 storage at iter  2700\n",
      "Early stopping counter:  4 / 5\n",
      "\u001b[32m[06/09 05:52:02 d2.utils.events]: \u001b[0m eta: 6:08:40  iter: 2719  total_loss: 0.2161  loss_cls: 0.04232  loss_box_reg: 0.1756  loss_rpn_cls: 0.002158  loss_rpn_loc: 0.004958  time: 0.8062  data_time: 0.0087  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:52:18 d2.utils.events]: \u001b[0m eta: 6:08:37  iter: 2739  total_loss: 0.2051  loss_cls: 0.03577  loss_box_reg: 0.1536  loss_rpn_cls: 0.00236  loss_rpn_loc: 0.003791  time: 0.8062  data_time: 0.0078  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:52:34 d2.utils.events]: \u001b[0m eta: 6:08:25  iter: 2759  total_loss: 0.2247  loss_cls: 0.04725  loss_box_reg: 0.155  loss_rpn_cls: 0.002383  loss_rpn_loc: 0.00645  time: 0.8062  data_time: 0.0086  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:52:52 d2.utils.events]: \u001b[0m eta: 6:08:08  iter: 2779  total_loss: 0.2043  loss_cls: 0.04374  loss_box_reg: 0.1529  loss_rpn_cls: 0.003208  loss_rpn_loc: 0.004504  time: 0.8062  data_time: 0.0097  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:53:08 d2.utils.events]: \u001b[0m eta: 6:07:47  iter: 2799  total_loss: 0.2419  loss_cls: 0.04231  loss_box_reg: 0.1751  loss_rpn_cls: 0.003488  loss_rpn_loc: 0.009349  time: 0.8062  data_time: 0.0071  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 2800\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:53:10 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:53:15 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0022 s/iter. Inference: 0.4455 s/iter. Eval: 0.0006 s/iter. Total: 0.4483 s/iter. ETA=0:00:34\n",
      "\u001b[32m[06/09 05:53:20 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0027 s/iter. Inference: 0.4423 s/iter. Eval: 0.0006 s/iter. Total: 0.4457 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:53:26 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0028 s/iter. Inference: 0.4418 s/iter. Eval: 0.0006 s/iter. Total: 0.4453 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:53:31 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0028 s/iter. Inference: 0.4415 s/iter. Eval: 0.0007 s/iter. Total: 0.4450 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 05:53:36 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0028 s/iter. Inference: 0.4414 s/iter. Eval: 0.0006 s/iter. Total: 0.4449 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:53:41 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0028 s/iter. Inference: 0.4401 s/iter. Eval: 0.0006 s/iter. Total: 0.4436 s/iter. ETA=0:00:07\n",
      "\u001b[32m[06/09 05:53:47 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0028 s/iter. Inference: 0.4400 s/iter. Eval: 0.0006 s/iter. Total: 0.4435 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:53:49 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.053908 (0.441118 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:53:49 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:36 (0.436926 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:53:49 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:53:49 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:53:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:53:49 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:53:49 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.13 seconds.\n",
      "\u001b[32m[06/09 05:53:49 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:53:49 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.542\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.774\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.648\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.369\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.609\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.799\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.492\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.659\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.659\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.525\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.713\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.867\n",
      "\u001b[32m[06/09 05:53:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 54.156 | 77.363 | 64.822 | 36.943 | 60.942 | 79.852 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 54.15584192481201, 'AP50': 77.36285259928847, 'AP75': 64.82189651117058, 'APs': 36.94314752599152, 'APm': 60.94249103845799, 'APl': 79.85215165640233})])\n",
      "Putting scalar for val/AP:  54.15584192481201 at iter 2800 storage at iter  2800\n",
      "Early stopping counter:  0 / 5\n",
      "\u001b[32m[06/09 05:54:08 d2.utils.events]: \u001b[0m eta: 6:07:30  iter: 2819  total_loss: 0.2065  loss_cls: 0.04124  loss_box_reg: 0.1626  loss_rpn_cls: 0.002547  loss_rpn_loc: 0.005363  time: 0.8062  data_time: 0.0092  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:54:24 d2.utils.events]: \u001b[0m eta: 6:07:16  iter: 2839  total_loss: 0.2207  loss_cls: 0.0435  loss_box_reg: 0.1723  loss_rpn_cls: 0.001767  loss_rpn_loc: 0.005257  time: 0.8062  data_time: 0.0078  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:54:41 d2.utils.events]: \u001b[0m eta: 6:07:01  iter: 2859  total_loss: 0.2215  loss_cls: 0.03806  loss_box_reg: 0.1688  loss_rpn_cls: 0.001424  loss_rpn_loc: 0.00561  time: 0.8061  data_time: 0.0080  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:54:57 d2.utils.events]: \u001b[0m eta: 6:06:41  iter: 2879  total_loss: 0.2311  loss_cls: 0.04149  loss_box_reg: 0.1702  loss_rpn_cls: 0.001857  loss_rpn_loc: 0.007322  time: 0.8061  data_time: 0.0078  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:55:14 d2.utils.events]: \u001b[0m eta: 6:06:30  iter: 2899  total_loss: 0.2184  loss_cls: 0.04174  loss_box_reg: 0.1547  loss_rpn_cls: 0.002629  loss_rpn_loc: 0.005683  time: 0.8062  data_time: 0.0087  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 2900\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:55:15 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:55:20 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0024 s/iter. Inference: 0.4441 s/iter. Eval: 0.0006 s/iter. Total: 0.4471 s/iter. ETA=0:00:34\n",
      "\u001b[32m[06/09 05:55:25 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0034 s/iter. Inference: 0.4449 s/iter. Eval: 0.0006 s/iter. Total: 0.4491 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 05:55:31 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0032 s/iter. Inference: 0.4457 s/iter. Eval: 0.0007 s/iter. Total: 0.4497 s/iter. ETA=0:00:24\n",
      "\u001b[32m[06/09 05:55:36 d2.evaluation.evaluator]: \u001b[0mInference done 46/89. Dataloading: 0.0032 s/iter. Inference: 0.4473 s/iter. Eval: 0.0007 s/iter. Total: 0.4512 s/iter. ETA=0:00:19\n",
      "\u001b[32m[06/09 05:55:41 d2.evaluation.evaluator]: \u001b[0mInference done 58/89. Dataloading: 0.0031 s/iter. Inference: 0.4466 s/iter. Eval: 0.0006 s/iter. Total: 0.4504 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 05:55:46 d2.evaluation.evaluator]: \u001b[0mInference done 70/89. Dataloading: 0.0031 s/iter. Inference: 0.4461 s/iter. Eval: 0.0006 s/iter. Total: 0.4499 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 05:55:52 d2.evaluation.evaluator]: \u001b[0mInference done 82/89. Dataloading: 0.0031 s/iter. Inference: 0.4449 s/iter. Eval: 0.0006 s/iter. Total: 0.4487 s/iter. ETA=0:00:03\n",
      "\u001b[32m[06/09 05:55:55 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:37.432157 (0.445621 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:55:55 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (0.441348 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:55:55 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:55:55 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:55:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:55:55 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:55:55 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "\u001b[32m[06/09 05:55:55 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:55:55 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.527\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.754\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.602\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.362\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.588\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.788\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.489\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.518\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.701\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.861\n",
      "\u001b[32m[06/09 05:55:55 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.679 | 75.387 | 60.235 | 36.161 | 58.817 | 78.809 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.67941185070647, 'AP50': 75.38683681697377, 'AP75': 60.234894610408865, 'APs': 36.1606762561869, 'APm': 58.81656933960724, 'APl': 78.80917356619952})])\n",
      "Putting scalar for val/AP:  52.67941185070647 at iter 2900 storage at iter  2900\n",
      "Early stopping counter:  1 / 5\n",
      "\u001b[32m[06/09 05:56:10 d2.utils.events]: \u001b[0m eta: 6:06:19  iter: 2919  total_loss: 0.2163  loss_cls: 0.03872  loss_box_reg: 0.1664  loss_rpn_cls: 0.003089  loss_rpn_loc: 0.004583  time: 0.8062  data_time: 0.0073  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:56:26 d2.utils.events]: \u001b[0m eta: 6:06:06  iter: 2939  total_loss: 0.2171  loss_cls: 0.04526  loss_box_reg: 0.1538  loss_rpn_cls: 0.001705  loss_rpn_loc: 0.004466  time: 0.8062  data_time: 0.0081  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:56:43 d2.utils.events]: \u001b[0m eta: 6:05:49  iter: 2959  total_loss: 0.2077  loss_cls: 0.04072  loss_box_reg: 0.1504  loss_rpn_cls: 0.003116  loss_rpn_loc: 0.006003  time: 0.8062  data_time: 0.0087  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:56:58 d2.utils.events]: \u001b[0m eta: 6:05:21  iter: 2979  total_loss: 0.1902  loss_cls: 0.03088  loss_box_reg: 0.1427  loss_rpn_cls: 0.002375  loss_rpn_loc: 0.006638  time: 0.8059  data_time: 0.0054  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:57:14 d2.utils.events]: \u001b[0m eta: 6:04:57  iter: 2999  total_loss: 0.2248  loss_cls: 0.0413  loss_box_reg: 0.1711  loss_rpn_cls: 0.002004  loss_rpn_loc: 0.005222  time: 0.8056  data_time: 0.0060  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 3000\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:57:15 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:57:20 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0023 s/iter. Inference: 0.4244 s/iter. Eval: 0.0006 s/iter. Total: 0.4273 s/iter. ETA=0:00:33\n",
      "\u001b[32m[06/09 05:57:25 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0026 s/iter. Inference: 0.4228 s/iter. Eval: 0.0006 s/iter. Total: 0.4261 s/iter. ETA=0:00:28\n",
      "\u001b[32m[06/09 05:57:31 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0026 s/iter. Inference: 0.4245 s/iter. Eval: 0.0006 s/iter. Total: 0.4278 s/iter. ETA=0:00:23\n",
      "\u001b[32m[06/09 05:57:36 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0027 s/iter. Inference: 0.4239 s/iter. Eval: 0.0006 s/iter. Total: 0.4273 s/iter. ETA=0:00:17\n",
      "\u001b[32m[06/09 05:57:41 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0027 s/iter. Inference: 0.4228 s/iter. Eval: 0.0006 s/iter. Total: 0.4262 s/iter. ETA=0:00:12\n",
      "\u001b[32m[06/09 05:57:46 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0027 s/iter. Inference: 0.4215 s/iter. Eval: 0.0006 s/iter. Total: 0.4249 s/iter. ETA=0:00:07\n",
      "\u001b[32m[06/09 05:57:51 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0027 s/iter. Inference: 0.4217 s/iter. Eval: 0.0006 s/iter. Total: 0.4251 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:57:53 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:35.471856 (0.422284 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:57:53 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:35 (0.418533 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:57:53 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:57:53 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:57:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:57:53 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:57:53 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001b[32m[06/09 05:57:53 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:57:53 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.531\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.764\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.623\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.361\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.590\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.796\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.476\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.651\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.527\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.695\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.863\n",
      "\u001b[32m[06/09 05:57:53 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 53.131 | 76.390 | 62.317 | 36.072 | 58.956 | 79.610 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 53.13078063800719, 'AP50': 76.3901482492088, 'AP75': 62.31726756831695, 'APs': 36.07212445840816, 'APm': 58.95590807098956, 'APl': 79.61031095538019})])\n",
      "Putting scalar for val/AP:  53.13078063800719 at iter 3000 storage at iter  3000\n",
      "Early stopping counter:  2 / 5\n",
      "\u001b[32m[06/09 05:58:08 d2.utils.events]: \u001b[0m eta: 6:04:36  iter: 3019  total_loss: 0.2003  loss_cls: 0.03991  loss_box_reg: 0.1558  loss_rpn_cls: 0.001883  loss_rpn_loc: 0.004448  time: 0.8053  data_time: 0.0073  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:58:24 d2.utils.events]: \u001b[0m eta: 6:04:15  iter: 3039  total_loss: 0.2378  loss_cls: 0.04227  loss_box_reg: 0.1748  loss_rpn_cls: 0.002044  loss_rpn_loc: 0.005467  time: 0.8052  data_time: 0.0065  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:58:39 d2.utils.events]: \u001b[0m eta: 6:03:48  iter: 3059  total_loss: 0.2308  loss_cls: 0.03569  loss_box_reg: 0.1756  loss_rpn_cls: 0.001609  loss_rpn_loc: 0.005768  time: 0.8049  data_time: 0.0054  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:58:55 d2.utils.events]: \u001b[0m eta: 6:03:21  iter: 3079  total_loss: 0.2145  loss_cls: 0.03756  loss_box_reg: 0.1547  loss_rpn_cls: 0.002198  loss_rpn_loc: 0.00696  time: 0.8048  data_time: 0.0062  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 05:59:10 d2.utils.events]: \u001b[0m eta: 6:02:52  iter: 3099  total_loss: 0.1898  loss_cls: 0.03623  loss_box_reg: 0.1466  loss_rpn_cls: 0.003999  loss_rpn_loc: 0.004953  time: 0.8046  data_time: 0.0067  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 3100\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 05:59:11 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 05:59:16 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0023 s/iter. Inference: 0.4250 s/iter. Eval: 0.0006 s/iter. Total: 0.4279 s/iter. ETA=0:00:33\n",
      "\u001b[32m[06/09 05:59:21 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0026 s/iter. Inference: 0.4212 s/iter. Eval: 0.0006 s/iter. Total: 0.4245 s/iter. ETA=0:00:28\n",
      "\u001b[32m[06/09 05:59:26 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0027 s/iter. Inference: 0.4209 s/iter. Eval: 0.0006 s/iter. Total: 0.4242 s/iter. ETA=0:00:22\n",
      "\u001b[32m[06/09 05:59:31 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0027 s/iter. Inference: 0.4215 s/iter. Eval: 0.0006 s/iter. Total: 0.4248 s/iter. ETA=0:00:17\n",
      "\u001b[32m[06/09 05:59:36 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0027 s/iter. Inference: 0.4220 s/iter. Eval: 0.0006 s/iter. Total: 0.4254 s/iter. ETA=0:00:12\n",
      "\u001b[32m[06/09 05:59:42 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0027 s/iter. Inference: 0.4223 s/iter. Eval: 0.0006 s/iter. Total: 0.4257 s/iter. ETA=0:00:07\n",
      "\u001b[32m[06/09 05:59:47 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0027 s/iter. Inference: 0.4229 s/iter. Eval: 0.0006 s/iter. Total: 0.4263 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 05:59:49 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:35.578027 (0.423548 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:59:49 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:35 (0.419690 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 05:59:49 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 05:59:49 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 05:59:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 05:59:49 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 05:59:49 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.12 seconds.\n",
      "\u001b[32m[06/09 05:59:49 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 05:59:49 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.524\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.759\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.597\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.344\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.593\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.793\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.480\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.641\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.642\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.504\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.701\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.850\n",
      "\u001b[32m[06/09 05:59:49 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.410 | 75.920 | 59.734 | 34.381 | 59.318 | 79.257 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.409624177146966, 'AP50': 75.91972267629234, 'AP75': 59.734307738178714, 'APs': 34.38051314477288, 'APm': 59.31839971417403, 'APl': 79.25712025541328})])\n",
      "Putting scalar for val/AP:  52.409624177146966 at iter 3100 storage at iter  3100\n",
      "Early stopping counter:  3 / 5\n",
      "\u001b[32m[06/09 06:00:04 d2.utils.events]: \u001b[0m eta: 6:02:27  iter: 3119  total_loss: 0.2044  loss_cls: 0.03614  loss_box_reg: 0.1588  loss_rpn_cls: 0.00149  loss_rpn_loc: 0.005578  time: 0.8044  data_time: 0.0061  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 06:00:19 d2.utils.events]: \u001b[0m eta: 6:01:53  iter: 3139  total_loss: 0.227  loss_cls: 0.04428  loss_box_reg: 0.1656  loss_rpn_cls: 0.002636  loss_rpn_loc: 0.006081  time: 0.8042  data_time: 0.0060  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 06:00:35 d2.utils.events]: \u001b[0m eta: 6:01:26  iter: 3159  total_loss: 0.2079  loss_cls: 0.04253  loss_box_reg: 0.1591  loss_rpn_cls: 0.002165  loss_rpn_loc: 0.004522  time: 0.8040  data_time: 0.0060  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 06:00:51 d2.utils.events]: \u001b[0m eta: 6:01:01  iter: 3179  total_loss: 0.2274  loss_cls: 0.04244  loss_box_reg: 0.1635  loss_rpn_cls: 0.001915  loss_rpn_loc: 0.005568  time: 0.8038  data_time: 0.0056  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 06:01:06 d2.utils.events]: \u001b[0m eta: 6:00:26  iter: 3199  total_loss: 0.2242  loss_cls: 0.03265  loss_box_reg: 0.165  loss_rpn_cls: 0.001116  loss_rpn_loc: 0.004884  time: 0.8036  data_time: 0.0059  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 3200\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 06:01:07 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 06:01:12 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0019 s/iter. Inference: 0.4239 s/iter. Eval: 0.0006 s/iter. Total: 0.4264 s/iter. ETA=0:00:33\n",
      "\u001b[32m[06/09 06:01:17 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0024 s/iter. Inference: 0.4229 s/iter. Eval: 0.0006 s/iter. Total: 0.4260 s/iter. ETA=0:00:28\n",
      "\u001b[32m[06/09 06:01:23 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0026 s/iter. Inference: 0.4248 s/iter. Eval: 0.0006 s/iter. Total: 0.4281 s/iter. ETA=0:00:23\n",
      "\u001b[32m[06/09 06:01:28 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0027 s/iter. Inference: 0.4231 s/iter. Eval: 0.0006 s/iter. Total: 0.4264 s/iter. ETA=0:00:17\n",
      "\u001b[32m[06/09 06:01:33 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0027 s/iter. Inference: 0.4232 s/iter. Eval: 0.0006 s/iter. Total: 0.4265 s/iter. ETA=0:00:12\n",
      "\u001b[32m[06/09 06:01:38 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0027 s/iter. Inference: 0.4228 s/iter. Eval: 0.0006 s/iter. Total: 0.4262 s/iter. ETA=0:00:07\n",
      "\u001b[32m[06/09 06:01:43 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0027 s/iter. Inference: 0.4234 s/iter. Eval: 0.0006 s/iter. Total: 0.4268 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 06:01:45 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:35.574077 (0.423501 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 06:01:45 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:35 (0.419778 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 06:01:45 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 06:01:45 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 06:01:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 06:01:45 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 06:01:45 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001b[32m[06/09 06:01:45 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 06:01:45 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.523\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.755\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.620\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.354\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.588\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.794\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.483\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.642\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.643\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.504\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.699\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.857\n",
      "\u001b[32m[06/09 06:01:45 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.344 | 75.537 | 61.988 | 35.440 | 58.760 | 79.359 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.34361872258352, 'AP50': 75.53718984986884, 'AP75': 61.98818611910557, 'APs': 35.44013780001431, 'APm': 58.75962345578108, 'APl': 79.35898947122858})])\n",
      "Putting scalar for val/AP:  52.34361872258352 at iter 3200 storage at iter  3200\n",
      "Early stopping counter:  4 / 5\n",
      "\u001b[32m[06/09 06:02:00 d2.utils.events]: \u001b[0m eta: 6:00:02  iter: 3219  total_loss: 0.2172  loss_cls: 0.03474  loss_box_reg: 0.1581  loss_rpn_cls: 0.00258  loss_rpn_loc: 0.006093  time: 0.8035  data_time: 0.0065  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 06:02:16 d2.utils.events]: \u001b[0m eta: 5:59:33  iter: 3239  total_loss: 0.2429  loss_cls: 0.04083  loss_box_reg: 0.1802  loss_rpn_cls: 0.00151  loss_rpn_loc: 0.004661  time: 0.8033  data_time: 0.0056  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 06:02:32 d2.utils.events]: \u001b[0m eta: 5:58:57  iter: 3259  total_loss: 0.2186  loss_cls: 0.04008  loss_box_reg: 0.1635  loss_rpn_cls: 0.000881  loss_rpn_loc: 0.004659  time: 0.8031  data_time: 0.0061  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 06:02:47 d2.utils.events]: \u001b[0m eta: 5:58:22  iter: 3279  total_loss: 0.2227  loss_cls: 0.04348  loss_box_reg: 0.1735  loss_rpn_cls: 0.001352  loss_rpn_loc: 0.004591  time: 0.8030  data_time: 0.0059  lr: 0.00025  max_mem: 4944M\n",
      "\u001b[32m[06/09 06:03:03 d2.utils.events]: \u001b[0m eta: 5:57:42  iter: 3299  total_loss: 0.2128  loss_cls: 0.04344  loss_box_reg: 0.1516  loss_rpn_cls: 0.002075  loss_rpn_loc: 0.006001  time: 0.8028  data_time: 0.0061  lr: 0.00025  max_mem: 4944M\n",
      "after_step at step 3300\n",
      "starting evaluation!!!\n",
      "\u001b[32m[06/09 06:03:04 d2.evaluation.evaluator]: \u001b[0mStart inference on 89 batches\n",
      "\u001b[32m[06/09 06:03:09 d2.evaluation.evaluator]: \u001b[0mInference done 11/89. Dataloading: 0.0019 s/iter. Inference: 0.4224 s/iter. Eval: 0.0006 s/iter. Total: 0.4249 s/iter. ETA=0:00:33\n",
      "\u001b[32m[06/09 06:03:14 d2.evaluation.evaluator]: \u001b[0mInference done 23/89. Dataloading: 0.0025 s/iter. Inference: 0.4253 s/iter. Eval: 0.0006 s/iter. Total: 0.4285 s/iter. ETA=0:00:28\n",
      "\u001b[32m[06/09 06:03:19 d2.evaluation.evaluator]: \u001b[0mInference done 35/89. Dataloading: 0.0026 s/iter. Inference: 0.4240 s/iter. Eval: 0.0006 s/iter. Total: 0.4273 s/iter. ETA=0:00:23\n",
      "\u001b[32m[06/09 06:03:24 d2.evaluation.evaluator]: \u001b[0mInference done 47/89. Dataloading: 0.0026 s/iter. Inference: 0.4243 s/iter. Eval: 0.0006 s/iter. Total: 0.4276 s/iter. ETA=0:00:17\n",
      "\u001b[32m[06/09 06:03:30 d2.evaluation.evaluator]: \u001b[0mInference done 59/89. Dataloading: 0.0026 s/iter. Inference: 0.4240 s/iter. Eval: 0.0006 s/iter. Total: 0.4273 s/iter. ETA=0:00:12\n",
      "\u001b[32m[06/09 06:03:35 d2.evaluation.evaluator]: \u001b[0mInference done 71/89. Dataloading: 0.0027 s/iter. Inference: 0.4244 s/iter. Eval: 0.0006 s/iter. Total: 0.4278 s/iter. ETA=0:00:07\n",
      "\u001b[32m[06/09 06:03:40 d2.evaluation.evaluator]: \u001b[0mInference done 83/89. Dataloading: 0.0027 s/iter. Inference: 0.4242 s/iter. Eval: 0.0006 s/iter. Total: 0.4276 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 06:03:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:35.693114 (0.424918 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 06:03:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:35 (0.421199 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 06:03:42 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 06:03:42 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[06/09 06:03:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 06:03:42 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 06:03:42 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001b[32m[06/09 06:03:42 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 06:03:42 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.524\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.760\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.610\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.346\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.594\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.790\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.478\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.636\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.637\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.498\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.692\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.853\n",
      "\u001b[32m[06/09 06:03:42 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 52.435 | 76.034 | 61.047 | 34.649 | 59.381 | 79.031 |\n",
      "Results:  OrderedDict([('bbox', {'AP': 52.43520176920906, 'AP50': 76.03432353015678, 'AP75': 61.047434434136726, 'APs': 34.649460056884635, 'APm': 59.38136713696175, 'APl': 79.03084382678972})])\n",
      "Putting scalar for val/AP:  52.43520176920906 at iter 3300 storage at iter  3300\n",
      "Early stopping counter:  5 / 5\n",
      "Early stopping triggered\n",
      "\u001b[4m\u001b[5m\u001b[31mERROR\u001b[0m \u001b[32m[06/09 06:03:42 d2.engine.train_loop]: \u001b[0mException during training:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/philip/anaconda3/envs/detector/lib/python3.9/site-packages/detectron2/engine/train_loop.py\", line 150, in train\n",
      "    self.after_step()\n",
      "  File \"/home/philip/anaconda3/envs/detector/lib/python3.9/site-packages/detectron2/engine/train_loop.py\", line 180, in after_step\n",
      "    h.after_step()\n",
      "  File \"/home/philip/Desktop/Masterthesis/masterthesis/ObjectDetection/EarlyStopping.py\", line 92, in after_step\n",
      "    raise Exception(\"Early stopping\")\n",
      "Exception: Early stopping\n",
      "\u001b[32m[06/09 06:03:42 d2.engine.hooks]: \u001b[0mOverall training speed: 3298 iterations in 0:44:08 (0.8031 s / it)\n",
      "\u001b[32m[06/09 06:03:42 d2.engine.hooks]: \u001b[0mTotal training time: 1:07:50 (0:23:41 on hooks)\n",
      "\u001b[32m[06/09 06:03:42 d2.utils.events]: \u001b[0m eta: 5:57:36  iter: 3300  total_loss: 0.2057  loss_cls: 0.04344  loss_box_reg: 0.1407  loss_rpn_cls: 0.002442  loss_rpn_loc: 0.006001  time: 0.8028  data_time: 0.0061  lr: 0.00025  max_mem: 4944M\n",
      "Training stopped early\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from EarlyStopping import EarlyStopping\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "\n",
    "# cfg = get_cfg()\n",
    "# trainer = DefaultTrainer(cfg)\n",
    "# trainer.register_callback(EarlyStopping(cfg, trainer.model, patience=3, eval_period=1))\n",
    "# trainer.resume_or_load(resume=False)\n",
    "# trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "# cfg.DATASETS.TRAIN = (\"dataset_train_snowy\", \"dataset_train_snowy2\")\n",
    "# cfg.DATASETS.TRAIN = (\"dataset_train_snowy_clean\", \"dataset_train_snowy2_clean\", \"dataset_train_reduced\",) # (\"dataset_train_original\", \"dataset_train_generated_cleaned\")\n",
    "# cfg.DATASETS.TRAIN = (\"snowy_day_train\", )\n",
    "# registered_generated_data_ad.append(\"dataset_train_reduced\")\n",
    "# cfg.DATASETS.TRAIN = (\"dataset_train_reduced\", )\n",
    "cfg.DATASETS.TRAIN = tuple(registered_generated_data_ad)\n",
    "# cfg.DATASETS.VAL = (\"snowy_day_val\", )\n",
    "cfg.DATASETS.VAL = (\"dataset_val\", )\n",
    "# cfg.DATASETS.VAL = (\"dataset_val\", ) # , \"dataset_train_original\", \"dataset_train_generated\", \"dataset_train_generated_cleaned\")\n",
    "cfg.DATASETS.TEST = () # \"dataset_test\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4  # This is the real \"batch size\" commonly known to deep learning people\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 30000    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # The \"RoIHead batch size\". 128 is faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 # TODO: set to 1 again!!!  # only has one class. (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
    "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
    "# cfg.EVAL_PERIOD = 1000\n",
    "\n",
    "# log every step instead of every 20 steps\n",
    "cfg.SOLVER.LOG_PERIOD = 1\n",
    "\n",
    "# also train on images without labels\n",
    "\n",
    "\n",
    "# store each run in a new directory for better logging\n",
    "out_dir = cfg.OUTPUT_DIR\n",
    "existing_dirs = os.listdir(out_dir)\n",
    "cfg.OUTPUT_DIR = os.path.join(out_dir, \"run_\" + str(len(existing_dirs)))\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# create a notes.txt file to store notes about the run, such as the used training data\n",
    "notes = {}\n",
    "notes[\"train_data\"] = cfg.DATASETS.TRAIN\n",
    "notes[\"val_data\"] = cfg.DATASETS.VAL\n",
    "\n",
    "with open(os.path.join(cfg.OUTPUT_DIR, \"notes.txt\"), \"w\") as f:\n",
    "    json.dump(notes, f)\n",
    "\n",
    "val_evaluator = COCOEvaluator(cfg.DATASETS.VAL[0], cfg, True, output_dir=\"./output/\")\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.register_hooks([EarlyStopping(cfg, trainer.model, val_evaluator, patience=5, eval_period=100)]) # 500\n",
    "trainer.resume_or_load(resume=False)\n",
    "try:    \n",
    "    trainer.train()\n",
    "except:\n",
    "    print(\"Training stopped early\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "id": "hBXeH8UXFcqU",
    "outputId": "9fc116b5-5831-43f5-9fdd-721039b6c87a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-be4f76f9578b63c4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-be4f76f9578b63c4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6009;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at training curves in tensorboard:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e4vdDIOXyxF"
   },
   "source": [
    "## Inference & evaluation using the trained model\n",
    "Now, let's run inference with the trained model on the balloon validation dataset. First, let's create a predictor using the model we just trained:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Ya5nEuMELeq8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/09 06:03:45 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./output/run_182/model_best.pth ...\n"
     ]
    }
   ],
   "source": [
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "# cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_best.pth\")  # path to the model we just trained\n",
    "# cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.007 # set a custom testing threshold\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7 # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWq1XHfDWiXO"
   },
   "source": [
    "Then, we randomly select several samples to visualize the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "U5LhISJqWXgM",
    "outputId": "2d8dc713-4368-434d-d86d-8163693e678d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/09 06:03:46 d2.data.datasets.coco]: \u001b[0mLoaded 3438 images in COCO format from ../data/Waste_Bin_Detection_Dataset/cloudy_2021_04_09_16_02_cam5_filtered (validation and test dataset images and ground truths)/validation_set_static_gt.json\n"
     ]
    }
   ],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "dataset_dicts = DatasetCatalog.get(\"dataset_val\")\n",
    "metadata = MetadataCatalog.get(\"dataset_val\")\n",
    "counter = 0\n",
    "for d in dataset_dicts:\n",
    "    if counter > 2:\n",
    "        break\n",
    "# for d in random.sample(dataset_dicts, 30):    \n",
    "    if len(d[\"annotations\"]) == 0:\n",
    "        continue\n",
    "\n",
    "    counter += 1\n",
    "    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=metadata, \n",
    "                   scale=0.5#, \n",
    "                #    instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    # draw the sample with the correct label and the predicted label\n",
    "    correct_out = v.draw_dataset_dict(d)\n",
    "    cv2_imshow(correct_out.get_image()[:, :, ::-1], 4000)\n",
    "\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1], 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_to_evaluate = [\"dataset_val\", \"dataset_train_original\", \"dataset_train_generated\", \"dataset_train_generated_cleaned\"] # \"dataset_val\", \n",
    "# for dataset in dataset_to_evaluate:\n",
    "#     # print out which classes are in the dataset\n",
    "#     classes = MetadataCatalog.get(dataset)#.thing_classes\n",
    "#     print(\"Classes in {}: {}\".format(dataset, classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kblA1IyFvWbT"
   },
   "source": [
    "We can also evaluate its performance using AP metric implemented in COCO API.\n",
    "This gives an AP of ~70. Not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(os.path.join(cfg.OUTPUT_DIR, \"inferences.pkl\"), \"wb\") as f:\n",
    "#         pickle.dump(inferences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the inferences\n",
    "# with open(os.path.join(cfg.OUTPUT_DIR, \"inferences.pkl\"), \"rb\") as f:\n",
    "#     inferences_re = pickle.load(f)\n",
    "# print(inferences_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferences = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h9tECBQCvMv3",
    "outputId": "c9f72ae9-d23c-44f7-8c52-8dd575400810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/09 06:04:35 d2.data.datasets.coco]: \u001b[0mLoaded 3437 images in COCO format from ../data/Waste_Bin_Detection_Dataset/cloudy_2021_04_09_16_02_cam5_filtered (validation and test dataset images and ground truths)/test_set_static_gt.json\n",
      "\u001b[32m[06/09 06:04:35 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[06/09 06:04:35 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>\n",
      "\u001b[32m[06/09 06:04:35 d2.data.common]: \u001b[0mSerializing 3437 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[06/09 06:04:35 d2.data.common]: \u001b[0mSerialized dataset takes 1.01 MiB\n",
      "\u001b[32m[06/09 06:04:35 d2.evaluation.evaluator]: \u001b[0mStart inference on 3437 batches\n",
      "\u001b[32m[06/09 06:04:36 d2.evaluation.evaluator]: \u001b[0mInference done 11/3437. Dataloading: 0.0007 s/iter. Inference: 0.0987 s/iter. Eval: 0.0001 s/iter. Total: 0.0995 s/iter. ETA=0:05:40\n",
      "\u001b[32m[06/09 06:04:41 d2.evaluation.evaluator]: \u001b[0mInference done 60/3437. Dataloading: 0.0010 s/iter. Inference: 0.1007 s/iter. Eval: 0.0001 s/iter. Total: 0.1018 s/iter. ETA=0:05:43\n",
      "\u001b[32m[06/09 06:04:46 d2.evaluation.evaluator]: \u001b[0mInference done 110/3437. Dataloading: 0.0010 s/iter. Inference: 0.1005 s/iter. Eval: 0.0001 s/iter. Total: 0.1016 s/iter. ETA=0:05:38\n",
      "\u001b[32m[06/09 06:04:51 d2.evaluation.evaluator]: \u001b[0mInference done 158/3437. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0001 s/iter. Total: 0.1030 s/iter. ETA=0:05:37\n",
      "\u001b[32m[06/09 06:04:56 d2.evaluation.evaluator]: \u001b[0mInference done 205/3437. Dataloading: 0.0011 s/iter. Inference: 0.1026 s/iter. Eval: 0.0001 s/iter. Total: 0.1038 s/iter. ETA=0:05:35\n",
      "\u001b[32m[06/09 06:05:01 d2.evaluation.evaluator]: \u001b[0mInference done 252/3437. Dataloading: 0.0011 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:05:32\n",
      "\u001b[32m[06/09 06:05:06 d2.evaluation.evaluator]: \u001b[0mInference done 299/3437. Dataloading: 0.0011 s/iter. Inference: 0.1038 s/iter. Eval: 0.0001 s/iter. Total: 0.1050 s/iter. ETA=0:05:29\n",
      "\u001b[32m[06/09 06:05:11 d2.evaluation.evaluator]: \u001b[0mInference done 346/3437. Dataloading: 0.0011 s/iter. Inference: 0.1042 s/iter. Eval: 0.0001 s/iter. Total: 0.1054 s/iter. ETA=0:05:25\n",
      "\u001b[32m[06/09 06:05:17 d2.evaluation.evaluator]: \u001b[0mInference done 394/3437. Dataloading: 0.0011 s/iter. Inference: 0.1042 s/iter. Eval: 0.0001 s/iter. Total: 0.1055 s/iter. ETA=0:05:20\n",
      "\u001b[32m[06/09 06:05:22 d2.evaluation.evaluator]: \u001b[0mInference done 441/3437. Dataloading: 0.0011 s/iter. Inference: 0.1046 s/iter. Eval: 0.0001 s/iter. Total: 0.1058 s/iter. ETA=0:05:16\n",
      "\u001b[32m[06/09 06:05:27 d2.evaluation.evaluator]: \u001b[0mInference done 488/3437. Dataloading: 0.0011 s/iter. Inference: 0.1048 s/iter. Eval: 0.0001 s/iter. Total: 0.1061 s/iter. ETA=0:05:12\n",
      "\u001b[32m[06/09 06:05:32 d2.evaluation.evaluator]: \u001b[0mInference done 535/3437. Dataloading: 0.0011 s/iter. Inference: 0.1050 s/iter. Eval: 0.0001 s/iter. Total: 0.1062 s/iter. ETA=0:05:08\n",
      "\u001b[32m[06/09 06:05:37 d2.evaluation.evaluator]: \u001b[0mInference done 581/3437. Dataloading: 0.0011 s/iter. Inference: 0.1054 s/iter. Eval: 0.0001 s/iter. Total: 0.1066 s/iter. ETA=0:05:04\n",
      "\u001b[32m[06/09 06:05:42 d2.evaluation.evaluator]: \u001b[0mInference done 627/3437. Dataloading: 0.0011 s/iter. Inference: 0.1055 s/iter. Eval: 0.0001 s/iter. Total: 0.1068 s/iter. ETA=0:05:00\n",
      "\u001b[32m[06/09 06:05:47 d2.evaluation.evaluator]: \u001b[0mInference done 674/3437. Dataloading: 0.0011 s/iter. Inference: 0.1056 s/iter. Eval: 0.0001 s/iter. Total: 0.1069 s/iter. ETA=0:04:55\n",
      "\u001b[32m[06/09 06:05:52 d2.evaluation.evaluator]: \u001b[0mInference done 720/3437. Dataloading: 0.0011 s/iter. Inference: 0.1058 s/iter. Eval: 0.0001 s/iter. Total: 0.1071 s/iter. ETA=0:04:50\n",
      "\u001b[32m[06/09 06:05:57 d2.evaluation.evaluator]: \u001b[0mInference done 766/3437. Dataloading: 0.0011 s/iter. Inference: 0.1059 s/iter. Eval: 0.0001 s/iter. Total: 0.1072 s/iter. ETA=0:04:46\n",
      "\u001b[32m[06/09 06:06:02 d2.evaluation.evaluator]: \u001b[0mInference done 812/3437. Dataloading: 0.0011 s/iter. Inference: 0.1060 s/iter. Eval: 0.0001 s/iter. Total: 0.1073 s/iter. ETA=0:04:41\n",
      "\u001b[32m[06/09 06:06:07 d2.evaluation.evaluator]: \u001b[0mInference done 858/3437. Dataloading: 0.0011 s/iter. Inference: 0.1062 s/iter. Eval: 0.0001 s/iter. Total: 0.1074 s/iter. ETA=0:04:37\n",
      "\u001b[32m[06/09 06:06:12 d2.evaluation.evaluator]: \u001b[0mInference done 904/3437. Dataloading: 0.0011 s/iter. Inference: 0.1063 s/iter. Eval: 0.0001 s/iter. Total: 0.1075 s/iter. ETA=0:04:32\n",
      "\u001b[32m[06/09 06:06:17 d2.evaluation.evaluator]: \u001b[0mInference done 950/3437. Dataloading: 0.0011 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1077 s/iter. ETA=0:04:27\n",
      "\u001b[32m[06/09 06:06:22 d2.evaluation.evaluator]: \u001b[0mInference done 996/3437. Dataloading: 0.0011 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1077 s/iter. ETA=0:04:23\n",
      "\u001b[32m[06/09 06:06:27 d2.evaluation.evaluator]: \u001b[0mInference done 1042/3437. Dataloading: 0.0011 s/iter. Inference: 0.1066 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:04:18\n",
      "\u001b[32m[06/09 06:06:32 d2.evaluation.evaluator]: \u001b[0mInference done 1088/3437. Dataloading: 0.0011 s/iter. Inference: 0.1067 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:04:13\n",
      "\u001b[32m[06/09 06:06:37 d2.evaluation.evaluator]: \u001b[0mInference done 1133/3437. Dataloading: 0.0011 s/iter. Inference: 0.1068 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:04:09\n",
      "\u001b[32m[06/09 06:06:42 d2.evaluation.evaluator]: \u001b[0mInference done 1179/3437. Dataloading: 0.0011 s/iter. Inference: 0.1069 s/iter. Eval: 0.0001 s/iter. Total: 0.1082 s/iter. ETA=0:04:04\n",
      "\u001b[32m[06/09 06:06:48 d2.evaluation.evaluator]: \u001b[0mInference done 1224/3437. Dataloading: 0.0011 s/iter. Inference: 0.1070 s/iter. Eval: 0.0001 s/iter. Total: 0.1083 s/iter. ETA=0:03:59\n",
      "\u001b[32m[06/09 06:06:53 d2.evaluation.evaluator]: \u001b[0mInference done 1269/3437. Dataloading: 0.0011 s/iter. Inference: 0.1072 s/iter. Eval: 0.0001 s/iter. Total: 0.1084 s/iter. ETA=0:03:55\n",
      "\u001b[32m[06/09 06:06:58 d2.evaluation.evaluator]: \u001b[0mInference done 1314/3437. Dataloading: 0.0011 s/iter. Inference: 0.1073 s/iter. Eval: 0.0001 s/iter. Total: 0.1085 s/iter. ETA=0:03:50\n",
      "\u001b[32m[06/09 06:07:03 d2.evaluation.evaluator]: \u001b[0mInference done 1360/3437. Dataloading: 0.0011 s/iter. Inference: 0.1073 s/iter. Eval: 0.0001 s/iter. Total: 0.1086 s/iter. ETA=0:03:45\n",
      "\u001b[32m[06/09 06:07:08 d2.evaluation.evaluator]: \u001b[0mInference done 1406/3437. Dataloading: 0.0011 s/iter. Inference: 0.1073 s/iter. Eval: 0.0001 s/iter. Total: 0.1086 s/iter. ETA=0:03:40\n",
      "\u001b[32m[06/09 06:07:13 d2.evaluation.evaluator]: \u001b[0mInference done 1452/3437. Dataloading: 0.0011 s/iter. Inference: 0.1074 s/iter. Eval: 0.0001 s/iter. Total: 0.1086 s/iter. ETA=0:03:35\n",
      "\u001b[32m[06/09 06:07:18 d2.evaluation.evaluator]: \u001b[0mInference done 1498/3437. Dataloading: 0.0011 s/iter. Inference: 0.1074 s/iter. Eval: 0.0001 s/iter. Total: 0.1087 s/iter. ETA=0:03:30\n",
      "\u001b[32m[06/09 06:07:23 d2.evaluation.evaluator]: \u001b[0mInference done 1544/3437. Dataloading: 0.0011 s/iter. Inference: 0.1075 s/iter. Eval: 0.0001 s/iter. Total: 0.1087 s/iter. ETA=0:03:25\n",
      "\u001b[32m[06/09 06:07:28 d2.evaluation.evaluator]: \u001b[0mInference done 1590/3437. Dataloading: 0.0011 s/iter. Inference: 0.1075 s/iter. Eval: 0.0001 s/iter. Total: 0.1087 s/iter. ETA=0:03:20\n",
      "\u001b[32m[06/09 06:07:33 d2.evaluation.evaluator]: \u001b[0mInference done 1636/3437. Dataloading: 0.0011 s/iter. Inference: 0.1075 s/iter. Eval: 0.0001 s/iter. Total: 0.1088 s/iter. ETA=0:03:15\n",
      "\u001b[32m[06/09 06:07:38 d2.evaluation.evaluator]: \u001b[0mInference done 1682/3437. Dataloading: 0.0011 s/iter. Inference: 0.1075 s/iter. Eval: 0.0001 s/iter. Total: 0.1088 s/iter. ETA=0:03:10\n",
      "\u001b[32m[06/09 06:07:43 d2.evaluation.evaluator]: \u001b[0mInference done 1728/3437. Dataloading: 0.0011 s/iter. Inference: 0.1076 s/iter. Eval: 0.0001 s/iter. Total: 0.1088 s/iter. ETA=0:03:05\n",
      "\u001b[32m[06/09 06:07:48 d2.evaluation.evaluator]: \u001b[0mInference done 1774/3437. Dataloading: 0.0011 s/iter. Inference: 0.1076 s/iter. Eval: 0.0001 s/iter. Total: 0.1088 s/iter. ETA=0:03:01\n",
      "\u001b[32m[06/09 06:07:53 d2.evaluation.evaluator]: \u001b[0mInference done 1820/3437. Dataloading: 0.0011 s/iter. Inference: 0.1076 s/iter. Eval: 0.0001 s/iter. Total: 0.1089 s/iter. ETA=0:02:56\n",
      "\u001b[32m[06/09 06:07:58 d2.evaluation.evaluator]: \u001b[0mInference done 1866/3437. Dataloading: 0.0011 s/iter. Inference: 0.1076 s/iter. Eval: 0.0001 s/iter. Total: 0.1089 s/iter. ETA=0:02:51\n",
      "\u001b[32m[06/09 06:08:03 d2.evaluation.evaluator]: \u001b[0mInference done 1912/3437. Dataloading: 0.0011 s/iter. Inference: 0.1077 s/iter. Eval: 0.0001 s/iter. Total: 0.1089 s/iter. ETA=0:02:46\n",
      "\u001b[32m[06/09 06:08:08 d2.evaluation.evaluator]: \u001b[0mInference done 1958/3437. Dataloading: 0.0011 s/iter. Inference: 0.1077 s/iter. Eval: 0.0001 s/iter. Total: 0.1090 s/iter. ETA=0:02:41\n",
      "\u001b[32m[06/09 06:08:13 d2.evaluation.evaluator]: \u001b[0mInference done 2004/3437. Dataloading: 0.0011 s/iter. Inference: 0.1077 s/iter. Eval: 0.0001 s/iter. Total: 0.1090 s/iter. ETA=0:02:36\n",
      "\u001b[32m[06/09 06:08:18 d2.evaluation.evaluator]: \u001b[0mInference done 2050/3437. Dataloading: 0.0011 s/iter. Inference: 0.1078 s/iter. Eval: 0.0001 s/iter. Total: 0.1090 s/iter. ETA=0:02:31\n",
      "\u001b[32m[06/09 06:08:23 d2.evaluation.evaluator]: \u001b[0mInference done 2095/3437. Dataloading: 0.0011 s/iter. Inference: 0.1078 s/iter. Eval: 0.0001 s/iter. Total: 0.1091 s/iter. ETA=0:02:26\n",
      "\u001b[32m[06/09 06:08:28 d2.evaluation.evaluator]: \u001b[0mInference done 2141/3437. Dataloading: 0.0011 s/iter. Inference: 0.1078 s/iter. Eval: 0.0001 s/iter. Total: 0.1091 s/iter. ETA=0:02:21\n",
      "\u001b[32m[06/09 06:08:34 d2.evaluation.evaluator]: \u001b[0mInference done 2188/3437. Dataloading: 0.0011 s/iter. Inference: 0.1078 s/iter. Eval: 0.0001 s/iter. Total: 0.1091 s/iter. ETA=0:02:16\n",
      "\u001b[32m[06/09 06:08:39 d2.evaluation.evaluator]: \u001b[0mInference done 2234/3437. Dataloading: 0.0011 s/iter. Inference: 0.1078 s/iter. Eval: 0.0001 s/iter. Total: 0.1091 s/iter. ETA=0:02:11\n",
      "\u001b[32m[06/09 06:08:44 d2.evaluation.evaluator]: \u001b[0mInference done 2280/3437. Dataloading: 0.0011 s/iter. Inference: 0.1079 s/iter. Eval: 0.0001 s/iter. Total: 0.1091 s/iter. ETA=0:02:06\n",
      "\u001b[32m[06/09 06:08:49 d2.evaluation.evaluator]: \u001b[0mInference done 2326/3437. Dataloading: 0.0011 s/iter. Inference: 0.1079 s/iter. Eval: 0.0001 s/iter. Total: 0.1091 s/iter. ETA=0:02:01\n",
      "\u001b[32m[06/09 06:08:54 d2.evaluation.evaluator]: \u001b[0mInference done 2373/3437. Dataloading: 0.0011 s/iter. Inference: 0.1079 s/iter. Eval: 0.0001 s/iter. Total: 0.1091 s/iter. ETA=0:01:56\n",
      "\u001b[32m[06/09 06:08:59 d2.evaluation.evaluator]: \u001b[0mInference done 2420/3437. Dataloading: 0.0011 s/iter. Inference: 0.1078 s/iter. Eval: 0.0001 s/iter. Total: 0.1091 s/iter. ETA=0:01:50\n",
      "\u001b[32m[06/09 06:09:04 d2.evaluation.evaluator]: \u001b[0mInference done 2467/3437. Dataloading: 0.0011 s/iter. Inference: 0.1078 s/iter. Eval: 0.0001 s/iter. Total: 0.1090 s/iter. ETA=0:01:45\n",
      "\u001b[32m[06/09 06:09:09 d2.evaluation.evaluator]: \u001b[0mInference done 2514/3437. Dataloading: 0.0011 s/iter. Inference: 0.1077 s/iter. Eval: 0.0001 s/iter. Total: 0.1090 s/iter. ETA=0:01:40\n",
      "\u001b[32m[06/09 06:09:14 d2.evaluation.evaluator]: \u001b[0mInference done 2561/3437. Dataloading: 0.0011 s/iter. Inference: 0.1077 s/iter. Eval: 0.0001 s/iter. Total: 0.1090 s/iter. ETA=0:01:35\n",
      "\u001b[32m[06/09 06:09:19 d2.evaluation.evaluator]: \u001b[0mInference done 2609/3437. Dataloading: 0.0011 s/iter. Inference: 0.1077 s/iter. Eval: 0.0001 s/iter. Total: 0.1089 s/iter. ETA=0:01:30\n",
      "\u001b[32m[06/09 06:09:24 d2.evaluation.evaluator]: \u001b[0mInference done 2656/3437. Dataloading: 0.0011 s/iter. Inference: 0.1076 s/iter. Eval: 0.0001 s/iter. Total: 0.1089 s/iter. ETA=0:01:25\n",
      "\u001b[32m[06/09 06:09:29 d2.evaluation.evaluator]: \u001b[0mInference done 2703/3437. Dataloading: 0.0011 s/iter. Inference: 0.1076 s/iter. Eval: 0.0001 s/iter. Total: 0.1089 s/iter. ETA=0:01:19\n",
      "\u001b[32m[06/09 06:09:34 d2.evaluation.evaluator]: \u001b[0mInference done 2751/3437. Dataloading: 0.0011 s/iter. Inference: 0.1075 s/iter. Eval: 0.0001 s/iter. Total: 0.1088 s/iter. ETA=0:01:14\n",
      "\u001b[32m[06/09 06:09:39 d2.evaluation.evaluator]: \u001b[0mInference done 2799/3437. Dataloading: 0.0011 s/iter. Inference: 0.1075 s/iter. Eval: 0.0001 s/iter. Total: 0.1087 s/iter. ETA=0:01:09\n",
      "\u001b[32m[06/09 06:09:44 d2.evaluation.evaluator]: \u001b[0mInference done 2847/3437. Dataloading: 0.0011 s/iter. Inference: 0.1074 s/iter. Eval: 0.0001 s/iter. Total: 0.1087 s/iter. ETA=0:01:04\n",
      "\u001b[32m[06/09 06:09:49 d2.evaluation.evaluator]: \u001b[0mInference done 2895/3437. Dataloading: 0.0011 s/iter. Inference: 0.1074 s/iter. Eval: 0.0001 s/iter. Total: 0.1086 s/iter. ETA=0:00:58\n",
      "\u001b[32m[06/09 06:09:55 d2.evaluation.evaluator]: \u001b[0mInference done 2943/3437. Dataloading: 0.0011 s/iter. Inference: 0.1073 s/iter. Eval: 0.0001 s/iter. Total: 0.1086 s/iter. ETA=0:00:53\n",
      "\u001b[32m[06/09 06:10:00 d2.evaluation.evaluator]: \u001b[0mInference done 2991/3437. Dataloading: 0.0011 s/iter. Inference: 0.1073 s/iter. Eval: 0.0001 s/iter. Total: 0.1085 s/iter. ETA=0:00:48\n",
      "\u001b[32m[06/09 06:10:05 d2.evaluation.evaluator]: \u001b[0mInference done 3039/3437. Dataloading: 0.0011 s/iter. Inference: 0.1072 s/iter. Eval: 0.0001 s/iter. Total: 0.1085 s/iter. ETA=0:00:43\n",
      "\u001b[32m[06/09 06:10:10 d2.evaluation.evaluator]: \u001b[0mInference done 3087/3437. Dataloading: 0.0011 s/iter. Inference: 0.1072 s/iter. Eval: 0.0001 s/iter. Total: 0.1084 s/iter. ETA=0:00:37\n",
      "\u001b[32m[06/09 06:10:15 d2.evaluation.evaluator]: \u001b[0mInference done 3135/3437. Dataloading: 0.0011 s/iter. Inference: 0.1071 s/iter. Eval: 0.0001 s/iter. Total: 0.1084 s/iter. ETA=0:00:32\n",
      "\u001b[32m[06/09 06:10:20 d2.evaluation.evaluator]: \u001b[0mInference done 3184/3437. Dataloading: 0.0011 s/iter. Inference: 0.1071 s/iter. Eval: 0.0001 s/iter. Total: 0.1083 s/iter. ETA=0:00:27\n",
      "\u001b[32m[06/09 06:10:25 d2.evaluation.evaluator]: \u001b[0mInference done 3231/3437. Dataloading: 0.0011 s/iter. Inference: 0.1071 s/iter. Eval: 0.0001 s/iter. Total: 0.1083 s/iter. ETA=0:00:22\n",
      "\u001b[32m[06/09 06:10:30 d2.evaluation.evaluator]: \u001b[0mInference done 3278/3437. Dataloading: 0.0011 s/iter. Inference: 0.1070 s/iter. Eval: 0.0001 s/iter. Total: 0.1083 s/iter. ETA=0:00:17\n",
      "\u001b[32m[06/09 06:10:35 d2.evaluation.evaluator]: \u001b[0mInference done 3326/3437. Dataloading: 0.0011 s/iter. Inference: 0.1070 s/iter. Eval: 0.0001 s/iter. Total: 0.1082 s/iter. ETA=0:00:12\n",
      "\u001b[32m[06/09 06:10:40 d2.evaluation.evaluator]: \u001b[0mInference done 3373/3437. Dataloading: 0.0011 s/iter. Inference: 0.1070 s/iter. Eval: 0.0001 s/iter. Total: 0.1082 s/iter. ETA=0:00:06\n",
      "\u001b[32m[06/09 06:10:45 d2.evaluation.evaluator]: \u001b[0mInference done 3421/3437. Dataloading: 0.0011 s/iter. Inference: 0.1070 s/iter. Eval: 0.0001 s/iter. Total: 0.1082 s/iter. ETA=0:00:01\n",
      "\u001b[32m[06/09 06:10:47 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:06:11.321672 (0.108194 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 06:10:47 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:06:07 (0.106939 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 06:10:47 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 06:10:47 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/dataset_test/coco_instances_results.json\n",
      "\u001b[32m[06/09 06:10:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 06:10:47 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 06:10:47 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001b[32m[06/09 06:10:47 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 06:10:47 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.425\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.578\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.519\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.218\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.503\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.757\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.451\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.558\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.558\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.374\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.637\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.828\n",
      "\u001b[32m[06/09 06:10:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 42.548 | 57.815 | 51.854 | 21.842 | 50.296 | 75.707 |\n",
      "OrderedDict([('bbox', {'AP': 42.547654967388496, 'AP50': 57.8150493000695, 'AP75': 51.8538636826284, 'APs': 21.84157292791832, 'APm': 50.29613930194129, 'APl': 75.70727061900836})])\n",
      "\u001b[32m[06/09 06:10:47 d2.data.datasets.coco]: \u001b[0mLoaded 4214 images in COCO format from ../data//test_datasets/snowy/snowy_day.json\n",
      "\u001b[32m[06/09 06:10:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[06/09 06:10:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>\n",
      "\u001b[32m[06/09 06:10:47 d2.data.common]: \u001b[0mSerializing 4214 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[06/09 06:10:47 d2.data.common]: \u001b[0mSerialized dataset takes 0.61 MiB\n",
      "\u001b[32m[06/09 06:10:47 d2.evaluation.evaluator]: \u001b[0mStart inference on 4214 batches\n",
      "\u001b[32m[06/09 06:10:50 d2.evaluation.evaluator]: \u001b[0mInference done 28/4214. Dataloading: 0.0023 s/iter. Inference: 0.1048 s/iter. Eval: 0.0001 s/iter. Total: 0.1072 s/iter. ETA=0:07:28\n",
      "\u001b[32m[06/09 06:10:55 d2.evaluation.evaluator]: \u001b[0mInference done 75/4214. Dataloading: 0.0015 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1080 s/iter. ETA=0:07:26\n",
      "\u001b[32m[06/09 06:11:00 d2.evaluation.evaluator]: \u001b[0mInference done 122/4214. Dataloading: 0.0013 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:07:21\n",
      "\u001b[32m[06/09 06:11:05 d2.evaluation.evaluator]: \u001b[0mInference done 170/4214. Dataloading: 0.0012 s/iter. Inference: 0.1060 s/iter. Eval: 0.0001 s/iter. Total: 0.1073 s/iter. ETA=0:07:14\n",
      "\u001b[32m[06/09 06:11:11 d2.evaluation.evaluator]: \u001b[0mInference done 217/4214. Dataloading: 0.0012 s/iter. Inference: 0.1062 s/iter. Eval: 0.0001 s/iter. Total: 0.1076 s/iter. ETA=0:07:09\n",
      "\u001b[32m[06/09 06:11:16 d2.evaluation.evaluator]: \u001b[0mInference done 263/4214. Dataloading: 0.0012 s/iter. Inference: 0.1066 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:07:06\n",
      "\u001b[32m[06/09 06:11:21 d2.evaluation.evaluator]: \u001b[0mInference done 309/4214. Dataloading: 0.0012 s/iter. Inference: 0.1069 s/iter. Eval: 0.0001 s/iter. Total: 0.1082 s/iter. ETA=0:07:02\n",
      "\u001b[32m[06/09 06:11:26 d2.evaluation.evaluator]: \u001b[0mInference done 356/4214. Dataloading: 0.0012 s/iter. Inference: 0.1068 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:06:57\n",
      "\u001b[32m[06/09 06:11:31 d2.evaluation.evaluator]: \u001b[0mInference done 402/4214. Dataloading: 0.0012 s/iter. Inference: 0.1070 s/iter. Eval: 0.0001 s/iter. Total: 0.1083 s/iter. ETA=0:06:52\n",
      "\u001b[32m[06/09 06:11:36 d2.evaluation.evaluator]: \u001b[0mInference done 449/4214. Dataloading: 0.0012 s/iter. Inference: 0.1069 s/iter. Eval: 0.0001 s/iter. Total: 0.1082 s/iter. ETA=0:06:47\n",
      "\u001b[32m[06/09 06:11:41 d2.evaluation.evaluator]: \u001b[0mInference done 497/4214. Dataloading: 0.0012 s/iter. Inference: 0.1066 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:06:40\n",
      "\u001b[32m[06/09 06:11:46 d2.evaluation.evaluator]: \u001b[0mInference done 544/4214. Dataloading: 0.0011 s/iter. Inference: 0.1066 s/iter. Eval: 0.0001 s/iter. Total: 0.1078 s/iter. ETA=0:06:35\n",
      "\u001b[32m[06/09 06:11:51 d2.evaluation.evaluator]: \u001b[0mInference done 588/4214. Dataloading: 0.0016 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1083 s/iter. ETA=0:06:32\n",
      "\u001b[32m[06/09 06:11:56 d2.evaluation.evaluator]: \u001b[0mInference done 636/4214. Dataloading: 0.0016 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:06:26\n",
      "\u001b[32m[06/09 06:12:01 d2.evaluation.evaluator]: \u001b[0mInference done 683/4214. Dataloading: 0.0016 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:06:21\n",
      "\u001b[32m[06/09 06:12:06 d2.evaluation.evaluator]: \u001b[0mInference done 730/4214. Dataloading: 0.0015 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:06:16\n",
      "\u001b[32m[06/09 06:12:11 d2.evaluation.evaluator]: \u001b[0mInference done 777/4214. Dataloading: 0.0015 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1080 s/iter. ETA=0:06:11\n",
      "\u001b[32m[06/09 06:12:16 d2.evaluation.evaluator]: \u001b[0mInference done 824/4214. Dataloading: 0.0015 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1080 s/iter. ETA=0:06:06\n",
      "\u001b[32m[06/09 06:12:21 d2.evaluation.evaluator]: \u001b[0mInference done 871/4214. Dataloading: 0.0015 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1080 s/iter. ETA=0:06:00\n",
      "\u001b[32m[06/09 06:12:26 d2.evaluation.evaluator]: \u001b[0mInference done 917/4214. Dataloading: 0.0014 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1080 s/iter. ETA=0:05:56\n",
      "\u001b[32m[06/09 06:12:31 d2.evaluation.evaluator]: \u001b[0mInference done 964/4214. Dataloading: 0.0014 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:05:51\n",
      "\u001b[32m[06/09 06:12:36 d2.evaluation.evaluator]: \u001b[0mInference done 1012/4214. Dataloading: 0.0014 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1080 s/iter. ETA=0:05:45\n",
      "\u001b[32m[06/09 06:12:42 d2.evaluation.evaluator]: \u001b[0mInference done 1060/4214. Dataloading: 0.0014 s/iter. Inference: 0.1063 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:05:40\n",
      "\u001b[32m[06/09 06:12:47 d2.evaluation.evaluator]: \u001b[0mInference done 1107/4214. Dataloading: 0.0014 s/iter. Inference: 0.1063 s/iter. Eval: 0.0001 s/iter. Total: 0.1078 s/iter. ETA=0:05:35\n",
      "\u001b[32m[06/09 06:12:52 d2.evaluation.evaluator]: \u001b[0mInference done 1154/4214. Dataloading: 0.0014 s/iter. Inference: 0.1063 s/iter. Eval: 0.0001 s/iter. Total: 0.1078 s/iter. ETA=0:05:29\n",
      "\u001b[32m[06/09 06:12:57 d2.evaluation.evaluator]: \u001b[0mInference done 1201/4214. Dataloading: 0.0014 s/iter. Inference: 0.1063 s/iter. Eval: 0.0001 s/iter. Total: 0.1078 s/iter. ETA=0:05:24\n",
      "\u001b[32m[06/09 06:13:02 d2.evaluation.evaluator]: \u001b[0mInference done 1247/4214. Dataloading: 0.0013 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:05:20\n",
      "\u001b[32m[06/09 06:13:07 d2.evaluation.evaluator]: \u001b[0mInference done 1294/4214. Dataloading: 0.0013 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:05:14\n",
      "\u001b[32m[06/09 06:13:12 d2.evaluation.evaluator]: \u001b[0mInference done 1341/4214. Dataloading: 0.0013 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1078 s/iter. ETA=0:05:09\n",
      "\u001b[32m[06/09 06:13:17 d2.evaluation.evaluator]: \u001b[0mInference done 1388/4214. Dataloading: 0.0013 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1078 s/iter. ETA=0:05:04\n",
      "\u001b[32m[06/09 06:13:22 d2.evaluation.evaluator]: \u001b[0mInference done 1434/4214. Dataloading: 0.0013 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:04:59\n",
      "\u001b[32m[06/09 06:13:27 d2.evaluation.evaluator]: \u001b[0mInference done 1481/4214. Dataloading: 0.0013 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:04:54\n",
      "\u001b[32m[06/09 06:13:32 d2.evaluation.evaluator]: \u001b[0mInference done 1528/4214. Dataloading: 0.0013 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:04:49\n",
      "\u001b[32m[06/09 06:13:37 d2.evaluation.evaluator]: \u001b[0mInference done 1576/4214. Dataloading: 0.0013 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1078 s/iter. ETA=0:04:44\n",
      "\u001b[32m[06/09 06:13:42 d2.evaluation.evaluator]: \u001b[0mInference done 1623/4214. Dataloading: 0.0013 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1078 s/iter. ETA=0:04:39\n",
      "\u001b[32m[06/09 06:13:47 d2.evaluation.evaluator]: \u001b[0mInference done 1670/4214. Dataloading: 0.0013 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1078 s/iter. ETA=0:04:34\n",
      "\u001b[32m[06/09 06:13:52 d2.evaluation.evaluator]: \u001b[0mInference done 1716/4214. Dataloading: 0.0013 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1078 s/iter. ETA=0:04:29\n",
      "\u001b[32m[06/09 06:13:57 d2.evaluation.evaluator]: \u001b[0mInference done 1763/4214. Dataloading: 0.0013 s/iter. Inference: 0.1064 s/iter. Eval: 0.0001 s/iter. Total: 0.1078 s/iter. ETA=0:04:24\n",
      "\u001b[32m[06/09 06:14:02 d2.evaluation.evaluator]: \u001b[0mInference done 1809/4214. Dataloading: 0.0013 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:04:19\n",
      "\u001b[32m[06/09 06:14:07 d2.evaluation.evaluator]: \u001b[0mInference done 1855/4214. Dataloading: 0.0013 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:04:14\n",
      "\u001b[32m[06/09 06:14:12 d2.evaluation.evaluator]: \u001b[0mInference done 1902/4214. Dataloading: 0.0013 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:04:09\n",
      "\u001b[32m[06/09 06:14:17 d2.evaluation.evaluator]: \u001b[0mInference done 1949/4214. Dataloading: 0.0013 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:04:04\n",
      "\u001b[32m[06/09 06:14:23 d2.evaluation.evaluator]: \u001b[0mInference done 1996/4214. Dataloading: 0.0013 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:03:59\n",
      "\u001b[32m[06/09 06:14:28 d2.evaluation.evaluator]: \u001b[0mInference done 2043/4214. Dataloading: 0.0013 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:03:54\n",
      "\u001b[32m[06/09 06:14:33 d2.evaluation.evaluator]: \u001b[0mInference done 2090/4214. Dataloading: 0.0012 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:03:49\n",
      "\u001b[32m[06/09 06:14:38 d2.evaluation.evaluator]: \u001b[0mInference done 2137/4214. Dataloading: 0.0012 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:03:44\n",
      "\u001b[32m[06/09 06:14:43 d2.evaluation.evaluator]: \u001b[0mInference done 2184/4214. Dataloading: 0.0012 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:03:38\n",
      "\u001b[32m[06/09 06:14:48 d2.evaluation.evaluator]: \u001b[0mInference done 2231/4214. Dataloading: 0.0012 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:03:33\n",
      "\u001b[32m[06/09 06:14:53 d2.evaluation.evaluator]: \u001b[0mInference done 2277/4214. Dataloading: 0.0012 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:03:28\n",
      "\u001b[32m[06/09 06:14:58 d2.evaluation.evaluator]: \u001b[0mInference done 2323/4214. Dataloading: 0.0012 s/iter. Inference: 0.1066 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:03:24\n",
      "\u001b[32m[06/09 06:15:03 d2.evaluation.evaluator]: \u001b[0mInference done 2370/4214. Dataloading: 0.0012 s/iter. Inference: 0.1066 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:03:19\n",
      "\u001b[32m[06/09 06:15:08 d2.evaluation.evaluator]: \u001b[0mInference done 2417/4214. Dataloading: 0.0012 s/iter. Inference: 0.1065 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:03:13\n",
      "\u001b[32m[06/09 06:15:13 d2.evaluation.evaluator]: \u001b[0mInference done 2463/4214. Dataloading: 0.0012 s/iter. Inference: 0.1066 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:03:08\n",
      "\u001b[32m[06/09 06:15:18 d2.evaluation.evaluator]: \u001b[0mInference done 2510/4214. Dataloading: 0.0012 s/iter. Inference: 0.1066 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:03:03\n",
      "\u001b[32m[06/09 06:15:23 d2.evaluation.evaluator]: \u001b[0mInference done 2556/4214. Dataloading: 0.0012 s/iter. Inference: 0.1066 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:02:58\n",
      "\u001b[32m[06/09 06:15:28 d2.evaluation.evaluator]: \u001b[0mInference done 2603/4214. Dataloading: 0.0012 s/iter. Inference: 0.1066 s/iter. Eval: 0.0001 s/iter. Total: 0.1079 s/iter. ETA=0:02:53\n",
      "\u001b[32m[06/09 06:15:33 d2.evaluation.evaluator]: \u001b[0mInference done 2649/4214. Dataloading: 0.0012 s/iter. Inference: 0.1066 s/iter. Eval: 0.0001 s/iter. Total: 0.1080 s/iter. ETA=0:02:48\n",
      "\u001b[32m[06/09 06:15:38 d2.evaluation.evaluator]: \u001b[0mInference done 2694/4214. Dataloading: 0.0012 s/iter. Inference: 0.1067 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:02:44\n",
      "\u001b[32m[06/09 06:15:43 d2.evaluation.evaluator]: \u001b[0mInference done 2741/4214. Dataloading: 0.0012 s/iter. Inference: 0.1067 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:02:39\n",
      "\u001b[32m[06/09 06:15:48 d2.evaluation.evaluator]: \u001b[0mInference done 2788/4214. Dataloading: 0.0012 s/iter. Inference: 0.1067 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:02:34\n",
      "\u001b[32m[06/09 06:15:54 d2.evaluation.evaluator]: \u001b[0mInference done 2834/4214. Dataloading: 0.0012 s/iter. Inference: 0.1068 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:02:29\n",
      "\u001b[32m[06/09 06:15:59 d2.evaluation.evaluator]: \u001b[0mInference done 2881/4214. Dataloading: 0.0012 s/iter. Inference: 0.1068 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:02:24\n",
      "\u001b[32m[06/09 06:16:04 d2.evaluation.evaluator]: \u001b[0mInference done 2927/4214. Dataloading: 0.0012 s/iter. Inference: 0.1068 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:02:19\n",
      "\u001b[32m[06/09 06:16:09 d2.evaluation.evaluator]: \u001b[0mInference done 2975/4214. Dataloading: 0.0012 s/iter. Inference: 0.1067 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:02:13\n",
      "\u001b[32m[06/09 06:16:14 d2.evaluation.evaluator]: \u001b[0mInference done 3022/4214. Dataloading: 0.0012 s/iter. Inference: 0.1067 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:02:08\n",
      "\u001b[32m[06/09 06:16:19 d2.evaluation.evaluator]: \u001b[0mInference done 3069/4214. Dataloading: 0.0012 s/iter. Inference: 0.1067 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:02:03\n",
      "\u001b[32m[06/09 06:16:24 d2.evaluation.evaluator]: \u001b[0mInference done 3115/4214. Dataloading: 0.0012 s/iter. Inference: 0.1068 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:01:58\n",
      "\u001b[32m[06/09 06:16:29 d2.evaluation.evaluator]: \u001b[0mInference done 3161/4214. Dataloading: 0.0012 s/iter. Inference: 0.1068 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:01:53\n",
      "\u001b[32m[06/09 06:16:34 d2.evaluation.evaluator]: \u001b[0mInference done 3208/4214. Dataloading: 0.0012 s/iter. Inference: 0.1068 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:01:48\n",
      "\u001b[32m[06/09 06:16:39 d2.evaluation.evaluator]: \u001b[0mInference done 3255/4214. Dataloading: 0.0012 s/iter. Inference: 0.1068 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:01:43\n",
      "\u001b[32m[06/09 06:16:44 d2.evaluation.evaluator]: \u001b[0mInference done 3302/4214. Dataloading: 0.0012 s/iter. Inference: 0.1068 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:01:38\n",
      "\u001b[32m[06/09 06:16:49 d2.evaluation.evaluator]: \u001b[0mInference done 3349/4214. Dataloading: 0.0012 s/iter. Inference: 0.1068 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:01:33\n",
      "\u001b[32m[06/09 06:16:54 d2.evaluation.evaluator]: \u001b[0mInference done 3395/4214. Dataloading: 0.0012 s/iter. Inference: 0.1068 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:01:28\n",
      "\u001b[32m[06/09 06:16:59 d2.evaluation.evaluator]: \u001b[0mInference done 3442/4214. Dataloading: 0.0012 s/iter. Inference: 0.1068 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:01:23\n",
      "\u001b[32m[06/09 06:17:04 d2.evaluation.evaluator]: \u001b[0mInference done 3488/4214. Dataloading: 0.0012 s/iter. Inference: 0.1069 s/iter. Eval: 0.0001 s/iter. Total: 0.1082 s/iter. ETA=0:01:18\n",
      "\u001b[32m[06/09 06:17:10 d2.evaluation.evaluator]: \u001b[0mInference done 3534/4214. Dataloading: 0.0012 s/iter. Inference: 0.1069 s/iter. Eval: 0.0001 s/iter. Total: 0.1082 s/iter. ETA=0:01:13\n",
      "\u001b[32m[06/09 06:17:15 d2.evaluation.evaluator]: \u001b[0mInference done 3581/4214. Dataloading: 0.0012 s/iter. Inference: 0.1069 s/iter. Eval: 0.0001 s/iter. Total: 0.1082 s/iter. ETA=0:01:08\n",
      "\u001b[32m[06/09 06:17:20 d2.evaluation.evaluator]: \u001b[0mInference done 3628/4214. Dataloading: 0.0012 s/iter. Inference: 0.1069 s/iter. Eval: 0.0001 s/iter. Total: 0.1082 s/iter. ETA=0:01:03\n",
      "\u001b[32m[06/09 06:17:25 d2.evaluation.evaluator]: \u001b[0mInference done 3673/4214. Dataloading: 0.0012 s/iter. Inference: 0.1069 s/iter. Eval: 0.0001 s/iter. Total: 0.1082 s/iter. ETA=0:00:58\n",
      "\u001b[32m[06/09 06:17:30 d2.evaluation.evaluator]: \u001b[0mInference done 3719/4214. Dataloading: 0.0012 s/iter. Inference: 0.1070 s/iter. Eval: 0.0001 s/iter. Total: 0.1083 s/iter. ETA=0:00:53\n",
      "\u001b[32m[06/09 06:17:35 d2.evaluation.evaluator]: \u001b[0mInference done 3765/4214. Dataloading: 0.0012 s/iter. Inference: 0.1070 s/iter. Eval: 0.0001 s/iter. Total: 0.1083 s/iter. ETA=0:00:48\n",
      "\u001b[32m[06/09 06:17:40 d2.evaluation.evaluator]: \u001b[0mInference done 3811/4214. Dataloading: 0.0012 s/iter. Inference: 0.1070 s/iter. Eval: 0.0001 s/iter. Total: 0.1083 s/iter. ETA=0:00:43\n",
      "\u001b[32m[06/09 06:17:45 d2.evaluation.evaluator]: \u001b[0mInference done 3857/4214. Dataloading: 0.0012 s/iter. Inference: 0.1070 s/iter. Eval: 0.0001 s/iter. Total: 0.1083 s/iter. ETA=0:00:38\n",
      "\u001b[32m[06/09 06:17:50 d2.evaluation.evaluator]: \u001b[0mInference done 3903/4214. Dataloading: 0.0012 s/iter. Inference: 0.1070 s/iter. Eval: 0.0001 s/iter. Total: 0.1083 s/iter. ETA=0:00:33\n",
      "\u001b[32m[06/09 06:17:55 d2.evaluation.evaluator]: \u001b[0mInference done 3950/4214. Dataloading: 0.0012 s/iter. Inference: 0.1070 s/iter. Eval: 0.0001 s/iter. Total: 0.1083 s/iter. ETA=0:00:28\n",
      "\u001b[32m[06/09 06:18:00 d2.evaluation.evaluator]: \u001b[0mInference done 3997/4214. Dataloading: 0.0012 s/iter. Inference: 0.1070 s/iter. Eval: 0.0001 s/iter. Total: 0.1083 s/iter. ETA=0:00:23\n",
      "\u001b[32m[06/09 06:18:05 d2.evaluation.evaluator]: \u001b[0mInference done 4046/4214. Dataloading: 0.0012 s/iter. Inference: 0.1070 s/iter. Eval: 0.0001 s/iter. Total: 0.1083 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 06:18:10 d2.evaluation.evaluator]: \u001b[0mInference done 4096/4214. Dataloading: 0.0012 s/iter. Inference: 0.1069 s/iter. Eval: 0.0001 s/iter. Total: 0.1082 s/iter. ETA=0:00:12\n",
      "\u001b[32m[06/09 06:18:15 d2.evaluation.evaluator]: \u001b[0mInference done 4146/4214. Dataloading: 0.0012 s/iter. Inference: 0.1068 s/iter. Eval: 0.0001 s/iter. Total: 0.1081 s/iter. ETA=0:00:07\n",
      "\u001b[32m[06/09 06:18:20 d2.evaluation.evaluator]: \u001b[0mInference done 4195/4214. Dataloading: 0.0012 s/iter. Inference: 0.1067 s/iter. Eval: 0.0001 s/iter. Total: 0.1080 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 06:18:22 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:07:34.595373 (0.108006 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 06:18:22 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:07:29 (0.106703 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 06:18:22 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 06:18:22 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/dataset_snowy_day/coco_instances_results.json\n",
      "\u001b[32m[06/09 06:18:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 06:18:22 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 06:18:22 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001b[32m[06/09 06:18:22 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 06:18:22 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.236\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.356\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.285\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.138\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.299\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.495\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.316\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.404\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.404\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.262\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.502\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.627\n",
      "\u001b[32m[06/09 06:18:22 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 23.600 | 35.603 | 28.463 | 13.760 | 29.930 | 49.543 |\n",
      "OrderedDict([('bbox', {'AP': 23.600083632609127, 'AP50': 35.603254412352406, 'AP75': 28.463029485227004, 'APs': 13.760304126736134, 'APm': 29.93013310806596, 'APl': 49.54303351456491})])\n",
      "\u001b[32m[06/09 06:18:22 d2.data.datasets.coco]: \u001b[0mLoaded 4349 images in COCO format from ../data//test_datasets/snowy/snowy_night.json\n",
      "\u001b[32m[06/09 06:18:22 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[06/09 06:18:22 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>\n",
      "\u001b[32m[06/09 06:18:22 d2.data.common]: \u001b[0mSerializing 4349 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[06/09 06:18:22 d2.data.common]: \u001b[0mSerialized dataset takes 0.62 MiB\n",
      "\u001b[32m[06/09 06:18:22 d2.evaluation.evaluator]: \u001b[0mStart inference on 4349 batches\n",
      "\u001b[32m[06/09 06:18:26 d2.evaluation.evaluator]: \u001b[0mInference done 28/4349. Dataloading: 0.0009 s/iter. Inference: 0.1013 s/iter. Eval: 0.0001 s/iter. Total: 0.1024 s/iter. ETA=0:07:22\n",
      "\u001b[32m[06/09 06:18:31 d2.evaluation.evaluator]: \u001b[0mInference done 78/4349. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0001 s/iter. Total: 0.1020 s/iter. ETA=0:07:15\n",
      "\u001b[32m[06/09 06:18:36 d2.evaluation.evaluator]: \u001b[0mInference done 129/4349. Dataloading: 0.0010 s/iter. Inference: 0.0996 s/iter. Eval: 0.0001 s/iter. Total: 0.1007 s/iter. ETA=0:07:05\n",
      "\u001b[32m[06/09 06:18:41 d2.evaluation.evaluator]: \u001b[0mInference done 178/4349. Dataloading: 0.0010 s/iter. Inference: 0.1004 s/iter. Eval: 0.0001 s/iter. Total: 0.1015 s/iter. ETA=0:07:03\n",
      "\u001b[32m[06/09 06:18:46 d2.evaluation.evaluator]: \u001b[0mInference done 228/4349. Dataloading: 0.0010 s/iter. Inference: 0.1004 s/iter. Eval: 0.0001 s/iter. Total: 0.1016 s/iter. ETA=0:06:58\n",
      "\u001b[32m[06/09 06:18:51 d2.evaluation.evaluator]: \u001b[0mInference done 277/4349. Dataloading: 0.0010 s/iter. Inference: 0.1007 s/iter. Eval: 0.0001 s/iter. Total: 0.1018 s/iter. ETA=0:06:54\n",
      "\u001b[32m[06/09 06:18:56 d2.evaluation.evaluator]: \u001b[0mInference done 326/4349. Dataloading: 0.0010 s/iter. Inference: 0.1008 s/iter. Eval: 0.0001 s/iter. Total: 0.1019 s/iter. ETA=0:06:50\n",
      "\u001b[32m[06/09 06:19:01 d2.evaluation.evaluator]: \u001b[0mInference done 376/4349. Dataloading: 0.0010 s/iter. Inference: 0.1007 s/iter. Eval: 0.0001 s/iter. Total: 0.1018 s/iter. ETA=0:06:44\n",
      "\u001b[32m[06/09 06:19:06 d2.evaluation.evaluator]: \u001b[0mInference done 425/4349. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0001 s/iter. Total: 0.1020 s/iter. ETA=0:06:40\n",
      "\u001b[32m[06/09 06:19:11 d2.evaluation.evaluator]: \u001b[0mInference done 474/4349. Dataloading: 0.0010 s/iter. Inference: 0.1011 s/iter. Eval: 0.0001 s/iter. Total: 0.1022 s/iter. ETA=0:06:36\n",
      "\u001b[32m[06/09 06:19:16 d2.evaluation.evaluator]: \u001b[0mInference done 522/4349. Dataloading: 0.0010 s/iter. Inference: 0.1014 s/iter. Eval: 0.0001 s/iter. Total: 0.1026 s/iter. ETA=0:06:32\n",
      "\u001b[32m[06/09 06:19:21 d2.evaluation.evaluator]: \u001b[0mInference done 571/4349. Dataloading: 0.0010 s/iter. Inference: 0.1015 s/iter. Eval: 0.0001 s/iter. Total: 0.1027 s/iter. ETA=0:06:27\n",
      "\u001b[32m[06/09 06:19:26 d2.evaluation.evaluator]: \u001b[0mInference done 619/4349. Dataloading: 0.0010 s/iter. Inference: 0.1017 s/iter. Eval: 0.0001 s/iter. Total: 0.1028 s/iter. ETA=0:06:23\n",
      "\u001b[32m[06/09 06:19:31 d2.evaluation.evaluator]: \u001b[0mInference done 668/4349. Dataloading: 0.0010 s/iter. Inference: 0.1018 s/iter. Eval: 0.0001 s/iter. Total: 0.1029 s/iter. ETA=0:06:18\n",
      "\u001b[32m[06/09 06:19:36 d2.evaluation.evaluator]: \u001b[0mInference done 716/4349. Dataloading: 0.0010 s/iter. Inference: 0.1019 s/iter. Eval: 0.0001 s/iter. Total: 0.1030 s/iter. ETA=0:06:14\n",
      "\u001b[32m[06/09 06:19:42 d2.evaluation.evaluator]: \u001b[0mInference done 764/4349. Dataloading: 0.0010 s/iter. Inference: 0.1020 s/iter. Eval: 0.0001 s/iter. Total: 0.1032 s/iter. ETA=0:06:09\n",
      "\u001b[32m[06/09 06:19:47 d2.evaluation.evaluator]: \u001b[0mInference done 812/4349. Dataloading: 0.0010 s/iter. Inference: 0.1021 s/iter. Eval: 0.0001 s/iter. Total: 0.1033 s/iter. ETA=0:06:05\n",
      "\u001b[32m[06/09 06:19:52 d2.evaluation.evaluator]: \u001b[0mInference done 860/4349. Dataloading: 0.0010 s/iter. Inference: 0.1022 s/iter. Eval: 0.0001 s/iter. Total: 0.1033 s/iter. ETA=0:06:00\n",
      "\u001b[32m[06/09 06:19:57 d2.evaluation.evaluator]: \u001b[0mInference done 908/4349. Dataloading: 0.0010 s/iter. Inference: 0.1023 s/iter. Eval: 0.0001 s/iter. Total: 0.1035 s/iter. ETA=0:05:56\n",
      "\u001b[32m[06/09 06:20:02 d2.evaluation.evaluator]: \u001b[0mInference done 956/4349. Dataloading: 0.0010 s/iter. Inference: 0.1024 s/iter. Eval: 0.0001 s/iter. Total: 0.1036 s/iter. ETA=0:05:51\n",
      "\u001b[32m[06/09 06:20:07 d2.evaluation.evaluator]: \u001b[0mInference done 1004/4349. Dataloading: 0.0010 s/iter. Inference: 0.1025 s/iter. Eval: 0.0001 s/iter. Total: 0.1037 s/iter. ETA=0:05:46\n",
      "\u001b[32m[06/09 06:20:12 d2.evaluation.evaluator]: \u001b[0mInference done 1053/4349. Dataloading: 0.0010 s/iter. Inference: 0.1025 s/iter. Eval: 0.0001 s/iter. Total: 0.1037 s/iter. ETA=0:05:41\n",
      "\u001b[32m[06/09 06:20:17 d2.evaluation.evaluator]: \u001b[0mInference done 1100/4349. Dataloading: 0.0010 s/iter. Inference: 0.1027 s/iter. Eval: 0.0001 s/iter. Total: 0.1038 s/iter. ETA=0:05:37\n",
      "\u001b[32m[06/09 06:20:22 d2.evaluation.evaluator]: \u001b[0mInference done 1148/4349. Dataloading: 0.0010 s/iter. Inference: 0.1027 s/iter. Eval: 0.0001 s/iter. Total: 0.1039 s/iter. ETA=0:05:32\n",
      "\u001b[32m[06/09 06:20:27 d2.evaluation.evaluator]: \u001b[0mInference done 1197/4349. Dataloading: 0.0010 s/iter. Inference: 0.1027 s/iter. Eval: 0.0001 s/iter. Total: 0.1038 s/iter. ETA=0:05:27\n",
      "\u001b[32m[06/09 06:20:32 d2.evaluation.evaluator]: \u001b[0mInference done 1245/4349. Dataloading: 0.0010 s/iter. Inference: 0.1027 s/iter. Eval: 0.0001 s/iter. Total: 0.1039 s/iter. ETA=0:05:22\n",
      "\u001b[32m[06/09 06:20:37 d2.evaluation.evaluator]: \u001b[0mInference done 1294/4349. Dataloading: 0.0010 s/iter. Inference: 0.1027 s/iter. Eval: 0.0001 s/iter. Total: 0.1039 s/iter. ETA=0:05:17\n",
      "\u001b[32m[06/09 06:20:42 d2.evaluation.evaluator]: \u001b[0mInference done 1342/4349. Dataloading: 0.0010 s/iter. Inference: 0.1028 s/iter. Eval: 0.0001 s/iter. Total: 0.1039 s/iter. ETA=0:05:12\n",
      "\u001b[32m[06/09 06:20:47 d2.evaluation.evaluator]: \u001b[0mInference done 1390/4349. Dataloading: 0.0010 s/iter. Inference: 0.1028 s/iter. Eval: 0.0001 s/iter. Total: 0.1040 s/iter. ETA=0:05:07\n",
      "\u001b[32m[06/09 06:20:52 d2.evaluation.evaluator]: \u001b[0mInference done 1438/4349. Dataloading: 0.0010 s/iter. Inference: 0.1029 s/iter. Eval: 0.0001 s/iter. Total: 0.1040 s/iter. ETA=0:05:02\n",
      "\u001b[32m[06/09 06:20:57 d2.evaluation.evaluator]: \u001b[0mInference done 1486/4349. Dataloading: 0.0010 s/iter. Inference: 0.1029 s/iter. Eval: 0.0001 s/iter. Total: 0.1040 s/iter. ETA=0:04:57\n",
      "\u001b[32m[06/09 06:21:02 d2.evaluation.evaluator]: \u001b[0mInference done 1534/4349. Dataloading: 0.0010 s/iter. Inference: 0.1030 s/iter. Eval: 0.0001 s/iter. Total: 0.1041 s/iter. ETA=0:04:53\n",
      "\u001b[32m[06/09 06:21:07 d2.evaluation.evaluator]: \u001b[0mInference done 1582/4349. Dataloading: 0.0010 s/iter. Inference: 0.1030 s/iter. Eval: 0.0001 s/iter. Total: 0.1042 s/iter. ETA=0:04:48\n",
      "\u001b[32m[06/09 06:21:13 d2.evaluation.evaluator]: \u001b[0mInference done 1630/4349. Dataloading: 0.0010 s/iter. Inference: 0.1031 s/iter. Eval: 0.0001 s/iter. Total: 0.1042 s/iter. ETA=0:04:43\n",
      "\u001b[32m[06/09 06:21:18 d2.evaluation.evaluator]: \u001b[0mInference done 1679/4349. Dataloading: 0.0010 s/iter. Inference: 0.1031 s/iter. Eval: 0.0001 s/iter. Total: 0.1042 s/iter. ETA=0:04:38\n",
      "\u001b[32m[06/09 06:21:23 d2.evaluation.evaluator]: \u001b[0mInference done 1727/4349. Dataloading: 0.0010 s/iter. Inference: 0.1031 s/iter. Eval: 0.0001 s/iter. Total: 0.1042 s/iter. ETA=0:04:33\n",
      "\u001b[32m[06/09 06:21:28 d2.evaluation.evaluator]: \u001b[0mInference done 1775/4349. Dataloading: 0.0010 s/iter. Inference: 0.1031 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:04:28\n",
      "\u001b[32m[06/09 06:21:33 d2.evaluation.evaluator]: \u001b[0mInference done 1823/4349. Dataloading: 0.0010 s/iter. Inference: 0.1031 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:04:23\n",
      "\u001b[32m[06/09 06:21:38 d2.evaluation.evaluator]: \u001b[0mInference done 1872/4349. Dataloading: 0.0010 s/iter. Inference: 0.1031 s/iter. Eval: 0.0001 s/iter. Total: 0.1042 s/iter. ETA=0:04:18\n",
      "\u001b[32m[06/09 06:21:43 d2.evaluation.evaluator]: \u001b[0mInference done 1919/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:04:13\n",
      "\u001b[32m[06/09 06:21:48 d2.evaluation.evaluator]: \u001b[0mInference done 1967/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:04:08\n",
      "\u001b[32m[06/09 06:21:53 d2.evaluation.evaluator]: \u001b[0mInference done 2016/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:04:03\n",
      "\u001b[32m[06/09 06:21:58 d2.evaluation.evaluator]: \u001b[0mInference done 2065/4349. Dataloading: 0.0010 s/iter. Inference: 0.1031 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:03:58\n",
      "\u001b[32m[06/09 06:22:03 d2.evaluation.evaluator]: \u001b[0mInference done 2113/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:03:53\n",
      "\u001b[32m[06/09 06:22:08 d2.evaluation.evaluator]: \u001b[0mInference done 2162/4349. Dataloading: 0.0010 s/iter. Inference: 0.1031 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:03:48\n",
      "\u001b[32m[06/09 06:22:13 d2.evaluation.evaluator]: \u001b[0mInference done 2210/4349. Dataloading: 0.0010 s/iter. Inference: 0.1031 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:03:43\n",
      "\u001b[32m[06/09 06:22:18 d2.evaluation.evaluator]: \u001b[0mInference done 2258/4349. Dataloading: 0.0010 s/iter. Inference: 0.1031 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:03:38\n",
      "\u001b[32m[06/09 06:22:23 d2.evaluation.evaluator]: \u001b[0mInference done 2306/4349. Dataloading: 0.0010 s/iter. Inference: 0.1031 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:03:33\n",
      "\u001b[32m[06/09 06:22:28 d2.evaluation.evaluator]: \u001b[0mInference done 2354/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:03:28\n",
      "\u001b[32m[06/09 06:22:33 d2.evaluation.evaluator]: \u001b[0mInference done 2403/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:03:22\n",
      "\u001b[32m[06/09 06:22:38 d2.evaluation.evaluator]: \u001b[0mInference done 2451/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:03:17\n",
      "\u001b[32m[06/09 06:22:43 d2.evaluation.evaluator]: \u001b[0mInference done 2499/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:03:13\n",
      "\u001b[32m[06/09 06:22:48 d2.evaluation.evaluator]: \u001b[0mInference done 2546/4349. Dataloading: 0.0010 s/iter. Inference: 0.1033 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:03:08\n",
      "\u001b[32m[06/09 06:22:54 d2.evaluation.evaluator]: \u001b[0mInference done 2595/4349. Dataloading: 0.0010 s/iter. Inference: 0.1033 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:03:03\n",
      "\u001b[32m[06/09 06:22:59 d2.evaluation.evaluator]: \u001b[0mInference done 2644/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:02:57\n",
      "\u001b[32m[06/09 06:23:04 d2.evaluation.evaluator]: \u001b[0mInference done 2692/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:02:52\n",
      "\u001b[32m[06/09 06:23:09 d2.evaluation.evaluator]: \u001b[0mInference done 2740/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:02:47\n",
      "\u001b[32m[06/09 06:23:14 d2.evaluation.evaluator]: \u001b[0mInference done 2788/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:02:42\n",
      "\u001b[32m[06/09 06:23:19 d2.evaluation.evaluator]: \u001b[0mInference done 2836/4349. Dataloading: 0.0010 s/iter. Inference: 0.1033 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:02:37\n",
      "\u001b[32m[06/09 06:23:24 d2.evaluation.evaluator]: \u001b[0mInference done 2883/4349. Dataloading: 0.0010 s/iter. Inference: 0.1033 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:02:33\n",
      "\u001b[32m[06/09 06:23:29 d2.evaluation.evaluator]: \u001b[0mInference done 2932/4349. Dataloading: 0.0010 s/iter. Inference: 0.1033 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:02:27\n",
      "\u001b[32m[06/09 06:23:34 d2.evaluation.evaluator]: \u001b[0mInference done 2980/4349. Dataloading: 0.0010 s/iter. Inference: 0.1033 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:02:22\n",
      "\u001b[32m[06/09 06:23:39 d2.evaluation.evaluator]: \u001b[0mInference done 3029/4349. Dataloading: 0.0010 s/iter. Inference: 0.1033 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:02:17\n",
      "\u001b[32m[06/09 06:23:44 d2.evaluation.evaluator]: \u001b[0mInference done 3078/4349. Dataloading: 0.0010 s/iter. Inference: 0.1033 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:02:12\n",
      "\u001b[32m[06/09 06:23:49 d2.evaluation.evaluator]: \u001b[0mInference done 3126/4349. Dataloading: 0.0010 s/iter. Inference: 0.1033 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:02:07\n",
      "\u001b[32m[06/09 06:23:54 d2.evaluation.evaluator]: \u001b[0mInference done 3175/4349. Dataloading: 0.0010 s/iter. Inference: 0.1033 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:02:02\n",
      "\u001b[32m[06/09 06:23:59 d2.evaluation.evaluator]: \u001b[0mInference done 3224/4349. Dataloading: 0.0010 s/iter. Inference: 0.1033 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:01:57\n",
      "\u001b[32m[06/09 06:24:04 d2.evaluation.evaluator]: \u001b[0mInference done 3272/4349. Dataloading: 0.0010 s/iter. Inference: 0.1033 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:01:52\n",
      "\u001b[32m[06/09 06:24:09 d2.evaluation.evaluator]: \u001b[0mInference done 3320/4349. Dataloading: 0.0010 s/iter. Inference: 0.1033 s/iter. Eval: 0.0001 s/iter. Total: 0.1044 s/iter. ETA=0:01:47\n",
      "\u001b[32m[06/09 06:24:14 d2.evaluation.evaluator]: \u001b[0mInference done 3370/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:01:42\n",
      "\u001b[32m[06/09 06:24:19 d2.evaluation.evaluator]: \u001b[0mInference done 3419/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:01:37\n",
      "\u001b[32m[06/09 06:24:24 d2.evaluation.evaluator]: \u001b[0mInference done 3468/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:01:31\n",
      "\u001b[32m[06/09 06:24:30 d2.evaluation.evaluator]: \u001b[0mInference done 3517/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:01:26\n",
      "\u001b[32m[06/09 06:24:35 d2.evaluation.evaluator]: \u001b[0mInference done 3565/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:01:21\n",
      "\u001b[32m[06/09 06:24:40 d2.evaluation.evaluator]: \u001b[0mInference done 3614/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:01:16\n",
      "\u001b[32m[06/09 06:24:45 d2.evaluation.evaluator]: \u001b[0mInference done 3662/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:01:11\n",
      "\u001b[32m[06/09 06:24:50 d2.evaluation.evaluator]: \u001b[0mInference done 3711/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:01:06\n",
      "\u001b[32m[06/09 06:24:55 d2.evaluation.evaluator]: \u001b[0mInference done 3759/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:01:01\n",
      "\u001b[32m[06/09 06:25:00 d2.evaluation.evaluator]: \u001b[0mInference done 3808/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:00:56\n",
      "\u001b[32m[06/09 06:25:05 d2.evaluation.evaluator]: \u001b[0mInference done 3857/4349. Dataloading: 0.0010 s/iter. Inference: 0.1031 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:00:51\n",
      "\u001b[32m[06/09 06:25:10 d2.evaluation.evaluator]: \u001b[0mInference done 3906/4349. Dataloading: 0.0010 s/iter. Inference: 0.1031 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:00:46\n",
      "\u001b[32m[06/09 06:25:15 d2.evaluation.evaluator]: \u001b[0mInference done 3954/4349. Dataloading: 0.0010 s/iter. Inference: 0.1031 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:00:41\n",
      "\u001b[32m[06/09 06:25:20 d2.evaluation.evaluator]: \u001b[0mInference done 4002/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:00:36\n",
      "\u001b[32m[06/09 06:25:25 d2.evaluation.evaluator]: \u001b[0mInference done 4050/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:00:31\n",
      "\u001b[32m[06/09 06:25:30 d2.evaluation.evaluator]: \u001b[0mInference done 4098/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:00:26\n",
      "\u001b[32m[06/09 06:25:35 d2.evaluation.evaluator]: \u001b[0mInference done 4147/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:00:21\n",
      "\u001b[32m[06/09 06:25:40 d2.evaluation.evaluator]: \u001b[0mInference done 4195/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:00:16\n",
      "\u001b[32m[06/09 06:25:45 d2.evaluation.evaluator]: \u001b[0mInference done 4243/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:00:11\n",
      "\u001b[32m[06/09 06:25:50 d2.evaluation.evaluator]: \u001b[0mInference done 4291/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:00:06\n",
      "\u001b[32m[06/09 06:25:55 d2.evaluation.evaluator]: \u001b[0mInference done 4340/4349. Dataloading: 0.0010 s/iter. Inference: 0.1032 s/iter. Eval: 0.0001 s/iter. Total: 0.1043 s/iter. ETA=0:00:00\n",
      "\u001b[32m[06/09 06:25:56 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:07:33.103986 (0.104306 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 06:25:56 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:07:28 (0.103159 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 06:25:56 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 06:25:56 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/dataset_snowy_night/coco_instances_results.json\n",
      "\u001b[32m[06/09 06:25:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 06:25:56 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 06:25:56 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.11 seconds.\n",
      "\u001b[32m[06/09 06:25:56 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 06:25:56 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.278\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.381\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.342\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.310\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.430\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.384\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.418\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.418\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.132\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.465\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.529\n",
      "\u001b[32m[06/09 06:25:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 27.776 | 38.116 | 34.173 | 5.264 | 30.981 | 42.980 |\n",
      "OrderedDict([('bbox', {'AP': 27.776043002759778, 'AP50': 38.116109964977255, 'AP75': 34.17306850805909, 'APs': 5.263774824687438, 'APm': 30.98147964883191, 'APl': 42.98042222575774})])\n",
      "\u001b[32m[06/09 06:25:57 d2.data.datasets.coco]: \u001b[0mLoaded 1377 images in COCO format from ../data//test_datasets/rb/rb.json\n",
      "\u001b[32m[06/09 06:25:57 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[06/09 06:25:57 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>\n",
      "\u001b[32m[06/09 06:25:57 d2.data.common]: \u001b[0mSerializing 1377 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[06/09 06:25:57 d2.data.common]: \u001b[0mSerialized dataset takes 0.18 MiB\n",
      "\u001b[32m[06/09 06:25:57 d2.evaluation.evaluator]: \u001b[0mStart inference on 1377 batches\n",
      "\u001b[32m[06/09 06:26:00 d2.evaluation.evaluator]: \u001b[0mInference done 32/1377. Dataloading: 0.0017 s/iter. Inference: 0.1010 s/iter. Eval: 0.0001 s/iter. Total: 0.1027 s/iter. ETA=0:02:18\n",
      "\u001b[32m[06/09 06:26:05 d2.evaluation.evaluator]: \u001b[0mInference done 81/1377. Dataloading: 0.0013 s/iter. Inference: 0.1018 s/iter. Eval: 0.0001 s/iter. Total: 0.1032 s/iter. ETA=0:02:13\n",
      "\u001b[32m[06/09 06:26:10 d2.evaluation.evaluator]: \u001b[0mInference done 130/1377. Dataloading: 0.0012 s/iter. Inference: 0.1017 s/iter. Eval: 0.0001 s/iter. Total: 0.1030 s/iter. ETA=0:02:08\n",
      "\u001b[32m[06/09 06:26:16 d2.evaluation.evaluator]: \u001b[0mInference done 179/1377. Dataloading: 0.0011 s/iter. Inference: 0.1019 s/iter. Eval: 0.0001 s/iter. Total: 0.1031 s/iter. ETA=0:02:03\n",
      "\u001b[32m[06/09 06:26:21 d2.evaluation.evaluator]: \u001b[0mInference done 228/1377. Dataloading: 0.0011 s/iter. Inference: 0.1017 s/iter. Eval: 0.0001 s/iter. Total: 0.1030 s/iter. ETA=0:01:58\n",
      "\u001b[32m[06/09 06:26:26 d2.evaluation.evaluator]: \u001b[0mInference done 277/1377. Dataloading: 0.0011 s/iter. Inference: 0.1018 s/iter. Eval: 0.0001 s/iter. Total: 0.1030 s/iter. ETA=0:01:53\n",
      "\u001b[32m[06/09 06:26:31 d2.evaluation.evaluator]: \u001b[0mInference done 327/1377. Dataloading: 0.0011 s/iter. Inference: 0.1016 s/iter. Eval: 0.0001 s/iter. Total: 0.1028 s/iter. ETA=0:01:47\n",
      "\u001b[32m[06/09 06:26:36 d2.evaluation.evaluator]: \u001b[0mInference done 376/1377. Dataloading: 0.0011 s/iter. Inference: 0.1015 s/iter. Eval: 0.0001 s/iter. Total: 0.1028 s/iter. ETA=0:01:42\n",
      "\u001b[32m[06/09 06:26:41 d2.evaluation.evaluator]: \u001b[0mInference done 425/1377. Dataloading: 0.0011 s/iter. Inference: 0.1017 s/iter. Eval: 0.0001 s/iter. Total: 0.1029 s/iter. ETA=0:01:37\n",
      "\u001b[32m[06/09 06:26:46 d2.evaluation.evaluator]: \u001b[0mInference done 475/1377. Dataloading: 0.0011 s/iter. Inference: 0.1015 s/iter. Eval: 0.0001 s/iter. Total: 0.1027 s/iter. ETA=0:01:32\n",
      "\u001b[32m[06/09 06:26:51 d2.evaluation.evaluator]: \u001b[0mInference done 524/1377. Dataloading: 0.0011 s/iter. Inference: 0.1015 s/iter. Eval: 0.0001 s/iter. Total: 0.1027 s/iter. ETA=0:01:27\n",
      "\u001b[32m[06/09 06:26:56 d2.evaluation.evaluator]: \u001b[0mInference done 574/1377. Dataloading: 0.0011 s/iter. Inference: 0.1014 s/iter. Eval: 0.0001 s/iter. Total: 0.1027 s/iter. ETA=0:01:22\n",
      "\u001b[32m[06/09 06:27:01 d2.evaluation.evaluator]: \u001b[0mInference done 624/1377. Dataloading: 0.0011 s/iter. Inference: 0.1013 s/iter. Eval: 0.0001 s/iter. Total: 0.1025 s/iter. ETA=0:01:17\n",
      "\u001b[32m[06/09 06:27:06 d2.evaluation.evaluator]: \u001b[0mInference done 674/1377. Dataloading: 0.0011 s/iter. Inference: 0.1012 s/iter. Eval: 0.0001 s/iter. Total: 0.1024 s/iter. ETA=0:01:12\n",
      "\u001b[32m[06/09 06:27:11 d2.evaluation.evaluator]: \u001b[0mInference done 723/1377. Dataloading: 0.0011 s/iter. Inference: 0.1013 s/iter. Eval: 0.0001 s/iter. Total: 0.1025 s/iter. ETA=0:01:07\n",
      "\u001b[32m[06/09 06:27:16 d2.evaluation.evaluator]: \u001b[0mInference done 772/1377. Dataloading: 0.0011 s/iter. Inference: 0.1014 s/iter. Eval: 0.0001 s/iter. Total: 0.1025 s/iter. ETA=0:01:02\n",
      "\u001b[32m[06/09 06:27:21 d2.evaluation.evaluator]: \u001b[0mInference done 820/1377. Dataloading: 0.0011 s/iter. Inference: 0.1015 s/iter. Eval: 0.0001 s/iter. Total: 0.1027 s/iter. ETA=0:00:57\n",
      "\u001b[32m[06/09 06:27:26 d2.evaluation.evaluator]: \u001b[0mInference done 869/1377. Dataloading: 0.0011 s/iter. Inference: 0.1015 s/iter. Eval: 0.0001 s/iter. Total: 0.1027 s/iter. ETA=0:00:52\n",
      "\u001b[32m[06/09 06:27:31 d2.evaluation.evaluator]: \u001b[0mInference done 919/1377. Dataloading: 0.0011 s/iter. Inference: 0.1014 s/iter. Eval: 0.0001 s/iter. Total: 0.1026 s/iter. ETA=0:00:47\n",
      "\u001b[32m[06/09 06:27:36 d2.evaluation.evaluator]: \u001b[0mInference done 965/1377. Dataloading: 0.0011 s/iter. Inference: 0.1018 s/iter. Eval: 0.0001 s/iter. Total: 0.1030 s/iter. ETA=0:00:42\n",
      "\u001b[32m[06/09 06:27:42 d2.evaluation.evaluator]: \u001b[0mInference done 1012/1377. Dataloading: 0.0011 s/iter. Inference: 0.1020 s/iter. Eval: 0.0001 s/iter. Total: 0.1032 s/iter. ETA=0:00:37\n",
      "\u001b[32m[06/09 06:27:47 d2.evaluation.evaluator]: \u001b[0mInference done 1058/1377. Dataloading: 0.0011 s/iter. Inference: 0.1023 s/iter. Eval: 0.0001 s/iter. Total: 0.1035 s/iter. ETA=0:00:33\n",
      "\u001b[32m[06/09 06:27:52 d2.evaluation.evaluator]: \u001b[0mInference done 1105/1377. Dataloading: 0.0011 s/iter. Inference: 0.1025 s/iter. Eval: 0.0001 s/iter. Total: 0.1037 s/iter. ETA=0:00:28\n",
      "\u001b[32m[06/09 06:27:57 d2.evaluation.evaluator]: \u001b[0mInference done 1154/1377. Dataloading: 0.0011 s/iter. Inference: 0.1024 s/iter. Eval: 0.0001 s/iter. Total: 0.1037 s/iter. ETA=0:00:23\n",
      "\u001b[32m[06/09 06:28:02 d2.evaluation.evaluator]: \u001b[0mInference done 1203/1377. Dataloading: 0.0011 s/iter. Inference: 0.1024 s/iter. Eval: 0.0001 s/iter. Total: 0.1037 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 06:28:07 d2.evaluation.evaluator]: \u001b[0mInference done 1252/1377. Dataloading: 0.0011 s/iter. Inference: 0.1024 s/iter. Eval: 0.0001 s/iter. Total: 0.1037 s/iter. ETA=0:00:12\n",
      "\u001b[32m[06/09 06:28:12 d2.evaluation.evaluator]: \u001b[0mInference done 1300/1377. Dataloading: 0.0012 s/iter. Inference: 0.1024 s/iter. Eval: 0.0001 s/iter. Total: 0.1037 s/iter. ETA=0:00:07\n",
      "\u001b[32m[06/09 06:28:17 d2.evaluation.evaluator]: \u001b[0mInference done 1349/1377. Dataloading: 0.0012 s/iter. Inference: 0.1024 s/iter. Eval: 0.0001 s/iter. Total: 0.1037 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 06:28:20 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:22.365413 (0.103765 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 06:28:20 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:02:20 (0.102417 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 06:28:20 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 06:28:20 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/dataset_rb/coco_instances_results.json\n",
      "\u001b[32m[06/09 06:28:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 06:28:20 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 06:28:20 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.02 seconds.\n",
      "\u001b[32m[06/09 06:28:20 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 06:28:20 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.304\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.427\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.406\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.127\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.335\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.680\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.390\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.505\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.505\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.313\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.584\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.714\n",
      "\u001b[32m[06/09 06:28:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 30.402 | 42.728 | 40.584 | 12.693 | 33.506 | 68.037 |\n",
      "OrderedDict([('bbox', {'AP': 30.40195674307304, 'AP50': 42.72811016526964, 'AP75': 40.58437940251869, 'APs': 12.692840743718959, 'APm': 33.50554827971782, 'APl': 68.03731911652703})])\n",
      "\u001b[32m[06/09 06:28:20 d2.data.datasets.coco]: \u001b[0mLoaded 7144 images in COCO format from ../data/Waste_Bin_Detection_Dataset/sunny_2021_03_23_14_33_cam5_filtered (training dataset images and ground truths)/sunny_reduced_static.json\n",
      "\u001b[32m[06/09 06:28:20 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[06/09 06:28:20 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common.NumpySerializedList'>\n",
      "\u001b[32m[06/09 06:28:20 d2.data.common]: \u001b[0mSerializing 7144 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[06/09 06:28:20 d2.data.common]: \u001b[0mSerialized dataset takes 1.93 MiB\n",
      "\u001b[32m[06/09 06:28:20 d2.evaluation.evaluator]: \u001b[0mStart inference on 7144 batches\n",
      "\u001b[32m[06/09 06:28:22 d2.evaluation.evaluator]: \u001b[0mInference done 17/7144. Dataloading: 0.0009 s/iter. Inference: 0.1012 s/iter. Eval: 0.0001 s/iter. Total: 0.1022 s/iter. ETA=0:12:08\n",
      "\u001b[32m[06/09 06:28:27 d2.evaluation.evaluator]: \u001b[0mInference done 67/7144. Dataloading: 0.0010 s/iter. Inference: 0.1003 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:11:57\n",
      "\u001b[32m[06/09 06:28:32 d2.evaluation.evaluator]: \u001b[0mInference done 116/7144. Dataloading: 0.0010 s/iter. Inference: 0.1009 s/iter. Eval: 0.0001 s/iter. Total: 0.1021 s/iter. ETA=0:11:57\n",
      "\u001b[32m[06/09 06:28:37 d2.evaluation.evaluator]: \u001b[0mInference done 167/7144. Dataloading: 0.0010 s/iter. Inference: 0.0999 s/iter. Eval: 0.0001 s/iter. Total: 0.1010 s/iter. ETA=0:11:44\n",
      "\u001b[32m[06/09 06:28:42 d2.evaluation.evaluator]: \u001b[0mInference done 217/7144. Dataloading: 0.0010 s/iter. Inference: 0.0997 s/iter. Eval: 0.0001 s/iter. Total: 0.1009 s/iter. ETA=0:11:38\n",
      "\u001b[32m[06/09 06:28:47 d2.evaluation.evaluator]: \u001b[0mInference done 267/7144. Dataloading: 0.0010 s/iter. Inference: 0.0999 s/iter. Eval: 0.0001 s/iter. Total: 0.1011 s/iter. ETA=0:11:35\n",
      "\u001b[32m[06/09 06:28:52 d2.evaluation.evaluator]: \u001b[0mInference done 317/7144. Dataloading: 0.0010 s/iter. Inference: 0.0998 s/iter. Eval: 0.0001 s/iter. Total: 0.1010 s/iter. ETA=0:11:29\n",
      "\u001b[32m[06/09 06:28:57 d2.evaluation.evaluator]: \u001b[0mInference done 366/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:11:26\n",
      "\u001b[32m[06/09 06:29:02 d2.evaluation.evaluator]: \u001b[0mInference done 415/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:11:21\n",
      "\u001b[32m[06/09 06:29:07 d2.evaluation.evaluator]: \u001b[0mInference done 465/7144. Dataloading: 0.0011 s/iter. Inference: 0.1001 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:11:16\n",
      "\u001b[32m[06/09 06:29:12 d2.evaluation.evaluator]: \u001b[0mInference done 515/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:11:10\n",
      "\u001b[32m[06/09 06:29:17 d2.evaluation.evaluator]: \u001b[0mInference done 564/7144. Dataloading: 0.0011 s/iter. Inference: 0.1001 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:11:06\n",
      "\u001b[32m[06/09 06:29:22 d2.evaluation.evaluator]: \u001b[0mInference done 614/7144. Dataloading: 0.0011 s/iter. Inference: 0.1001 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:11:01\n",
      "\u001b[32m[06/09 06:29:28 d2.evaluation.evaluator]: \u001b[0mInference done 663/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:10:57\n",
      "\u001b[32m[06/09 06:29:33 d2.evaluation.evaluator]: \u001b[0mInference done 713/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:10:52\n",
      "\u001b[32m[06/09 06:29:38 d2.evaluation.evaluator]: \u001b[0mInference done 763/7144. Dataloading: 0.0011 s/iter. Inference: 0.1003 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:10:47\n",
      "\u001b[32m[06/09 06:29:43 d2.evaluation.evaluator]: \u001b[0mInference done 813/7144. Dataloading: 0.0011 s/iter. Inference: 0.1003 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:10:42\n",
      "\u001b[32m[06/09 06:29:48 d2.evaluation.evaluator]: \u001b[0mInference done 864/7144. Dataloading: 0.0011 s/iter. Inference: 0.1001 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:10:36\n",
      "\u001b[32m[06/09 06:29:53 d2.evaluation.evaluator]: \u001b[0mInference done 914/7144. Dataloading: 0.0011 s/iter. Inference: 0.1001 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:10:31\n",
      "\u001b[32m[06/09 06:29:58 d2.evaluation.evaluator]: \u001b[0mInference done 965/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:10:25\n",
      "\u001b[32m[06/09 06:30:03 d2.evaluation.evaluator]: \u001b[0mInference done 1015/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:10:20\n",
      "\u001b[32m[06/09 06:30:08 d2.evaluation.evaluator]: \u001b[0mInference done 1065/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:10:15\n",
      "\u001b[32m[06/09 06:30:13 d2.evaluation.evaluator]: \u001b[0mInference done 1115/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:10:10\n",
      "\u001b[32m[06/09 06:30:18 d2.evaluation.evaluator]: \u001b[0mInference done 1166/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:10:04\n",
      "\u001b[32m[06/09 06:30:23 d2.evaluation.evaluator]: \u001b[0mInference done 1216/7144. Dataloading: 0.0011 s/iter. Inference: 0.0999 s/iter. Eval: 0.0001 s/iter. Total: 0.1011 s/iter. ETA=0:09:59\n",
      "\u001b[32m[06/09 06:30:28 d2.evaluation.evaluator]: \u001b[0mInference done 1265/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:09:54\n",
      "\u001b[32m[06/09 06:30:33 d2.evaluation.evaluator]: \u001b[0mInference done 1315/7144. Dataloading: 0.0011 s/iter. Inference: 0.0999 s/iter. Eval: 0.0001 s/iter. Total: 0.1011 s/iter. ETA=0:09:49\n",
      "\u001b[32m[06/09 06:30:38 d2.evaluation.evaluator]: \u001b[0mInference done 1365/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:09:44\n",
      "\u001b[32m[06/09 06:30:43 d2.evaluation.evaluator]: \u001b[0mInference done 1415/7144. Dataloading: 0.0011 s/iter. Inference: 0.0999 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:09:39\n",
      "\u001b[32m[06/09 06:30:48 d2.evaluation.evaluator]: \u001b[0mInference done 1464/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:09:34\n",
      "\u001b[32m[06/09 06:30:53 d2.evaluation.evaluator]: \u001b[0mInference done 1514/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:09:29\n",
      "\u001b[32m[06/09 06:30:59 d2.evaluation.evaluator]: \u001b[0mInference done 1564/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:09:24\n",
      "\u001b[32m[06/09 06:31:04 d2.evaluation.evaluator]: \u001b[0mInference done 1614/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:09:19\n",
      "\u001b[32m[06/09 06:31:09 d2.evaluation.evaluator]: \u001b[0mInference done 1665/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:09:14\n",
      "\u001b[32m[06/09 06:31:14 d2.evaluation.evaluator]: \u001b[0mInference done 1714/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:09:09\n",
      "\u001b[32m[06/09 06:31:19 d2.evaluation.evaluator]: \u001b[0mInference done 1764/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:09:04\n",
      "\u001b[32m[06/09 06:31:24 d2.evaluation.evaluator]: \u001b[0mInference done 1814/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:08:59\n",
      "\u001b[32m[06/09 06:31:29 d2.evaluation.evaluator]: \u001b[0mInference done 1865/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:08:54\n",
      "\u001b[32m[06/09 06:31:34 d2.evaluation.evaluator]: \u001b[0mInference done 1915/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:08:49\n",
      "\u001b[32m[06/09 06:31:39 d2.evaluation.evaluator]: \u001b[0mInference done 1965/7144. Dataloading: 0.0011 s/iter. Inference: 0.0999 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:08:43\n",
      "\u001b[32m[06/09 06:31:44 d2.evaluation.evaluator]: \u001b[0mInference done 2016/7144. Dataloading: 0.0011 s/iter. Inference: 0.0999 s/iter. Eval: 0.0001 s/iter. Total: 0.1011 s/iter. ETA=0:08:38\n",
      "\u001b[32m[06/09 06:31:49 d2.evaluation.evaluator]: \u001b[0mInference done 2066/7144. Dataloading: 0.0011 s/iter. Inference: 0.0999 s/iter. Eval: 0.0001 s/iter. Total: 0.1011 s/iter. ETA=0:08:33\n",
      "\u001b[32m[06/09 06:31:54 d2.evaluation.evaluator]: \u001b[0mInference done 2115/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:08:28\n",
      "\u001b[32m[06/09 06:31:59 d2.evaluation.evaluator]: \u001b[0mInference done 2165/7144. Dataloading: 0.0011 s/iter. Inference: 0.0999 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:08:23\n",
      "\u001b[32m[06/09 06:32:04 d2.evaluation.evaluator]: \u001b[0mInference done 2215/7144. Dataloading: 0.0011 s/iter. Inference: 0.0999 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:08:18\n",
      "\u001b[32m[06/09 06:32:09 d2.evaluation.evaluator]: \u001b[0mInference done 2264/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:08:13\n",
      "\u001b[32m[06/09 06:32:14 d2.evaluation.evaluator]: \u001b[0mInference done 2313/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:08:09\n",
      "\u001b[32m[06/09 06:32:19 d2.evaluation.evaluator]: \u001b[0mInference done 2362/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:08:04\n",
      "\u001b[32m[06/09 06:32:25 d2.evaluation.evaluator]: \u001b[0mInference done 2412/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:07:59\n",
      "\u001b[32m[06/09 06:32:30 d2.evaluation.evaluator]: \u001b[0mInference done 2462/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:07:53\n",
      "\u001b[32m[06/09 06:32:35 d2.evaluation.evaluator]: \u001b[0mInference done 2512/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:07:48\n",
      "\u001b[32m[06/09 06:32:40 d2.evaluation.evaluator]: \u001b[0mInference done 2563/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:07:43\n",
      "\u001b[32m[06/09 06:32:45 d2.evaluation.evaluator]: \u001b[0mInference done 2613/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1012 s/iter. ETA=0:07:38\n",
      "\u001b[32m[06/09 06:32:50 d2.evaluation.evaluator]: \u001b[0mInference done 2662/7144. Dataloading: 0.0011 s/iter. Inference: 0.1000 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:07:33\n",
      "\u001b[32m[06/09 06:32:55 d2.evaluation.evaluator]: \u001b[0mInference done 2710/7144. Dataloading: 0.0011 s/iter. Inference: 0.1001 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:07:29\n",
      "\u001b[32m[06/09 06:33:00 d2.evaluation.evaluator]: \u001b[0mInference done 2760/7144. Dataloading: 0.0011 s/iter. Inference: 0.1001 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:07:24\n",
      "\u001b[32m[06/09 06:33:05 d2.evaluation.evaluator]: \u001b[0mInference done 2810/7144. Dataloading: 0.0011 s/iter. Inference: 0.1001 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:07:19\n",
      "\u001b[32m[06/09 06:33:10 d2.evaluation.evaluator]: \u001b[0mInference done 2860/7144. Dataloading: 0.0011 s/iter. Inference: 0.1001 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:07:14\n",
      "\u001b[32m[06/09 06:33:15 d2.evaluation.evaluator]: \u001b[0mInference done 2911/7144. Dataloading: 0.0011 s/iter. Inference: 0.1001 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:07:08\n",
      "\u001b[32m[06/09 06:33:20 d2.evaluation.evaluator]: \u001b[0mInference done 2961/7144. Dataloading: 0.0011 s/iter. Inference: 0.1001 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:07:03\n",
      "\u001b[32m[06/09 06:33:25 d2.evaluation.evaluator]: \u001b[0mInference done 3011/7144. Dataloading: 0.0011 s/iter. Inference: 0.1001 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:06:58\n",
      "\u001b[32m[06/09 06:33:30 d2.evaluation.evaluator]: \u001b[0mInference done 3060/7144. Dataloading: 0.0011 s/iter. Inference: 0.1001 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:06:53\n",
      "\u001b[32m[06/09 06:33:35 d2.evaluation.evaluator]: \u001b[0mInference done 3110/7144. Dataloading: 0.0011 s/iter. Inference: 0.1001 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:06:48\n",
      "\u001b[32m[06/09 06:33:40 d2.evaluation.evaluator]: \u001b[0mInference done 3159/7144. Dataloading: 0.0011 s/iter. Inference: 0.1001 s/iter. Eval: 0.0001 s/iter. Total: 0.1013 s/iter. ETA=0:06:43\n",
      "\u001b[32m[06/09 06:33:45 d2.evaluation.evaluator]: \u001b[0mInference done 3208/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:06:39\n",
      "\u001b[32m[06/09 06:33:51 d2.evaluation.evaluator]: \u001b[0mInference done 3257/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:06:34\n",
      "\u001b[32m[06/09 06:33:56 d2.evaluation.evaluator]: \u001b[0mInference done 3306/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:06:29\n",
      "\u001b[32m[06/09 06:34:01 d2.evaluation.evaluator]: \u001b[0mInference done 3356/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:06:24\n",
      "\u001b[32m[06/09 06:34:06 d2.evaluation.evaluator]: \u001b[0mInference done 3406/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:06:19\n",
      "\u001b[32m[06/09 06:34:11 d2.evaluation.evaluator]: \u001b[0mInference done 3457/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:06:13\n",
      "\u001b[32m[06/09 06:34:16 d2.evaluation.evaluator]: \u001b[0mInference done 3507/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:06:08\n",
      "\u001b[32m[06/09 06:34:21 d2.evaluation.evaluator]: \u001b[0mInference done 3557/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:06:03\n",
      "\u001b[32m[06/09 06:34:26 d2.evaluation.evaluator]: \u001b[0mInference done 3606/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:05:58\n",
      "\u001b[32m[06/09 06:34:31 d2.evaluation.evaluator]: \u001b[0mInference done 3655/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:05:53\n",
      "\u001b[32m[06/09 06:34:36 d2.evaluation.evaluator]: \u001b[0mInference done 3705/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:05:48\n",
      "\u001b[32m[06/09 06:34:41 d2.evaluation.evaluator]: \u001b[0mInference done 3755/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:05:43\n",
      "\u001b[32m[06/09 06:34:46 d2.evaluation.evaluator]: \u001b[0mInference done 3805/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:05:38\n",
      "\u001b[32m[06/09 06:34:51 d2.evaluation.evaluator]: \u001b[0mInference done 3855/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:05:33\n",
      "\u001b[32m[06/09 06:34:56 d2.evaluation.evaluator]: \u001b[0mInference done 3904/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:05:28\n",
      "\u001b[32m[06/09 06:35:01 d2.evaluation.evaluator]: \u001b[0mInference done 3953/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1015 s/iter. ETA=0:05:23\n",
      "\u001b[32m[06/09 06:35:06 d2.evaluation.evaluator]: \u001b[0mInference done 4002/7144. Dataloading: 0.0011 s/iter. Inference: 0.1003 s/iter. Eval: 0.0001 s/iter. Total: 0.1015 s/iter. ETA=0:05:18\n",
      "\u001b[32m[06/09 06:35:11 d2.evaluation.evaluator]: \u001b[0mInference done 4052/7144. Dataloading: 0.0011 s/iter. Inference: 0.1003 s/iter. Eval: 0.0001 s/iter. Total: 0.1015 s/iter. ETA=0:05:13\n",
      "\u001b[32m[06/09 06:35:17 d2.evaluation.evaluator]: \u001b[0mInference done 4103/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:05:08\n",
      "\u001b[32m[06/09 06:35:22 d2.evaluation.evaluator]: \u001b[0mInference done 4153/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:05:03\n",
      "\u001b[32m[06/09 06:35:27 d2.evaluation.evaluator]: \u001b[0mInference done 4203/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:04:58\n",
      "\u001b[32m[06/09 06:35:32 d2.evaluation.evaluator]: \u001b[0mInference done 4253/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:04:53\n",
      "\u001b[32m[06/09 06:35:37 d2.evaluation.evaluator]: \u001b[0mInference done 4302/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:04:48\n",
      "\u001b[32m[06/09 06:35:42 d2.evaluation.evaluator]: \u001b[0mInference done 4353/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:04:43\n",
      "\u001b[32m[06/09 06:35:47 d2.evaluation.evaluator]: \u001b[0mInference done 4403/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:04:37\n",
      "\u001b[32m[06/09 06:35:52 d2.evaluation.evaluator]: \u001b[0mInference done 4452/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:04:32\n",
      "\u001b[32m[06/09 06:35:57 d2.evaluation.evaluator]: \u001b[0mInference done 4500/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:04:28\n",
      "\u001b[32m[06/09 06:36:02 d2.evaluation.evaluator]: \u001b[0mInference done 4550/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:04:23\n",
      "\u001b[32m[06/09 06:36:07 d2.evaluation.evaluator]: \u001b[0mInference done 4600/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:04:18\n",
      "\u001b[32m[06/09 06:36:12 d2.evaluation.evaluator]: \u001b[0mInference done 4649/7144. Dataloading: 0.0011 s/iter. Inference: 0.1003 s/iter. Eval: 0.0001 s/iter. Total: 0.1015 s/iter. ETA=0:04:13\n",
      "\u001b[32m[06/09 06:36:17 d2.evaluation.evaluator]: \u001b[0mInference done 4700/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:04:07\n",
      "\u001b[32m[06/09 06:36:22 d2.evaluation.evaluator]: \u001b[0mInference done 4750/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:04:02\n",
      "\u001b[32m[06/09 06:36:27 d2.evaluation.evaluator]: \u001b[0mInference done 4800/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:03:57\n",
      "\u001b[32m[06/09 06:36:32 d2.evaluation.evaluator]: \u001b[0mInference done 4850/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:03:52\n",
      "\u001b[32m[06/09 06:36:37 d2.evaluation.evaluator]: \u001b[0mInference done 4899/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:03:47\n",
      "\u001b[32m[06/09 06:36:42 d2.evaluation.evaluator]: \u001b[0mInference done 4948/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:03:42\n",
      "\u001b[32m[06/09 06:36:47 d2.evaluation.evaluator]: \u001b[0mInference done 4998/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:03:37\n",
      "\u001b[32m[06/09 06:36:52 d2.evaluation.evaluator]: \u001b[0mInference done 5049/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:03:32\n",
      "\u001b[32m[06/09 06:36:57 d2.evaluation.evaluator]: \u001b[0mInference done 5099/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:03:27\n",
      "\u001b[32m[06/09 06:37:02 d2.evaluation.evaluator]: \u001b[0mInference done 5149/7144. Dataloading: 0.0011 s/iter. Inference: 0.1002 s/iter. Eval: 0.0001 s/iter. Total: 0.1014 s/iter. ETA=0:03:22\n",
      "\u001b[32m[06/09 06:37:07 d2.evaluation.evaluator]: \u001b[0mInference done 5195/7144. Dataloading: 0.0011 s/iter. Inference: 0.1003 s/iter. Eval: 0.0001 s/iter. Total: 0.1015 s/iter. ETA=0:03:17\n",
      "\u001b[32m[06/09 06:37:13 d2.evaluation.evaluator]: \u001b[0mInference done 5243/7144. Dataloading: 0.0011 s/iter. Inference: 0.1003 s/iter. Eval: 0.0001 s/iter. Total: 0.1015 s/iter. ETA=0:03:12\n",
      "\u001b[32m[06/09 06:37:18 d2.evaluation.evaluator]: \u001b[0mInference done 5290/7144. Dataloading: 0.0011 s/iter. Inference: 0.1004 s/iter. Eval: 0.0001 s/iter. Total: 0.1016 s/iter. ETA=0:03:08\n",
      "\u001b[32m[06/09 06:37:23 d2.evaluation.evaluator]: \u001b[0mInference done 5338/7144. Dataloading: 0.0011 s/iter. Inference: 0.1004 s/iter. Eval: 0.0001 s/iter. Total: 0.1016 s/iter. ETA=0:03:03\n",
      "\u001b[32m[06/09 06:37:28 d2.evaluation.evaluator]: \u001b[0mInference done 5385/7144. Dataloading: 0.0011 s/iter. Inference: 0.1004 s/iter. Eval: 0.0001 s/iter. Total: 0.1016 s/iter. ETA=0:02:58\n",
      "\u001b[32m[06/09 06:37:33 d2.evaluation.evaluator]: \u001b[0mInference done 5432/7144. Dataloading: 0.0011 s/iter. Inference: 0.1005 s/iter. Eval: 0.0001 s/iter. Total: 0.1017 s/iter. ETA=0:02:54\n",
      "\u001b[32m[06/09 06:37:38 d2.evaluation.evaluator]: \u001b[0mInference done 5480/7144. Dataloading: 0.0011 s/iter. Inference: 0.1005 s/iter. Eval: 0.0001 s/iter. Total: 0.1017 s/iter. ETA=0:02:49\n",
      "\u001b[32m[06/09 06:37:43 d2.evaluation.evaluator]: \u001b[0mInference done 5528/7144. Dataloading: 0.0011 s/iter. Inference: 0.1005 s/iter. Eval: 0.0001 s/iter. Total: 0.1018 s/iter. ETA=0:02:44\n",
      "\u001b[32m[06/09 06:37:48 d2.evaluation.evaluator]: \u001b[0mInference done 5576/7144. Dataloading: 0.0011 s/iter. Inference: 0.1006 s/iter. Eval: 0.0001 s/iter. Total: 0.1018 s/iter. ETA=0:02:39\n",
      "\u001b[32m[06/09 06:37:53 d2.evaluation.evaluator]: \u001b[0mInference done 5623/7144. Dataloading: 0.0011 s/iter. Inference: 0.1006 s/iter. Eval: 0.0001 s/iter. Total: 0.1018 s/iter. ETA=0:02:34\n",
      "\u001b[32m[06/09 06:37:58 d2.evaluation.evaluator]: \u001b[0mInference done 5670/7144. Dataloading: 0.0011 s/iter. Inference: 0.1007 s/iter. Eval: 0.0001 s/iter. Total: 0.1019 s/iter. ETA=0:02:30\n",
      "\u001b[32m[06/09 06:38:03 d2.evaluation.evaluator]: \u001b[0mInference done 5718/7144. Dataloading: 0.0011 s/iter. Inference: 0.1007 s/iter. Eval: 0.0001 s/iter. Total: 0.1019 s/iter. ETA=0:02:25\n",
      "\u001b[32m[06/09 06:38:08 d2.evaluation.evaluator]: \u001b[0mInference done 5765/7144. Dataloading: 0.0011 s/iter. Inference: 0.1007 s/iter. Eval: 0.0001 s/iter. Total: 0.1019 s/iter. ETA=0:02:20\n",
      "\u001b[32m[06/09 06:38:13 d2.evaluation.evaluator]: \u001b[0mInference done 5813/7144. Dataloading: 0.0011 s/iter. Inference: 0.1007 s/iter. Eval: 0.0001 s/iter. Total: 0.1020 s/iter. ETA=0:02:15\n",
      "\u001b[32m[06/09 06:38:18 d2.evaluation.evaluator]: \u001b[0mInference done 5860/7144. Dataloading: 0.0011 s/iter. Inference: 0.1008 s/iter. Eval: 0.0001 s/iter. Total: 0.1020 s/iter. ETA=0:02:10\n",
      "\u001b[32m[06/09 06:38:23 d2.evaluation.evaluator]: \u001b[0mInference done 5907/7144. Dataloading: 0.0011 s/iter. Inference: 0.1008 s/iter. Eval: 0.0001 s/iter. Total: 0.1020 s/iter. ETA=0:02:06\n",
      "\u001b[32m[06/09 06:38:28 d2.evaluation.evaluator]: \u001b[0mInference done 5953/7144. Dataloading: 0.0011 s/iter. Inference: 0.1009 s/iter. Eval: 0.0001 s/iter. Total: 0.1021 s/iter. ETA=0:02:01\n",
      "\u001b[32m[06/09 06:38:33 d2.evaluation.evaluator]: \u001b[0mInference done 6000/7144. Dataloading: 0.0011 s/iter. Inference: 0.1009 s/iter. Eval: 0.0001 s/iter. Total: 0.1021 s/iter. ETA=0:01:56\n",
      "\u001b[32m[06/09 06:38:38 d2.evaluation.evaluator]: \u001b[0mInference done 6048/7144. Dataloading: 0.0011 s/iter. Inference: 0.1009 s/iter. Eval: 0.0001 s/iter. Total: 0.1022 s/iter. ETA=0:01:51\n",
      "\u001b[32m[06/09 06:38:43 d2.evaluation.evaluator]: \u001b[0mInference done 6096/7144. Dataloading: 0.0011 s/iter. Inference: 0.1010 s/iter. Eval: 0.0001 s/iter. Total: 0.1022 s/iter. ETA=0:01:47\n",
      "\u001b[32m[06/09 06:38:48 d2.evaluation.evaluator]: \u001b[0mInference done 6143/7144. Dataloading: 0.0011 s/iter. Inference: 0.1010 s/iter. Eval: 0.0001 s/iter. Total: 0.1022 s/iter. ETA=0:01:42\n",
      "\u001b[32m[06/09 06:38:53 d2.evaluation.evaluator]: \u001b[0mInference done 6189/7144. Dataloading: 0.0011 s/iter. Inference: 0.1011 s/iter. Eval: 0.0001 s/iter. Total: 0.1023 s/iter. ETA=0:01:37\n",
      "\u001b[32m[06/09 06:38:58 d2.evaluation.evaluator]: \u001b[0mInference done 6236/7144. Dataloading: 0.0011 s/iter. Inference: 0.1011 s/iter. Eval: 0.0001 s/iter. Total: 0.1023 s/iter. ETA=0:01:32\n",
      "\u001b[32m[06/09 06:39:03 d2.evaluation.evaluator]: \u001b[0mInference done 6283/7144. Dataloading: 0.0011 s/iter. Inference: 0.1011 s/iter. Eval: 0.0001 s/iter. Total: 0.1024 s/iter. ETA=0:01:28\n",
      "\u001b[32m[06/09 06:39:08 d2.evaluation.evaluator]: \u001b[0mInference done 6330/7144. Dataloading: 0.0011 s/iter. Inference: 0.1012 s/iter. Eval: 0.0001 s/iter. Total: 0.1024 s/iter. ETA=0:01:23\n",
      "\u001b[32m[06/09 06:39:13 d2.evaluation.evaluator]: \u001b[0mInference done 6377/7144. Dataloading: 0.0011 s/iter. Inference: 0.1012 s/iter. Eval: 0.0001 s/iter. Total: 0.1024 s/iter. ETA=0:01:18\n",
      "\u001b[32m[06/09 06:39:19 d2.evaluation.evaluator]: \u001b[0mInference done 6425/7144. Dataloading: 0.0011 s/iter. Inference: 0.1012 s/iter. Eval: 0.0001 s/iter. Total: 0.1025 s/iter. ETA=0:01:13\n",
      "\u001b[32m[06/09 06:39:24 d2.evaluation.evaluator]: \u001b[0mInference done 6472/7144. Dataloading: 0.0011 s/iter. Inference: 0.1013 s/iter. Eval: 0.0001 s/iter. Total: 0.1025 s/iter. ETA=0:01:08\n",
      "\u001b[32m[06/09 06:39:29 d2.evaluation.evaluator]: \u001b[0mInference done 6520/7144. Dataloading: 0.0011 s/iter. Inference: 0.1013 s/iter. Eval: 0.0001 s/iter. Total: 0.1025 s/iter. ETA=0:01:03\n",
      "\u001b[32m[06/09 06:39:34 d2.evaluation.evaluator]: \u001b[0mInference done 6568/7144. Dataloading: 0.0011 s/iter. Inference: 0.1013 s/iter. Eval: 0.0001 s/iter. Total: 0.1025 s/iter. ETA=0:00:59\n",
      "\u001b[32m[06/09 06:39:39 d2.evaluation.evaluator]: \u001b[0mInference done 6615/7144. Dataloading: 0.0011 s/iter. Inference: 0.1014 s/iter. Eval: 0.0001 s/iter. Total: 0.1026 s/iter. ETA=0:00:54\n",
      "\u001b[32m[06/09 06:39:44 d2.evaluation.evaluator]: \u001b[0mInference done 6663/7144. Dataloading: 0.0011 s/iter. Inference: 0.1014 s/iter. Eval: 0.0001 s/iter. Total: 0.1026 s/iter. ETA=0:00:49\n",
      "\u001b[32m[06/09 06:39:49 d2.evaluation.evaluator]: \u001b[0mInference done 6710/7144. Dataloading: 0.0011 s/iter. Inference: 0.1014 s/iter. Eval: 0.0001 s/iter. Total: 0.1026 s/iter. ETA=0:00:44\n",
      "\u001b[32m[06/09 06:39:54 d2.evaluation.evaluator]: \u001b[0mInference done 6759/7144. Dataloading: 0.0011 s/iter. Inference: 0.1014 s/iter. Eval: 0.0001 s/iter. Total: 0.1026 s/iter. ETA=0:00:39\n",
      "\u001b[32m[06/09 06:39:59 d2.evaluation.evaluator]: \u001b[0mInference done 6809/7144. Dataloading: 0.0011 s/iter. Inference: 0.1014 s/iter. Eval: 0.0001 s/iter. Total: 0.1026 s/iter. ETA=0:00:34\n",
      "\u001b[32m[06/09 06:40:04 d2.evaluation.evaluator]: \u001b[0mInference done 6861/7144. Dataloading: 0.0011 s/iter. Inference: 0.1014 s/iter. Eval: 0.0001 s/iter. Total: 0.1026 s/iter. ETA=0:00:29\n",
      "\u001b[32m[06/09 06:40:09 d2.evaluation.evaluator]: \u001b[0mInference done 6911/7144. Dataloading: 0.0011 s/iter. Inference: 0.1014 s/iter. Eval: 0.0001 s/iter. Total: 0.1026 s/iter. ETA=0:00:23\n",
      "\u001b[32m[06/09 06:40:14 d2.evaluation.evaluator]: \u001b[0mInference done 6962/7144. Dataloading: 0.0011 s/iter. Inference: 0.1013 s/iter. Eval: 0.0001 s/iter. Total: 0.1025 s/iter. ETA=0:00:18\n",
      "\u001b[32m[06/09 06:40:19 d2.evaluation.evaluator]: \u001b[0mInference done 7013/7144. Dataloading: 0.0011 s/iter. Inference: 0.1013 s/iter. Eval: 0.0001 s/iter. Total: 0.1025 s/iter. ETA=0:00:13\n",
      "\u001b[32m[06/09 06:40:24 d2.evaluation.evaluator]: \u001b[0mInference done 7063/7144. Dataloading: 0.0011 s/iter. Inference: 0.1013 s/iter. Eval: 0.0001 s/iter. Total: 0.1025 s/iter. ETA=0:00:08\n",
      "\u001b[32m[06/09 06:40:29 d2.evaluation.evaluator]: \u001b[0mInference done 7115/7144. Dataloading: 0.0011 s/iter. Inference: 0.1012 s/iter. Eval: 0.0001 s/iter. Total: 0.1025 s/iter. ETA=0:00:02\n",
      "\u001b[32m[06/09 06:40:32 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:12:11.428421 (0.102455 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 06:40:32 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:12:02 (0.101240 s / iter per device, on 1 devices)\n",
      "\u001b[32m[06/09 06:40:32 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[06/09 06:40:32 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/dataset_train_reduced/coco_instances_results.json\n",
      "\u001b[32m[06/09 06:40:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[06/09 06:40:32 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[06/09 06:40:32 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.15 seconds.\n",
      "\u001b[32m[06/09 06:40:32 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[06/09 06:40:32 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.590\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.737\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.680\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.384\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.628\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.918\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.570\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.807\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.807\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.687\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.857\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.946\n",
      "\u001b[32m[06/09 06:40:32 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 59.024 | 73.689 | 68.021 | 38.365 | 62.845 | 91.762 |\n",
      "OrderedDict([('bbox', {'AP': 59.02370103593897, 'AP50': 73.68915875359374, 'AP75': 68.02132451332707, 'APs': 38.365219130290015, 'APm': 62.84521954226195, 'APl': 91.76188903356719})])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "# dataset_to_evaluate = [\"dataset_train_original\", \"dataset_train_generated\", \"dataset_train_generated_cleaned\", \"dataset_val\", \"dataset_snowy_day\", \"dataset_snowy_night\", \"dataset_argo\"] # \"dataset_val\", \n",
    "# dataset_to_evaluate = [\"dataset_val\", \"dataset_train_generated_cleaned\"] # \"dataset_val\", \n",
    "dataset_to_evaluate = [\"dataset_test\", \"dataset_snowy_day\", \"dataset_snowy_night\", \"dataset_rb\", \"dataset_train_reduced\"] # , \"dataset_val\"] # [\"dataset_val\", \"dataset_snowy_day\"] # , \"dataset_snowy_night3\", \"dataset_argo3\"]\n",
    "# dataset_to_evaluate = [\"dataset_train_snowy_prev2\", \"dataset_train_snowy2_prev2\", \"dataset_train_snowy_clean_prev2\", \"dataset_train_snowy2_clean_prev2\"]\n",
    "# dataset_to_evaluate = [\"dataset_train_reduced\"]\n",
    "# dataset_to_evaluate = [\"sunny_test\", \"cloudy_val_test\", \"snowy_day_test\", \"snowy_night_test\", \"rb_test\"]\n",
    "# inferences = {}\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.000001\n",
    "for dataset in dataset_to_evaluate:\n",
    "    evaluator = COCOEvaluator(dataset, output_dir=\"./output/\" + dataset)\n",
    "    val_loader = build_detection_test_loader(cfg, dataset)\n",
    "    inferences[dataset] = inference_on_dataset(predictor.model, val_loader, evaluator) # predictor.model\n",
    "    print(inferences[dataset])\n",
    "    # save all inferences in a file\n",
    "    with open(os.path.join(cfg.OUTPUT_DIR, \"inferences.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(inferences, f)\n",
    "\n",
    "all_inferences[cfg.DATASETS.TRAIN] = inferences\n",
    "\n",
    "    # print(inference_on_dataset(predictor.model, val_loader, evaluator))\n",
    "# another equivalent way to evaluate the model is to use `trainer.test`\n",
    "\n",
    "# Trained on original dataset + generated (cleaned) dataset\n",
    "# |   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
    "# |:------:|:------:|:------:|:------:|:------:|:------:|\n",
    "# | 22.630 | 34.140 | 27.150 | 9.977  | 36.584 | 54.122 |  dataset_train_original (~7000 images, ~1000 labels)\n",
    "# | 44.636 | 73.037 | 47.180 | 28.148 | 49.000 | 49.672 |  dataset_train_generated (~60 images, 60 labels)\n",
    "# | 48.803 | 78.352 | 53.138 | 30.783 | 52.991 | 55.455 |  dataset_train_generated_cleaned \n",
    "# | 23.292 | 35.682 | 26.742 | 11.776 | 33.034 | 47.688 |  dataset_val\n",
    "# Trained on original dataset\n",
    "# |   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
    "# | 22.014 | 34.676 | 26.207 | 10.798 | 36.681 | 54.749 | dataset_train_original\n",
    "# | 43.123 | 73.737 | 44.663 | 38.779 | 44.977 | 48.450 | dataset_train_generated\n",
    "# | 47.539 | 79.497 | 51.194 | 44.103 | 49.014 | 53.176 | dataset_train_generated_cleaned\n",
    "# | 21.913 | 34.243 | 27.446 | 11.634 | 32.517 | 48.396 | dataset_val\n",
    "\n",
    "# Learnings: Quite some variation between runs\n",
    "# New snowy training not improving results\n",
    "# Plan: clean up snowy data 2\n",
    "# How exactly try out difference between original and generated data?\n",
    "# Take only one img of each original sequence used for generation?\n",
    "# apply my method on the original data, analyze results\n",
    "\n",
    "\n",
    "# Trained on reduced original dataset\n",
    "# |   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
    "# | 45.756 | 67.222 | 51.345 | 25.949 | 55.818 | 78.282 | dataset_val_original_reduced\n",
    "# | 21.538 | 31.880 | 25.038 | 9.948 | 26.263 | 50.461 | dataset_snowy_day \n",
    "# Trained on snowy train set\n",
    "# | 18.280 | 29.895 | 20.292 | 9.300 | 27.073 | 38.439 |\n",
    "# | 9.994  | 16.164 | 12.322 | 6.468 | 16.309 | 18.593 |\n",
    "# Combined unique train and snowy train set\n",
    "# | 38.833 | 58.102 | 45.095 | 19.591 | 47.430 | 75.208 |\n",
    "# | 21.495 | 32.802 | 25.634 | 9.489 | 29.433 | 49.897 |\n",
    "# Combined unique train and snowy train set (cleaned)\n",
    "# | 37.469 | 53.721 | 42.719 | 18.028 | 45.765 | 78.251 |\n",
    "# | 24.375 | 36.477 | 30.229 | 12.737 | 30.198 | 53.065 |\n",
    "# Trained on automatically reduced (to max 3) snowy train set\n",
    "# | 38.031 | 55.635 | 43.982 | 19.117 | 46.702 | 77.870 |\n",
    "# | 22.047 | 33.283 | 25.873 | 11.772 | 30.238 | 51.071 |\n",
    "# Trained on automatically reduced (to max 1) snowy train set\n",
    "# | 36.117 | 51.607 | 40.976 | 18.035 | 43.181 | 73.940 |\n",
    "# | 22.901 | 34.549 | 26.880 | 13.523 | 25.643 | 51.252 |\n",
    "# Trained on automatically reduced (to max 1) snowy train set\n",
    "# | 37.685 | 52.681 | 43.755 | 19.267 | 47.180 | 68.820 |\n",
    "# | 24.363 | 35.289 | 30.541 | 14.746 | 29.810 | 47.583 |\n",
    "# Trained on automatically reduced (to max 1) snowy2 train set (cleaned)\n",
    "# | 36.995 | 51.467 | 43.097 | 19.577 | 44.768 | 70.279 |\n",
    "# | 22.009 | 31.875 | 27.657 | 12.179 | 27.253 | 46.786 |\n",
    "# Trained on automatically reduced (to max 1) origin + snowy 1+2 train set\n",
    "# | 37.177 | 53.430 | 42.364 | 20.412 | 43.247 | 75.661 |\n",
    "# | 22.720 | 34.215 | 27.544 | 13.802 | 26.913 | 46.457 |\n",
    "# | 35.735 | 51.473 | 41.615 | 15.925 | 44.434 | 75.166 | dataset_test_original_reduced\n",
    "\n",
    "\n",
    "# Trained on reduced original dataset\n",
    "# |   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
    "# | 42.227 | 58.580 | 51.519 | 20.045 | 49.418 | 77.425 | dataset_test_original_reduced\n",
    "# | 23.288 | 35.369 | 27.442 | 12.593 | 30.684 | 50.503 | dataset_snowy_day\n",
    "# | 30.954 | 42.883 | 39.340 | 6.343  | 33.531 | 49.271 | dataset_snowy_night\n",
    "# | 26.906 | 37.472 | 34.993 | 10.497 | 28.353 | 67.791 | dataset_rb\n",
    "# Trained on reduced original ds + snowy train set\n",
    "# | 40.526 | 55.548 | 47.742 | 18.466 | 49.051 | 72.051 | dataset_test_original_reduced\n",
    "# | 22.646 | 32.486 | 29.322 | 14.175 | 28.426 | 45.474 | dataset_snowy_day\n",
    "# | 32.458 | 44.207 | 39.656 | 9.361  | 37.541 | 42.727 | dataset_snowy_night\n",
    "# | 30.169 | 43.084 | 40.356 | 11.836 | 30.764 | 70.756 | dataset_rb\n",
    "# Trained on snowy train set\n",
    "# | 27.400 | 43.050 | 34.106 | 12.741 | 32.122 | 55.029 | dataset_test_original_reduced\n",
    "# | 13.037 | 20.674 | 16.070 | 8.540  | 17.123 | 28.456 | dataset_snowy_day\n",
    "# | 16.335 | 25.245 | 22.863 | 9.003  | 19.103 | 22.028 | dataset_snowy_night\n",
    "# | 19.437 | 32.808 | 26.297 | 9.343  | 24.582 | 49.679 | dataset_rb\n",
    "# Trained on reduced original ds+snowy11!!!_clean train set (110)\n",
    "## | 40.629 | 55.369 | 48.316 | 20.481 | 47.228 | 71.452 | dataset_test_original_reduced\n",
    "## | 23.651 | 34.864 | 29.611 | 14.119 | 28.572 | 47.609 | dataset_snowy_day\n",
    "## | 32.591 | 44.266 | 39.507 | 10.260 | 37.368 | 40.815 | dataset_snowy_night\n",
    "## | 28.068 | 38.899 | 36.524 | 11.019 | 26.866 | 71.075 | dataset_rb\n",
    "# Trained on reduced original ds+snowy12_clean train set\n",
    "# | 40.249 | 55.174 | 46.785 | 18.387 | 45.818 | 75.338 | dataset_test_original_reduced\n",
    "# | 22.504 | 33.711 | 27.502 | 12.444 | 26.322 | 50.499 | dataset_snowy_day\n",
    "# | 30.741 | 42.843 | 37.571 | 8.163  | 36.492 | 40.848 | dataset_snowy_night\n",
    "# | 28.301 | 39.012 | 35.747 | 10.758 | 26.531 | 72.088 | dataset_rb\n",
    "\n",
    "# Trained on reduced original ds+snowy2_clean train set, validated on base+snowy1\n",
    "# | 40.870 | 55.616 | 46.718 | 18.025 | 48.349 | 75.099 |\n",
    "# | 24.301 | 36.334 | 32.269 | 14.011 | 29.479 | 50.280 |\n",
    "# | 33.000 | 44.968 | 38.284 | 9.169  | 37.113 | 44.403 |\n",
    "# | 31.123 | 41.978 | 38.914 | 11.120 | 32.393 | 74.179 |\n",
    "\n",
    "# Trained on reduced original ds+snowy2_clean train set, validated on base val (again for test purposes) (run126)\n",
    "# | 41.216 | 56.311 | 48.854 | 19.793 | 48.123 | 74.205 |\n",
    "# | 23.317 | 34.646 | 27.433 | 13.878 | 27.546 | 50.096 |\n",
    "# | 32.696 | 44.561 | 38.976 | 5.875  | 39.534 | 42.136 |\n",
    "# | 31.938 | 44.228 | 41.942 | 15.081 | 31.874 | 71.841 |\n",
    "# Trained on reduced original ds+snowy2_clean train set, validated on base val (again for test purposes) (run129), corrected EarlyStop for +1\n",
    "# | 41.586 | 57.958 | 49.488 | 19.917 | 49.430 | 75.520 |\n",
    "# | 20.348 | 31.701 | 22.793 | 11.257 | 27.991 | 49.673 |\n",
    "# | 29.590 | 42.131 | 37.192 | 4.390  | 33.983 | 44.437 |\n",
    "# | 28.617 | 41.018 | 39.489 | 14.029 | 29.738 | 68.481 |\n",
    "\n",
    "# Trained on reduced original ds+snowy2 train set 5 shared_feats, validated on base val\n",
    "# | 40.940 | 55.415 | 47.934 | 19.880 | 47.156 | 74.997 |\n",
    "# | 24.242 | 35.449 | 29.178 | 13.602 | 28.188 | 51.382 |\n",
    "# | 31.979 | 44.839 | 39.228 | 8.137 | 35.326 | 47.263 |\n",
    "# | 27.096 | 37.766 | 34.528 | 10.253 | 25.181 | 72.292 |\n",
    "# | 41.531 | 57.572 | 47.809 | 22.123 | 46.618 | 72.393 | validation set\n",
    "# | 50.841 | 75.138 | 59.625 | 33.334 | 57.911 | 76.433 | validation set at end of training\n",
    "# Trained on snowy2 train set 5shared_feats, validated on base val\n",
    "# | 9.003 | 14.244 | 10.563 | 4.673 | 14.189 | 10.277 |\n",
    "# | 4.896 | 7.581  | 6.363  | 5.266 | 6.202 | 3.402 |\n",
    "# | 5.324 | 7.788  | 7.329  | 9.433 | 6.142 | 3.748 |\n",
    "# | 6.676 | 9.884  | 8.396  | 3.642 | 9.981 | 21.080 |\n",
    "# | 9.684 | 14.752 | 11.823 | 8.190 | 12.894 | 8.984 |\n",
    "\n",
    "# Trained on reduced original ds+snowy2 train set 3 shared_feats, validated on base val\n",
    "# | 41.350 | 56.059 | 48.035 | 19.497 | 48.979 | 73.283 |\n",
    "# | 23.036 | 34.242 | 29.728 | 13.483 | 27.873 | 48.104 |\n",
    "# | 30.043 | 40.968 | 35.327 | 7.369 | 33.548 | 43.896 |\n",
    "# | 29.247 | 38.818 | 36.348 | 10.579 | 29.475 | 73.652 |\n",
    "\n",
    "# Trained on reduced original ds+snowy2 clean train set 5 shared_feats, validated on base val\n",
    "# | 39.231 | 53.806 | 45.978 | 18.716 | 45.395 | 76.120 |\n",
    "# | 22.942 | 33.397 | 26.931 | 12.528 | 28.458 | 50.749 |\n",
    "# | 28.345 | 39.803 | 34.844 | 4.790 | 31.903 | 45.840 |\n",
    "# | 26.162 | 36.147 | 34.613 | 13.855 | 25.274 | 72.274 |\n",
    "\n",
    "# Trained on Cloudy TS, validated on base val\n",
    "# | 73.398 | 90.075 | 86.368 | 59.004 | 79.380 | 92.290 |\n",
    "# | 27.903 | 40.912 | 33.097 | 15.120 | 36.454 | 56.199 |\n",
    "# | 36.365 | 49.972 | 45.951 | 12.264 | 36.646 | 59.991 |\n",
    "# | 36.947 | 51.098 | 48.613 | 16.844 | 39.489 | 71.109 |\n",
    "\n",
    "# Trained on Snowy day test set, validated on base val\n",
    "# | 36.795 | 50.766 | 44.389 | 18.234 | 44.289 | 66.710 |\n",
    "# | 46.484 | 60.183 | 56.992 | 34.869 | 53.521 | 76.481 |\n",
    "# | 38.418 | 54.010 | 43.548 | 14.544 | 37.980 | 59.682 |\n",
    "# | 26.050 | 36.007 | 33.870 | 15.842 | 27.827 | 50.114 |\n",
    "\n",
    "# Trained on Snowy night test set, validated on base val\n",
    "# | 26.042 | 36.250 | 31.881 | 8.690 | 30.962 | 58.396 |\n",
    "# | 17.130 | 25.504 | 22.580 | 10.059 | 24.466 | 35.319 |\n",
    "# | 64.369 | 79.325 | 77.266 | 53.482 | 58.087 | 88.358 |\n",
    "# | 7.783 | 11.361 | 9.423  | 3.953 | 11.219 | 22.678 |\n",
    "\n",
    "# Trained on RB Test set, validated on base val\n",
    "# | 30.353 | 43.750 | 37.713 | 12.837 | 41.747 | 58.280 |\n",
    "# | 15.401 | 22.403 | 19.459 | 10.184 | 20.932 | 32.432 |\n",
    "# | 25.105 | 35.440 | 33.512 | 13.819 | 30.068 | 30.163 |\n",
    "# | 51.459 | 65.659 | 61.971 | 35.790 | 58.098 | 86.360 |\n",
    "\n",
    "# Trained on Cloudy TS, validated on base val, only 2000 iterations\n",
    "# | 60.644 | 79.082 | 72.994 | 42.927 | 69.393 | 86.044 |\n",
    "# | 27.339 | 40.496 | 31.221 | 16.247 | 35.326 | 52.667 |\n",
    "# | 35.940 | 49.729 | 45.018 | 16.105 | 38.856 | 58.505 |\n",
    "# | 30.525 | 44.472 | 39.534 | 16.170 | 33.523 | 73.552 |\n",
    "# | 32.991 | 45.668 | 38.869 | 12.542 | 40.493 | 70.720 | # sunny train set\n",
    "\n",
    "# Trained on Sunny DS, validated on base val, run_151\n",
    "# | 38.445 | 51.501 | 45.589 | 17.469 | 45.061 | 71.953 |\n",
    "# | 22.620 | 33.039 | 28.874 | 12.032 | 27.091 | 46.991 |\n",
    "# | 29.439 | 40.622 | 35.526 | 6.896 | 34.094 | 39.532 |\n",
    "# | 23.700 | 32.438 | 29.837 | 10.063 | 19.758 | 68.526 |\n",
    "# | 47.135 | 60.447 | 55.340 | 21.158 | 52.799 | 77.901 |\n",
    "# |   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl  |\n",
    "# |:------:|:------:|:------:|:-----:|:------:|:-----:|\n",
    "# | 14.830 | 29.550 | 8.663  | 0.644 | 37.907 | 0.000 | Snowy1\n",
    "# | 21.744 | 43.705 | 23.492 | 6.561 | 35.805 | 80.000 | Snowy2\n",
    "# | 55.050 | 100.000 | 25.248 | 70.000 | 52.244 |  nan  |\n",
    "# | 44.797 | 83.168 | 37.030 | 32.673 | 72.525 |  nan  | Snowy2_clean\n",
    "# | 22.075 | 49.934 | 11.069 | 15.651 | 28.186 | 52.525 | Snowy1_prev\n",
    "# | 38.256 | 70.252 | 39.190 | 16.171 | 32.623 | 66.186 | Snowy2_prev\n",
    "# | 36.313 | 66.038 | 25.129 | 18.614 | 47.057 | 70.000 | Snowy1_clean_prev\n",
    "# | 32.216 | 65.966 | 25.493 | 17.592 | 50.402 | 55.149 | Snowy2_clean_prev\n",
    "\n",
    "# Trained on Sunny DS for 3700 steps, validated on base val, run_161\n",
    "# | 40.764 | 56.288 | 48.817 | 18.795 | 48.413 | 77.876 |\n",
    "# | 20.906 | 31.741 | 25.021 | 9.712 | 29.664 | 50.245 |\n",
    "# | 29.396 | 41.675 | 35.754 | 6.444 | 31.602 | 46.891 |\n",
    "# | 30.215 | 40.978 | 39.734 | 14.280 | 30.306 | 69.979 |\n",
    "# | 67.056 | 78.741 | 74.886 | 54.285 | 64.909 | 95.520 | # sunny train set\n",
    "\n",
    "# Trained on sunny + gen_ad, validated on cloudy_val, run_182\n",
    "# | 42.548 | 57.815 | 51.854 | 21.842 | 50.296 | 75.707 |\n",
    "# | 23.600 | 35.603 | 28.463 | 13.760 | 29.930 | 49.543 |\n",
    "# | 27.776 | 38.116 | 34.173 | 5.264 | 30.981 | 42.980 |\n",
    "# | 30.402 | 42.728 | 40.584 | 12.693 | 33.506 | 68.037 |\n",
    "# | 59.024 | 73.689 | 68.021 | 38.365 | 62.845 | 91.762 | # sunny train set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sunny 60/20/20 split\n",
    "# | 17.051 | 26.048 | 17.827 | 7.947 | 18.290 | 65.991 |\n",
    "# | 26.614 | 38.855 | 29.993 | 11.276 | 31.853 | 64.150 |\n",
    "# | 37.532 | 54.219 | 46.709 | 36.368 | 37.212 | 48.022 |\n",
    "# | 0.000 | 0.000  | 0.000  |  nan  | 0.000 |  nan  |\n",
    "# | 52.614 | 65.545 | 65.545 |  nan  | 35.000 | 90.000 |\n",
    "\n",
    "\n",
    "\n",
    "# Snowy Day (50/25/25)\n",
    "# | 0.000 | 0.000  | 0.000  | 0.000 | 0.000 | 0.000 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on ('2021_02_18_06_28', '2021_03_25_14_04', '2021_04_05_14_35', '2021_05_18_14_02', '2021_06_05_12_08', '2021_07_07_06_41', '2021_08_09_06_30', '2021_09_15_06_28', '2021_10_15_18_16', '2021_11_02_12_59', '2021_12_15_12_54', '2022_01_21_14_04', 'dataset_train_reduced') for 30000 iterations\n",
      "                                AP\tAP50\tAP75\tAPs\tAPm\tAPl\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('2021_02_18_06_28',)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m inferences \u001b[39m=\u001b[39m all_inferences[new_dataset]\n\u001b[1;32m     10\u001b[0m inferences_diff \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 11\u001b[0m source_inferences \u001b[39m=\u001b[39m all_inferences[src]\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m inf, source_inf \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(inferences, source_inferences):\n\u001b[1;32m     13\u001b[0m     \u001b[39m# print(inf.keys())\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39m# print(\"src\", source_inf.keys())\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39m# get the difference between the source and the current dataset inferences lists\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[39m# inferences_diff.append([inferences[i] - source_inferences[i] for i in range(len(inferences))])\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     inferences_diff\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mbbox\u001b[39m\u001b[39m'\u001b[39m: \n\u001b[1;32m     18\u001b[0m         {k: inf[\u001b[39m'\u001b[39m\u001b[39mbbox\u001b[39m\u001b[39m'\u001b[39m][k] \u001b[39m-\u001b[39m source_inf[\u001b[39m'\u001b[39m\u001b[39mbbox\u001b[39m\u001b[39m'\u001b[39m][k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m inf[\u001b[39m'\u001b[39m\u001b[39mbbox\u001b[39m\u001b[39m'\u001b[39m]}})\n",
      "\u001b[0;31mKeyError\u001b[0m: ('2021_02_18_06_28',)"
     ]
    }
   ],
   "source": [
    "dataset_to_evaluate = [\"dataset_train_original\", \"dataset_train_generated\", \"dataset_train_generated_cleaned\", \"dataset_val\"] # \"dataset_val\", \n",
    "max_dataset_len = max([len(dataset) for dataset in dataset_to_evaluate])\n",
    "print(f\"Trained on {cfg.DATASETS.TRAIN} for {cfg.SOLVER.MAX_ITER } iterations\")\n",
    "print(\" \" * (max_dataset_len + 1) + \"AP\\tAP50\\tAP75\\tAPs\\tAPm\\tAPl\")\n",
    "\n",
    "src = (cfg.DATASETS.TRAIN[0], )\n",
    "new_dataset = cfg.DATASETS.TRAIN\n",
    "# print(\"All_inferences\", all_inferences)\n",
    "inferences = all_inferences[new_dataset]\n",
    "inferences_diff = []\n",
    "source_inferences = all_inferences[src]\n",
    "for inf, source_inf in zip(inferences, source_inferences):\n",
    "    # print(inf.keys())\n",
    "    # print(\"src\", source_inf.keys())\n",
    "    # get the difference between the source and the current dataset inferences lists\n",
    "    # inferences_diff.append([inferences[i] - source_inferences[i] for i in range(len(inferences))])\n",
    "    inferences_diff.append({'bbox': \n",
    "        {k: inf['bbox'][k] - source_inf['bbox'][k] for k in inf['bbox']}})\n",
    "\n",
    "def print_result(datasets, inferences):\n",
    "    for dataset, inference in zip(datasets, inferences):\n",
    "        results = \"\"\n",
    "        # print(\"inf\", inference)\n",
    "        for value in inference[\"bbox\"].values():\n",
    "            results += \"\\t\" + str(round(value, 3))\n",
    "        # fill the dataset name with spaces to make the output look nice\n",
    "        dataset = dataset + \" \" * (max_dataset_len - len(dataset))\n",
    "        print(f\"{dataset}{results}\")\n",
    "\n",
    "# print(\"iinf\", inferences)\n",
    "print_result(dataset_to_evaluate, inferences)\n",
    "print(\"\")\n",
    "print(f\"Diff between {src} and {new_dataset}\")\n",
    "print_result(dataset_to_evaluate, inferences_diff)\n",
    "\n",
    "# for dataset, inference in zip(dataset_to_evaluate, inferences):\n",
    "#     results = \"\"\n",
    "#     for value in inference[\"bbox\"].values():\n",
    "#         results += \"\\t\" + str(round(value, 3))\n",
    "#     # fill the dataset name with spaces to make the output look nice\n",
    "#     dataset = dataset + \" \" * (max_dataset_len - len(dataset))\n",
    "#     print(f\"{dataset}{results}\")\n",
    "\n",
    "# Trained on ('dataset_train_original',) for 3000 iterations\n",
    "#                                   AP\t    AP50\tAP75\tAPs\t    APm\t    APl\n",
    "# dataset_train_original         \t47.712\t71.238\t54.205\t33.646\t59.207\t75.995\n",
    "# dataset_train_generated        \t50.498\t83.097\t56.722\t38.37\t53.308\t59.683\n",
    "# dataset_train_generated_cleaned\t55.923\t89.011\t64.326\t41.508\t58.227\t70.957\n",
    "# dataset_val                    \t41.819\t59.327\t50.96\t23.14\t50.126\t75.897\n",
    "\n",
    "# Trained on ('dataset_train_reduced',) for 3000 iterations\n",
    "#                                   AP\t    AP50\tAP75\tAPs\t    APm\t    APl\n",
    "# dataset_train_original         \t43.166\t62.304\t49.716\t27.37\t55.352\t78.294\n",
    "# dataset_train_generated        \t51.598\t84.813\t55.95\t41.168\t52.868\t61.18\n",
    "# dataset_train_generated_cleaned\t57.243\t90.986\t64.276\t46.277\t57.732\t72.756\n",
    "# dataset_val                    \t36.084\t52.41\t41.594\t16.307\t44.483\t77.121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base dataset:\n",
    "# Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.222\n",
    "#  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.346\n",
    "#  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.256\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.097\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.328\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.445\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.271\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.364\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.365\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.193\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.491\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.572\n",
    "# [02/07 21:20:55 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
    "# |   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
    "# |:------:|:------:|:------:|:-----:|:------:|:------:|\n",
    "# | 22.216 | 34.607 | 25.624 | 9.704 | 32.764 | 44.500 |\n",
    "\n",
    "# OrderedDict([('bbox', {'AP': 22.216189299980034, 'AP50': 34.60726221662003, 'AP75': 25.623723833208423, 'APs': 9.704298064507062, 'APm': 32.76426802413977, 'APl': 44.499573814016266})])\n",
    "\n",
    "# Only Generated:\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.207\n",
    "#  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.308\n",
    "#  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.249\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.325\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.482\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.257\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.334\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.334\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.141\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.477\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.557\n",
    "# [02/07 22:06:46 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
    "# |   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
    "# |:------:|:------:|:------:|:-----:|:------:|:------:|\n",
    "# | 20.737 | 30.766 | 24.910 | 6.382 | 32.485 | 48.188 |\n",
    "\n",
    "# Combined:\n",
    "# Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.224\n",
    "#  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.362\n",
    "#  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.241\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.106\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.346\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.459\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.280\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.394\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.395\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.223\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.535\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.556\n",
    "# [02/07 22:35:46 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
    "# |   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
    "# |:------:|:------:|:------:|:------:|:------:|:------:|\n",
    "# | 22.430 | 36.236 | 24.099 | 10.622 | 34.568 | 45.883 |\n",
    "# cleaned:\n",
    "# | 23.292 | 35.682 | 26.742 | 11.776 | 33.034 | 47.688 |\n",
    "\n",
    "# Cleaned combined:\n",
    "# Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.233\n",
    "#  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.357\n",
    "#  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.267\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.118\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.330\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.477\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.283\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.385\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.386\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.237\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.492\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.579\n",
    "# [02/14 22:21:44 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
    "# |   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
    "# |:------:|:------:|:------:|:------:|:------:|:------:|\n",
    "# | 23.292 | 35.682 | 26.742 | 11.776 | 33.034 | 47.688 |\n",
    "\n",
    "# Only Generated cleaned:\n",
    "# Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.207\n",
    "#  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.323\n",
    "#  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.244\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.068\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.323\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.508\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.263\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.363\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.363\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.179\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.502\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.569\n",
    "# [02/14 22:10:04 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
    "# |   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
    "# |:------:|:------:|:------:|:-----:|:------:|:------:|\n",
    "# | 20.746 | 32.317 | 24.438 | 6.771 | 32.274 | 50.778 |\n",
    "\n",
    "# Base         | 22.216 | 34.607 | 25.624 | 9.704  | 32.764 | 44.500 |\n",
    "# Combined     | 22.430 | 36.236 | 24.099 | 10.622 | 34.568 | 45.883 |\n",
    "# Combcleaned  | 23.292 | 35.682 | 26.742 | 11.776 | 33.034 | 47.688 |\n",
    "# GeneratedOnly| 20.737 | 30.766 | 24.910 | 6.382  | 32.485 | 48.188 |\n",
    "# CleanedOnly  | 20.746 | 32.317 | 24.438 | 6.771  | 32.274 | 50.778 |\n",
    "\n",
    "# Base on pure validation set:\n",
    "#               | 23.440 | 34.936 | 27.234 | 12.098 | 32.949 | 50.946 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from pycocotools import cocoeval\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "from pycocotools.coco import COCO\n",
    "# import importlib\n",
    "# importlib.reload(COCO)\n",
    "\n",
    "def plot_pr_curve(dataset_gt, dataset_dt):\n",
    "    gt_coco = COCO(dataset_gt)\n",
    "\n",
    "    # dt_coco = COCO(\"./output/\" + dataset + \"/coco_instances_results.json\")\n",
    "\n",
    "    # with open(\"./output/\" + dataset + \"/coco_instances_results.json\", 'r') as f:\n",
    "    #     dataset = json.load(f)\n",
    "    # print(dataset)\n",
    "\n",
    "    dt_coco = gt_coco.loadRes(\"./output/\" + dataset_dt + \"/coco_instances_results.json\")\n",
    "    coco_eval = cocoeval.COCOeval(gt_coco, dt_coco, 'bbox')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    # print(coco_eval.eval)\n",
    "    all_precision = coco_eval.eval['precision']\n",
    "\n",
    "    pr_5 = all_precision[0, :, 0, 0, 2] # data for IoU@0.5\n",
    "    pr_7 = all_precision[4, :, 0, 0, 2] # data for IoU@0.7\n",
    "    pr_9 = all_precision[8, :, 0, 0, 2] # data for IoU@0.9\n",
    "\n",
    "    x = np.arange(0, 1.01, 0.01)\n",
    "    plt.title('Precision-Recall Curve: ' + dataset_dt)\n",
    "    # add a subtitle\n",
    "    plt.suptitle(f'Trained on {cfg.DATASETS.TRAIN}', fontsize=10)\n",
    "    plt.plot(x, pr_5, label='IoU@0.5')\n",
    "    plt.plot(x, pr_7, label='IoU@0.7')\n",
    "    plt.plot(x, pr_9, label='IoU@0.9')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # label the axes\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "\n",
    "    # save the plot \n",
    "    filename = dataset_dt + '_trained_on_' + str(cfg.DATASETS.TRAIN) + '.png'\n",
    "    save_dir = \"output/pr-curves/\"\n",
    "    # create the dir\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    file_path = save_dir + filename\n",
    "    plt.savefig(file_path, dpi=100)\n",
    "    print(\"Saved at \" + file_path)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_evaluate = [(VAL_PATH + VAL_NAME, \"dataset_val\"),\n",
    "                    (TRAIN_PATH_ORIGINAL + TRAIN_NAME_ORIGINAL, \"dataset_train_original\"), \n",
    "                    (TRAIN_PATH_GENERATED + TRAIN_NAME_CLEANED, \"dataset_train_generated_cleaned\")]\n",
    "for dataset in dataset_to_evaluate:\n",
    "    plot_pr_curve(dataset[0], dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKBbjnLw5GGG"
   },
   "source": [
    "# Other types of builtin models\n",
    "\n",
    "We showcase simple demos of other types of models below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "id": "GYJrlXZC5M-J",
    "outputId": "e7dc1067-2b72-4686-8ca6-9d7182dd6dc9"
   },
   "outputs": [],
   "source": [
    "# Inference with a keypoint detection model\n",
    "# cfg = get_cfg()   # get a fresh new config\n",
    "# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
    "# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
    "# predictor = DefaultPredictor(cfg)\n",
    "# outputs = predictor(im)\n",
    "# v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "# out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "# cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "id": "roTj1N9F5uJ5",
    "outputId": "d333be49-e316-4db7-f19c-bb74c0e10364"
   },
   "outputs": [],
   "source": [
    "# Inference with a panoptic segmentation model\n",
    "# cfg = get_cfg()\n",
    "# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
    "# predictor = DefaultPredictor(cfg)\n",
    "# panoptic_seg, segments_info = predictor(im)[\"panoptic_seg\"]\n",
    "# v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "# out = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\n",
    "# cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiXadAb9Fv-L"
   },
   "source": [
    "# Run panoptic segmentation on a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "YU5_W8wJF02F",
    "outputId": "716b0f98-d5a7-4fdc-e4b1-bc46e72eb209"
   },
   "outputs": [],
   "source": [
    "# # This is the video we're going to process\n",
    "# from IPython.display import YouTubeVideo, display\n",
    "# video = YouTubeVideo(\"ll8TgCZ0plk\", width=500)\n",
    "# display(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a65jM_VFF2Hr"
   },
   "outputs": [],
   "source": [
    "# Install dependencies, download the video, and crop 5 seconds for processing\n",
    "# !pip install youtube-dl\n",
    "# !youtube-dl https://www.youtube.com/watch?v=ll8TgCZ0plk -f 22 -o video.mp4\n",
    "# !ffmpeg -i video.mp4 -t 00:00:06 -c:v copy video-clip.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyA4VmKcF61k"
   },
   "outputs": [],
   "source": [
    "# Run frame-by-frame inference demo on this video (takes 3-4 minutes) with the \"demo.py\" tool we provided in the repo.\n",
    "# !git clone https://github.com/facebookresearch/detectron2\n",
    "# # Note: this is currently BROKEN due to missing codec. See https://github.com/facebookresearch/detectron2/issues/2901 for workaround.\n",
    "# %run detectron2/demo/demo.py --config-file detectron2/configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml --video-input video-clip.mp4 --confidence-threshold 0.6 --output video-output.mkv \\\n",
    "#   --opts MODEL.WEIGHTS detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpLg_MAQGPUT"
   },
   "outputs": [],
   "source": [
    "# Download the results\n",
    "# from google.colab import files\n",
    "# files.download('video-output.mkv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "fed15cd7d2bbb8309c1078024eacb5fd2be5e193d4e59a60e7ec1a4a7e63b1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
